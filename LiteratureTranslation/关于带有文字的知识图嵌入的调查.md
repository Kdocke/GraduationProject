# A Survey on Knowledge Graph <u>Embeddings</u> with <u>Literals</u>: Which model links better Literal-ly? 

```
带文字的知识图嵌入综述：哪个模型在文字上链接得更好？

Embeddings: 嵌入
Literals: 文字
```

Genet Asefa Gesese, Russa Biswas, Mehwish Alam, and Harald Sack
FIZ Karlsruhe - Leibniz <u>Institute</u> for Information <u>Infrastructure</u> & Institute for Applied Informatics and Formal
Description Systems (AIFB), Karlsruhe Institute of Technology, Karlsruhe Germany
E-mails: genet-asefa.gesese@fiz-karlsruhe.de, russa.biswas@fiz-karlsruhe.de, mehwish.alam@fiz-karlsruhe.de, harald.sack@fiz-karlsruhe.de

```
杰内・阿塞法・格塞斯、鲁萨・比斯瓦斯、迈赫威・阿拉姆、哈拉德・萨克
卡尔斯鲁厄 - 莱布尼茨信息基础设施研究所 & 应用信息学和形式研究所
描述系统 (AIFB) ，卡尔斯鲁厄理工学院，德国卡尔斯鲁厄
电子邮件: genet-asefa.gesese@fiz-karlsru.de、russa.biswas@fiz-karlsruhe.de、mehwish.alam@fiz-karlsru.de、harald.sack@fiz-karlsruhe.de

institute: 研究所
Infrastructure: 基础设施
```

**Abstract.** Knowledge Graphs (KGs) are <u>composed</u> of structured information about a particular <u>domain</u> in the form of entities and relations. In addition to the structured information KGs help in <u>facilitating</u> interconnectivity and interoperability between different resources <u>represented</u> in the Linked Data Cloud. KGs have been used in a variety of applications such as entity linking, question answering, recommender systems, etc. However, KG applications suffer from high computational and storage costs. Hence, there arises the necessity for a representation able to map the high dimensional KGs into low dimensional spaces, i.e., embedding space, preserving structural as well as relational information. This paper <u>conducts</u> a survey of KG embedding models which not only consider the structured information contained in the form of entities and relations in a KG but also the unstructured information represented as literals such as text, numerical values, images, etc. Along with a <u>theoretical</u> analysis and comparison of the methods proposed so far for generating KG embeddings with literals, an <u>empirical</u> <u>evaluation</u> of the different methods under identical settings has been performed for the general task of link prediction.

```
摘要.知识图（KGs）由有关特定领域的结构化信息以实体和关系的形式组成。除结构化信息外，KGs 还有助于促进链接数据云中表示的不同资源之间的互连性和互操作性。KGs 已用于各种应用程序中，例如实体链接，问题解答，推荐系统等。但是，KGs 应用程序承受着高昂的计算和存储成本。因此，需要一种能够将高维 KGs 映射到低维空间（即，嵌入空间），保留结构以及相关信息的表示形式。本文对 KG 嵌入模型进行了调查，该模型不仅考虑 KG 中实体和关系形式所包含的结构化信息，而且还考虑以文本，数值，图像等文字形式表示的非结构化信息。除了到目前为止提出的用于生成带有文字的 KG 嵌入的方法的理论分析和比较之外，还针对链接预测的一般任务对相同设置下的不同方法进行了经验评估。

composed: 组成
domain: 领域
facilitating: 促进
represented: 代表
dimensional: 纬度
conducts: 进行
theoretical: 理论的
empirical: 经验
evaluation: 评估
```

**Keywords:** Knowledge Graphs, Knowledge Graph Embeddings, Knowledge Graph Embeddings with Literals, Link Prediction, Survey

```
关键字：知识图、知识图嵌入、具有文字的知识图嵌入、链接预测、调查
```

## 1. Introduction

```
引言
```

  **K**nowledge Graphs (KGs) have become quite <u>crucial</u> for storing structured information. There has been a sudden attention towards using KGs for various applications mainly in the area of artificial intelligence. For instance, in a more general sense, KGs can be used to support decision making process and to improve different machine learning applications such as question answering==[1]==, recommender systems==[2]==, and relation extraction==[3]==. Some of the most popular publicly available general purpose KGs are DBpedia==[4]==,
Freebase==[5]==, Wikidata==[6]==, and YAGO==[7]==. These general purpose KGs often consist of huge amount of facts constructed using billions of entities (represented as nodes) and relations (as edges connecting these nodes).

[1] A.Bordes, S.Chopra and J.Weston, Question Answering with <u>Subgraph</u> Embeddings, in: *Proceedings of the 2014 <u>Conference</u> on Empirical Methods in Natural Language Processing* *(EMNLP)*, Association for Computational Linguistics, Doha, Qatar, 2014, pp. 615–620. doi:10.3115/v1/D14-1067. https://www.aclweb.org/anthology/D14-1067.

[2] F.Zhang, N.J.Yuan, D.Lian, X.Xie and W.-Y. Ma, <u>Collaborative</u> Knowledge Base Embedding for Recommender Systems, in: *Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, KDD’16, ACM, New York, NY, USA, 2016, pp.353–362. ISBN 978-1-4503-4232-2. doi:10.1145/2939672.2939673.

[3] J.Weston, A.Bordes, O.Yakhnenko and N.Usunier, Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction, in: *Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.*,2013.

[4] J.Lehmann, R.Isele, M.Jakob, A.Jentzsch, D.Kontokostas, P.N.Mendes, S.Hellmann, M.Morsey, P.Van Kleef, S.Auer et al., DBpedia–A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia, *Semantic Web* (2015).

[5] K.Bollacker, C.Evans, P.Paritosh, T.Sturge and J.Taylor, Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge, in: *ACM SIGMOD international conference on Management of data*, 2008.

[6] D.Vrandeciˇc and M.Krötzsch, Wikidata: A Free Collaborative Knowledge Base (2014).

[7] F.Mahdisoltani, J.Biega and F.M.Suchanek, Yago3: A Knowledge Base from Multilingual Wikipedias, in: *CIDR*, 2013.

```
  知识图（KGs）对于存储结构化信息已变得至关重要。突然之间，人们开始关注将 KGs 用于各种应用程序，主要是在人工智能领域。例如，从更一般的意义上讲，KGs 可用于支持决策过程并改善不同的机器学习应用程序，例如问题回答，推荐系统和关系提取。一些最流行的可公开获得的通用 KGs 是 DBpedia，Freebase，Wikidata 和 YAGO。这些通用 KGs 通常由使用数十亿个实体（表示为节点）和关系（作为连接这些节点的边）构造的大量事实组成。
  
[1] A.Bordes, S.Chopra和J.Weston,《带子图嵌入的问答》, 载于:《 2014 年自然语言处理经验方法会议(EMNLP)会议录》, 计算语言学协会, 多哈, 卡塔尔, 2014年, 第615–620页.doi:10.3115/v1/D14-1067. https://www.aclweb.org/anthology/D14-1067.
[2] F.Zhang, N.J.Yuan, D.Lian, X.Xie和W.-Y.Ma, 面向推荐系统的协同知识库嵌入, 载于: 第22届 ACM SIGKDD知识发现和数据挖掘国际会议论文集, KDD’16, ACM, 美国纽约, 2016年, 第353-362页.ISBN 978-1-4503-4232-2.doi:10.1145/2939672.2939673.
[3] J.Weston, A.Bordes, O.Yakhnenko和N.Usunier, 将语言和知识库与关系提取的嵌入模型相连接, 载于: 2013年自然语言处理经验方法会议论文集, 2013年.
[4] J.Lehmann, R.Isele, M.Jakob, A.Jentzsch, D.Kontokostas, P.N.Mendes, S.Hellmann, M.Morsey, P.Van Kleef, S.Auer等人, DBpedia–大型多语言知识库, 摘自 Wikipedia, 语义网(2015).
[5] K.Bollacker, C.Evans, P.Paritosh, T.Sturge和J.Taylor, Freebase: 一个协同创建的人类知识结构图数据库, 载于: ACM SIGMOD 国际数据管理会议上, 2008年.
[6] D.Vrandeciˇc和M.Krötzsch, 维基数据: 免费的协作知识库(2014).
[7] F.Mahdisoltani, J.Biega和F.M.Suchanek, Yago3:多语言维基百科的知识库, CIDR, 2013年.

crucial: 关键的
sense: 意义
extraction: 提取
purpose: 目的

Subgraph: 子图
Conference: 会议
Collaborative: 协同
```

   Although KGs are effective in representing structured data, there exist some issues which hinder their
efficient manipulation such as i) different KGs are usually based on different <u>rigorous</u> symbolic frameworks
and this makes it hard to utilize their data in other applications==[8]== and ii) the fact that a significant number of important graph algorithms needed for the efficient manipulation and analysis of graphs have proven
to be NP-complete==[9]==. In order to address these issues and use a KG more efficiently, it is beneficial to
transform it into a low dimensional vector space while preserving its <u>underlying</u> <u>semantics</u>. To this end, various attempts have been made so far to learn vector representations (embeddings) for KGs. However, most of these approaches, including the current state-of-the-art TransE==[10]==, are structure-based embeddings which do not make use of any literal information i.e., only triples consisting of entities connected via properties are usually considered. This is a major disadvantage because
information encoded in the literals will be left unused when capturing the semantics of a certain entity.

[8] A.Bordes, J.Weston, R.Collobert and Y.Bengio, Learning Structured Embeddings of Knowledge Bases, in: *Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence*, AAAI’11, AAAI Press, 2011, pp.301–306.http://dl.acm.org/citation.cfm?id=2900423.2900470.

[9] M.R.Garey and D.S.Johnson, *Computers and Intractability; A Guide to the Theory of NP-Completeness*, W. H. Freeman & Co., New York, NY, USA, 1990. ISBN 0716710455.

[10] A.Bordes, N.Usunier, A.Garcia-Duran, J.Weston and O.Yakhnenko, Translating Embeddings for Modeling MultiRelational Data, in: *NIPS*, 2013.     

 ```
  尽管 KGs 可以有效地表示结构化数据，但是仍然存在一些妨碍其有效操作的问题，例如 i）不同的 KGs 通常基于不同的严格符号框架，这使得在其他应用程序中难以利用其数据，并且 ii）事实证明，有效操纵和分析图所需的大量重要图算法都是 NP 完全的。为了解决这些问题并更有效地使用 KG，将其转换为低维向量空间同时保留其基本语义是有益的。为此，迄今为止已经进行了各种尝试来学习 KGs 的向量表示（嵌入）。但是，这些方法中的大多数，包括当前最新的 TransE，都是基于结构的嵌入，不使用任何文字信息，即通常只考虑由通过属性连接的实体组成的三元组。这是一个主要缺点，因为在捕获某个实体的语义时，将不使用原义编码的信息。

[8] A.Bordes, J.Weston, R.Collobert和Y.Bengio,《学习结构化的知识嵌入》,载于:《第二十五届AAAI人工智能会议论文集》, AAAI'11, AAAI出版社, 2011年, 第301-306 页.http://dl.acm.org/citation.cfm?id=2900423.2900470
[9] M.R. Garey和D.S. Johnson, 《计算机与难处理》, NP 完全性理论指南, W.H.Freeman＆Co., 美国纽约, 1990年, ISBN 0716710455.
[10] A.Bordes, N.Usunier, A.Garcia-Duran, J.Weston和O.Yakhnenko,《翻译多关系数据建模的嵌入》, NIPS, 2013年.

rigorous: 严格的
underlying: 潜在的
semantics: 语义学
vector: 向量
 ```

   Literals can bring ad van tages to the process of learning KG embeddings in two major ways:

 ```
文字学习者可以通过两种主要方式将广告范式带入学习 KG 嵌入的过程:
 ```

1. *Learning embeddings for novel entities:* Novel entities are entities which are not linked to any other entity in the KG but have literal values associated with them such as their *textual description*. In most existing structure-based embedding models, it is not possible to learn embeddings for such novel entities. However, this can be addressed by utilizing the information represented in literals to learn embeddings. For example, considering the dataset FB15K-20==[11]==, which is a subset of Freebase, the entity ’/m/0gjd61t’ is a novel entity which does not occur in any of the training triples, but it has a description given as follows in the form *<subject, relation, object>*.

   |                                                              |
   | ------------------------------------------------------------ |
   | </m/0gjd61t, http://rdf.freebase.com/ns/common.topic.description, " Vincent Franklin is an English actor best known for his roles in comedy television programmes..."> |

   In order to learn the embedding for this particular entity (i.e., /m/0gjd61t), the model should be enough to make use of the entity’s description. DKRL==[11]== is one of those approaches which provide embeddings for novel entities using their descriptions.

[11] R.Xie, Z.Liu, J.Jia, H.Luan and M. Sun, Representation Learning of Knowledge Graphs with Entity Descriptions, in: *AAAI*, 2016.

```
1. 学习新实体的嵌入：新实体是未与 KG 中的任何其他实体链接但具有与其关联的文字值（例如其文字说明）的实体。在大多数现有的基于结构的嵌入模型中，无法学习此类新颖实体的嵌入。但是，这可以通过利用文字中表示的信息来学习嵌入来解决。例如，考虑数据集 FB15K-20（它是 Freebase 的子集），实体 '/m/ 0gjd61t' 是一个新颖的实体，在任何训练三元组中都不会出现，但它有如下形式的描述<主题，关系，对象>。

</m/0gjd61t, http://rdf.freebase.com/ns/common.topic.description, "文森特・富兰克林是一位英语演员，以在喜剧电视节目中的作用而闻名">

为了了解该特定实体（即 /m/ 0gjd61t）的嵌入，该模型应足以利用该实体的描述。DKRL 是使用其描述为新颖实体提供嵌入的那些方法之一。

[11] R.Xie, Z.Liu, J.Jia, H.Luan和M.Sun, 带有实体描述的知识图的表示学习, 载于:AAAI, 2016.
```

2. Improving the representation of entities in structure based embedding models: Literals play a vital role in improving the representation learning where an entity is required to appear in at least a minimum number of relational triples. For example, taking into consideration only the information provided in a sample KG presented in Figure 1, which is extracted from DBpedia, it is not possible to tell apart the entities dbr:David_Prowse, dbr:Carrie_Fisher, and dbr:Peter_Mayhew from one another. This is the case due to the fact that the only information that is available regarding these entities in this KG is that they all act in the  movie dbr:Return_of_the_Jedi and this is not enough to know which entities are similar to each other and which are not. Therefore, if some KG embedding model is trained using only this KG, it is not possible to get good representations for the entities dbr:David_Prowse, dbr:Carrie_Fisher, and dbr:Peter-Mayhew.

   ![dbr:Return_of_the_Jedi.jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/dbr：Return_of_the_Jedi.jpg>)

    Fig.1. A small fraction of triples taken from the KG DBpedia[4].

   However, having the model trained with more triples containing literal values for these entities, as shown in Figure 2, would improve the embeddings for the entities. For instance, looking at the values of the data relation foaf:gender, both dbr:David_Prowse and dbr:Peter_Mayhew are male whereas dbr:Carrie_Fisher is a female. This information alone enables the model to learn a better representation for these entities such that the entities dbr:David_-Prowse and dbr:Peter_Mayhew are more similar to each other than they are to dbr:Carrie_Fisher. The above example indicates that the use of literals along with their respective entities would add more semantics so that similar entities can be represented close to each other in the vector space while those dissimilar are further apart.

```
2.在基于结构的嵌入模型中改善实体的表示：在要求实体至少出现在最小数量的关系三元组中的情况下，文字在改善表示学习方面起着至关重要的作用。例如，仅考虑从 DBpedia 中提取的图1所示样本 KG 中提供的信息，就不可能将实体 dbr:David_Prowse，dbr:Carrie_Fisher 和 dbr:Peter_Mayhew 彼此区分开。之所以如此，是因为以下事实：关于该 KG 中这些实体的唯一可用信息是它们都在电影 dbr:Return_of_the_Jedi 中起作用，而这不足以知道哪些实体彼此相似，哪些不相似。因此，如果仅使用该 KG 训练某些 KG 嵌入模型，则无法获得实体 dbr:David_Prowse，dbr:Carrie_Fisher 和 dbr:Peter-Mayhew 的良好表示。

Fig.1.取自 KG DBpedia 的一小部分三元组

但是，如图 2 所示，使用包含这些实体字面值的更多三元组训练模型，将会改善实体的嵌入。例如，查看数据关系 foaf:gender的值，dbr:David_Prowse 和 dbr:Peter_Mayhew 都是男性，而 dbr:Carrie_Fisher 是女性。仅凭此信息，就可以使模型学习更好地表示这些实体，以使实体dbr: David-Prowse 和 dbr: Peter mayhew 彼此之间的相似性比 dbr: Carrie fisher 更高。上面的示例表明，使用文字及其各自的实体将增加更多的语义，以便相似的实体可以在向量空间中彼此接近地表示，而那些不相似的实体则进一步分开。
```

   Recently, some approaches have been proposed which incorporate the information underlying literals to generate KG embeddings. The types of literals considered in these embedding methods are either text, numeric, images, or multi-modal literals, i.e., a combination of more than one medium of information. These methods use different techniques in order to incorporate the literals into the KG embeddings. However, data typed literals are not addressed in these KG embedding models and surveys that are conducted on KG embeddings. The main challenge with data typed literals, such as date and time, is that they require additional semantics to be represented in KG embeddings.

![dbr:Return_of_the_Jedi.jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/dbr：Return_of_the_Jedi%2Babstract.jpg>)

   Fig.2. A small fraction of triples with literals taken from the KG DBpedia[4].

```   
  最近，人们提出了一些利用文本信息生成 KG 嵌入的方法。这些嵌入方法中考虑的文字类型是文本，数字，图像或多模式文字，即多种信息介质的组合。这些方法使用不同的技术，以便将文字合并到 KG 嵌入中。但是，在这些 KG 嵌入模型和对 KG 嵌入进行的调查中，未解决数据类型的文字。数据类型文字（例如日期和时间）的主要挑战在于，它们需要在 KG 嵌入中表示其他语义。
  
Fig.2.三元组的一小部分，其文字取自 KG DBpedia
  
incorporate: 包括
```

   In this survey paper, the focus lies on the analysis of different embedding approaches and highlight their advantages and drawbacks in handling different challenges. Moreover,a review of the different applications used for model evaluation by different KG embedding models has been given with experiments conducted specifically on the link prediction task. The contribution of this paper is summarized as follows:

```
本文重点分析了不同的嵌入方法，突出了它们在处理不同挑战方面的优缺点。此外，还对不同 KG 嵌入模型在模型评估中的应用进行了评述，并针对链接预测任务进行了实验。本文的贡献总结如下：

drawbacks: 缺点
```

1. A detailed analysis of the existing literal enriched KG embedding models and their approaches. In addition, the models are categorized into different classes based on the type of literals used.
2. An evaluation oriented comparison of the existing models on the link prediction task is performed under same experimental settings.
3. The research gaps in the area of KG embeddings in using literals are indicated which can open directions for further research.

```
1. 详细分析了现有文献丰富的 KG 嵌入模型及其方法。 此外，根据所使用的文本类型，模型被分为不同的类。
2. 在相同的实验设置下，对链接预测任务上的现有模型进行了面向评估的比较。
3. 指出了 KG 嵌入在文献使用方面存在的研究空白，为进一步的研究开辟了方向。
```

   The rest of this paper is organized as follows: Section 2 presents a brief overview of related work. In Section 3, the problem formulation including definitions, preliminaries, types of literals and research questions are provided while Section 4 analyses different KG embedding techniques with literals is discussed. Section 5 reviews different tasks used to train or evaluate the embedding models is given. Section 6 discusses the experiment conducted with the existing KG embedding models with literals on the link prediction task. Finally, concluding remarks summarize our findings on KGs with literals and are presented along with future directions in Section 7.

```
  本文的其余部分安排如下：第二部分简要介绍了相关工作。第三部分给出了问题的表述，包括定义、预备知识、文本类型和研究性问题，第四部分分析了不同的 KG 文本嵌入技术。第五部分回顾了训练和评估嵌入模型的不同任务。第六部分讨论了使用现有的 KG 嵌入模型对链接预测任务进行文字编码的实验。最后，结束语总结了我们对带有文字的KGs 的发现，并在第七部分中介绍了未来的方向。
  
preliminaries: 初步的
```

## 2. Related Work

```
相关工作
```

   This section describes the state-of-the-art algorithms proposed for generating KG embeddings. It also
gives a brief overview of the surveys already published following these lines and what is lacking in those studies.

```        
  本节介绍了用于生成 KG 嵌入的最新算法。它还简要概述了按照这些原则已经发表的调查以及这些研究中缺少的内容。
```

   Different KG embedding techniques have been proposed so far which can be categorized as translation based models, semantic matching models, models incorporating entity types, models incorporating relation paths, models using logical rules, models with temporal information, models using graph structures, and models incorporating information represented in literals. A brief overview of the most popular methods, including the state-of-the-art approaches for generating KG embeddings are short listed in Table 1 with respect to the previously defined categories. The main focus of the current study is to analyse the last category in Table 1, i.e., models incorporating information represented as literals in KGs.

```    
  到目前为止，已经提出了不同的 KG 嵌入技术，可以分为基于翻译的模型、语义匹配模型、包含实体类型的模型、包含关系路径的模型、包含逻辑规则的模型、包含时间信息的模型、包含图结构的模型和包含文本信息的模型。表1中相对于先前定义的类别，简要概述了最流行的方法，包括生成 KG 嵌入的最新方法。这项研究的主要重点是分析表 1 的最后一个类别，即包含以 KG 中的文字表示的信息的模型。

temporal:暂时的
```

   Few attempts have been made to conduct surveys on the techniques and applications of KG embeddings ==[52–54]==. The survey==[52]== is conducted on factorization based, random walk based, and deep learning based network embedding approaches such as DeepWalk, Node2vec, and etc. ==[53, 54]== discuss only RESCAL==[17]== and KREAR==[55]== as methods which use attributes of entities for KG embeddings, and focus mostly on the structure-based embedding methods, i.e., methods using non-attributive triples, for example, translation based embedding models listed in Table 1. However, RESCAL is a matrix-factorization method for relational learning which encodes each object/data property as a slice of the tensor leading to an increase in the dimensionality of the tensor automatically. This method suffers from efficiency issues if literals are utilized while generating KG embeddings. Similarly, KREAR only considers those data properties which have categorical values, i.e., fixed number of values and ignores those which take any random literals as values. One of the recent surveys==[56]== summarizes the methods proposed so far on refining KGs. However, this survey does not confine itself to embedding techniques and also does not consider most of the approaches which are making use of literals.

[17] M.Nickel, V.Tresp and H.-P.Kriegel, A Three-Way Model for Collective Learning on Multi-Relational Data., in: *ICML*, 2011.

[52] P.Goyal and E.Ferrara, Graph Embedding Techniques, Applications, and Performance: A Survey, *Knowl. -Based Syst.* (2018).

[53] H.Cai, V.W.Zheng and K.C.-C.Chang, A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications, *TKDE* (2018).

[54] Q.Wang, Z.Mao, B.Wang and L.Guo, Knowledge Graph Embedding: A Survey of Approaches and Applications, *TKDE* (2017).

[55] Y.Lin, Z.Liu and M.Sun, Knowledge Representation Learning with Entities, Attributes and Relations, *ethnicity* (2016).

[56] H.Paulheim, Knowledge graph refinement: A survey of approaches and evaluation methods, *Semantic Web* 8(3) (2017),489–508.doi:10.3233/SW-160218.     

```   
  很少有人尝试对 KG 嵌入的技术和应用进行调查。该调查基于因子分解，基于随机游动和基于深度学习的网络嵌入方法（例如 DeepWalk，Node2vec 等）进行。仅讨论 RESCAL 和 KREAR 作为使用实体属性进行 KG 嵌入的方法，重点讨论了基于结构的嵌入方法，即使用非属性三元组的方法，例如表1中列出的基于翻译的嵌入模型。但是，RESCAL 是一种用于关系学习的矩阵分解方法，该方法将每个对象/数据属性编码为张量的切片，从而导致张量的维数自动增加。如果在生成 KG 嵌入时使用文字，则该方法会遇到效率问题。同样，KREAR 仅考虑具有分类值（即固定数量的值）的那些数据属性，而忽略那些将任何随机文字作为值的数据属性。最近的一项调查总结了迄今为止提出的提炼 KGs 的方法。然而，此调查并没有局限于嵌入技术，也没有考虑大多数利用文本的方法。
  
[17] M.Nickel, V.Tresp和H.-P.Kriegel,《基于多关系数据的集体学习的三向模型》,载于:ICML, 2011年
[52] P.Goyal和E.Ferrara, “图形嵌入技术, 应用程序和性能:调查,知识”, 基于系统.(2018)
[53] H.Cai, V.W.Zheng和K.C.-C.Chang,《图嵌入的综合调查:问题,技术和应用》,TKDE(2018)
[54] Q.Wang, Z.Mao, B.Wang和L.Guo, 知识图嵌入:方法与应用概述, TKDE(2017)
[55] Y.Lin, Z.Liu和M.Sun, 带有实体, 属性和关系的知识表示学习, 种族(2016)
[56] H.Paulheim, 知识图细化:方法和评估方法概述, 语义网8(3)(2017), 489-508.doi:10.3233/SW-160218
```

   None of the surveys mentioned above include all the existing KG embedding models which make use of literals, such as the ones categorized as models incorporating information represented in literals in Table 1. To the best of our knowledge, this is the first attempt to analyse the algorithms proposed so far for generating KG embeddings using literals. In this paper, discussions on the type of literals, the embedding approaches, and the applications/tasks on which the embedding models are evaluated are given. A categorization of the models based on the type of literals they use is also provided.

```
  上述调查均未包括所有利用文字的现有 KG 嵌入模型，例如被归类为包含表 1 所示文字信息的模型。据我们所知，这是首次尝试分析迄今为止提出的使用文字生成 KG 嵌入的算法。在本文中，讨论了文字类型，嵌入方法以及评估嵌入模型的应用程序/任务。还提供了基于模型使用的文字类型的分类。
```

​                                           Table 1  KG embedding models and their categories.

|            Categories             |                            Models                            |
| :-------------------------------: | :----------------------------------------------------------: |
|   Translational Distance Models   | TransE ==[10]== and its extensions: TransH==[12]== TransR==[13]==, TransD==[14]==, TranSparse==[15]==, TransA==[16]==etc. |
|     Semantic Matching Models      | RESCAL==[17]== and Its Extensions: DistMult==[18]==, HolE==[19]==, ComplEx==[20]==, and etc. Semantic Matching with Neural Networks: SME==[21]==, NTN==[22]==, MLP==[23]==, and etc. |
|     Models using Entity Types     | Extended RESCAL==[24]==, SSE==[25]==, TKRL==[26]==, Type constrained representation learning==[27]==, Rules incorporated KG completion models==[28]==, TRESCAL==[29]==, Entity Hierarchy Embedding==[30]== |
|    Models using Relation Paths    | PTransE==[31]==, Traversing KGs in Vector Space==[32]==, RTRANSE==[33]==, Compositional vector space==[34]==, Reasoning using RNN==[35]==, Context-dependent KG embedding==[36]== |
|    Models using Logical Rules     | Rules incorporated KG completion models==[28]==, Large-scale Knowledge Base Completion==[37]==, KALE==[38]==, Logical Background Knowledge for Relation Extraction==[39]==, and etc. |
| Models using Temporal Information | Time-Aware Link Prediction==[40]==, coevolution of event and KGs[41], Knowevolve==[42]== |
|   Models using Graph Structures   | GAKE==[43]==, Link Prediction in Multi-relational Graphs==[44]== |
|       Models using Literals       | LiteralE==[45]==, TransEA==[46]==, KBLRN==[47]==, MTKGNN==[48]==, MKBE==[49]==, KDCoE==[50]==, DKRL==[11]==, IKRL==[51]==, and etc. |

[12] Z.Wang, J. Zhang, J.Feng and Z.Chen, Knowledge Graph Embedding by Translating on Hyper planes, 2014.https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8531.

[13] Y. Lin, Z. Liu, M.Sun, Y.Liu and X.Zhu, Learning Entity and Relation Embeddings for Knowledge Graph Completion, 2015. https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571.

[14] G.Ji, S.He, L.Xu, K.Liu and J.Zhao, Knowledge Graph Embedding via Dynamic Mapping Matrix, 2015, pp. 687–696.doi:10.3115/v1/P15-1067.

[15] G.Ji, K.Liu, S.He and J.Zhao, Knowledge Graph Completion with Adaptive Sparse Transfer Matrix, 2016. https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11982.

[16] Y.Jia, Y.Wang, H.Lin, X.Jin and X.Cheng, Locally Adaptive Translation for Knowledge Graph Embedding, in: *Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence*, AAAI’16, AAAI Press, 2016, pp. 992–998. http://dl.acm.org/citation.cfm?id=3015812.3015960.

[18] B.Yang, W.Yih, X.He, J.Gao and L. Deng, Embedding Entities and Relations for Learning and Inference in Knowledge Bases, in: *3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*, 2015. http://arxiv.org/abs/1412.6575.

[19] M.Nickel, L.Rosasco and T.Poggio, Holographic Embeddings of Knowledge Graphs, in: *Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence*, AAAI’16, AAAI Press, 2016, pp.1955–1961. http://dl.acm.org/citation.cfm?id=3016100.3016172.

[20] T.Trouillon, J.Welbl, S.Riedel, E.Gaussier and G.Bouchard, Complex Embeddings for Simple Link Prediction, in: *Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48*, ICML’16, JMLR.org, 2016, pp. 2071–2080. http://dl.acm.org/citation.cfm?id=3045390.3045609.

[21] A.Bordes, X.Glorot, J.Weston and Y.Bengio, A semantic matching energy function for learning with multirelational data, *Machine Learning* 94(2) (2014), 233–259.doi:10.1007/s10994-013-5363-6.

[22] R.Socher, D.Chen, C.D.Manning and A.Ng, Reasoning With Neural Tensor Networks for Knowledge Base Completion, in: *Advances in Neural Information Processing Systems 26*, C.J.C.Burges, L.Bottou, M.Welling, Z.Ghahramani and K.Q.Weinberger, eds, Curran Associates, Inc., 2013, pp. 926–934.

[23] X.Dong, E.Gabrilovich, G.Heitz, W.Horn, N.Lao, K.Murphy, T.Strohmann, S.Sun and W.Zhang, Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion, in: *Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, KDD ’14, ACM, New York, NY, USA, 2014, pp. 601–610. ISBN 978-1-4503-2956-9. doi:10.1145/2623330.2623623.

[24] M.Nickel, V.Tresp and H.-P.Kriegel, Factorizing Yago: Scalable Machine Learning for Linked Data, in: *Proceedings of the 21st international conference on World Wide Web*, ACM, 2012.

[25] S.Guo, Q.Wang, B.Wang, L.Wang and L.Guo, SSE: Semantically Smooth Embedding for Knowledge Graphs, *IEEE Transactions on Knowledge and Data Engineering* PP (2016),1–1.doi:10.1109/TKDE.2016.2638425.

[26] R.Xie, Z.Liu and M.Sun, Representation Learning of Knowledge Graphs with Hierarchical Types, in: *IJCAI*, 2016.

[27] D.Krompa β, S.Baier and V.Tresp, Type-Constrained Representation Learning in Knowledge Graphs, in: *Proceedings of the 14th International Conference on The Semantic Web ISWC 2015 - Volume 9366*, Springer-Verlag, Berlin, Heidelberg, 2015, pp.640–655.ISBN 978-3-319-25006-9.

[28] Q.Wang, B.Wang and L.Guo, Knowledge Base Completion Using Embeddings and Rules, in: *Proceedings of the 24th International Conference on Artificial Intelligence*, IJCAI’15, AAAI Press, 2015, pp. 1859–1865. ISBN 978-1-57735-738-4.http://dl.acm.org/citation.cfm?id=2832415.2832507.

[29] K.-W.Chang, W.-t.Yih, B.Yang and C.Meek, Typed Tensor Decomposition of Knowledge Bases for Relation Extraction, in: *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, Association for Computational Linguistics, Doha, Qatar, 2014, pp. 1568–1579. doi:10.3115/v1/D14-1165. https://www.aclweb.org/anthology/D14-1165.

[30] Z.Hu, P.Huang, Y.Deng, Y.Gao and E.Xing, Entity Hierarchy Embedding, in: *Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, Association for Computational Linguistics, Beijing, China, 2015, pp.1292–1300.doi:10.3115/v1/P15-1125. https://www.aclweb.org/anthology/P15-1125.

[31] Y.Lin, Z.Liu and M.Sun, Modeling Relation Paths for Representation Learning of Knowledge Bases, in: *EMNLP*, 2015.

[32] K.Guu, J.Miller and P.Liang, Traversing Knowledge Graphs in Vector Space, in: *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, Association for Computational Linguistics, Lisbon, Portugal, 2015, pp.318–327.doi:10.18653/v1/D15-1038. https://www.aclweb.org/anthology/D15-1038.

[33] A.García-Durán, A.Bordes and N.Usunier, Composing Relationships with Translations, in: *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, Association for Computational Linguistics, Lisbon, Portugal, 2015, pp.286–290.doi:10.18653/v1/D15-1034. https://www.aclweb.org/anthology/D15-1034.

[34] A.Neelakantan, B.Roth and A.Mccallum, Compositional Vector Space Models for Knowledge Base Completion 1(2015). doi:10.3115/v1/P15-1016.

[35] R.Das, A.Neelakantan, D.Belanger and A.Mccallum, Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks, 2017, pp.132–141.doi:10.18653/v1/E17-1013.

[36] Y.Luo, Q.Wang, B.Wang and L.Guo, Context-Dependent Knowledge Graph Embedding, in: *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, Association for Computational Linguistics, Lisbon, Portugal, 2015, pp.1656–1661.doi:10.18653/v1/D15-1191.https://www.aclweb.org/anthology/D15-1191.

[37] Z.Wei, J.Zhao, K.Liu, Z.Qi, Z.Sun and G.Tian, Largescale Knowledge Base Completion: Inferring via Grounding Network Sampling over Selected Instances, in: *CIKM*, 2015.

[38] S.Guo, Q.Wang, L.Wang, B.Wang and L.Guo, Jointly Embedding Knowledge Graphs and Logical Rules, 2016, pp.192–202.doi:10.18653/v1/D16-1019.

[39] T.Rocktäschel, S.Singh and S.Riedel, Injecting Logical Background Knowledge into Embeddings for Relation Extraction, in: *Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, Association for Computational Linguistics, Denver, Colorado, 2015, pp.1119–1129.doi:10.3115/v1/N15-1118. https://www.aclweb.org/anthology/N15-1118.

[40] T.Jiang, T.Liu, T.Ge, L.Sha, S.Li, B.Chang and Z.Sui, Encoding Temporal Information for Time-Aware Link Prediction, in: *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, Association for Computational Linguistics, Austin, Texas, 2016, pp.2350–2354.doi:10.18653/v1/D16-1260. https://www.aclweb.org/anthology/D16-1260.

[41] C.Esteban, V.Tresp, Y.Yang, S.Baier and D.KrompaÃ§, Predicting the co-evolution of event and Knowledge Graphs, in: *2016 19th International Conference on Iformation Fusion (FUSION)*, 2016, pp.98–105.

[42] R.Trivedi, H.Dai, Y.Wang and L.Song, Know-evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs, in: *Proceedings of the 34th International Conference on Machine Learning - Volume 70*, ICML’17, JMLR.org, 2017, pp.3462–3471.http://dl.acm.org/citation.cfm?id=3305890.3306039.

[43] J.Feng, M.Huang, Y.Yang and X.Zhu, GAKE: Graph Aware Knowledge Embedding, in: *Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers*, The COLING 2016 Organizing Committee, Osaka, Japan, 2016, pp.641–651.https://www.aclweb.org/anthology/C16-1062.

[44] X.Jiang, V.Tresp, Y.Huang and M.Nickel, Link Prediction in Multi-relational Graphs Using Additive Models, in: *Proceedings of the 2012 International Conference on Semantic Technologies Meet Recommender Systems & Big Data-Volume 919*, SeRSy’12, CEUR-WS.org, Aachen, Germany, Germany, 2012, pp.1–12.http://dl.acm.org/citation.cfm?id=2887638.2887639.

[45] A.Kristiadi, M.A.Khan, D.Lukovnikov, J.Lehmann and A.Fischer, Incorporating Literals into Knowledge Graph Embeddings, in: *ISWC2019*, 2019.

[46] Y.Wu and Z.Wang, Knowledge Graph Embedding with Numeric Attributes of Entities, in: *Rep4NLP@ACL*, 2018

[47] A.García-Durán and M.Niepert, KBlrn: End-to-End Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features, in: *UAI*, 2018.

[48] Y.Tay, A.T.Luu, M.C.Phan and S.C.Hui, Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs, *CoRR* (2017).

[49] P.Pezeshkpour, L.Chen and S.Singh, Embedding Multimodal Relational Data for Knowledge Base Completion, in: *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, 2018, pp.3208–3218.

[50] M.Chen, Y.Tian, K.W.Chang, S.Skiena and C.Zaniolo, Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-Lingual Entity Alignment, *arXiv preprint arXiv:1806.06478* (2018).

[51] R.Xie, Z.Liu, T.-S.Chua, H.-B.Luan and M.Sun, Imageembodied Knowledge Representation Learning, in: *IJCAI*,2017.

​                                                                       Table 1  KG 嵌入模型及其类别

|        类别        |                             模型                             |
| :----------------: | :----------------------------------------------------------: |
|    平移距离模型    | TransE==[10]== 及其扩展名：TransH==[12]== TransR==[13]==，TransD==[14]==，TranSparse==[15]==，TransA==[16]== 等。 |
|    语义匹配模型    | RESCAL==[17]==及其扩展名：DistMult==[18]==，HolE==[19]==，ComplEx==[20]==等。神经网络的语义匹配：SME==[21]==，NTN==[22]==，MLP==[23]==等。 |
| 使用实体类型的模型 | 扩展 RESCAL==[24]==，SSE==[25]==，TKRL==[26]==，类型约束表示学习==[27]==，包含 KG 完成模型的规则==[28]==，TRESCAL==[29]==，实体层次结构嵌入==[30]== |
| 使用关系路径的模型 | PTransE==[31]==，在向量空间中遍历 KG==[32]==，RTRANSE==[33]==，成分向量空间==[34]==，使用 RNN 进行推理==[35]==，上下文相关的 KG 嵌入==[36]== |
| 使用逻辑规则的模型 | 包含 KG 完成模型的规则==[28]==，大规模知识库完成[37]==，KALE==[38]==，用于关系提取的逻辑背景知识==[39]==等。 |
| 使用时间信息的模型 | 时间感知链路预测==[40]==，事件和 KG 的协同进化==[41]==，Knowevolve==[42]== |
|  使用图结构的模型  |          GAKE==[43]==，多关系图中的链接预测==[44]==          |
|   使用文字的模型   | LiteralE==[45]==，TransEA==[46]==，KBLRN==[47]==，MTKGNN==[48]==，MKBE==[49]==，KDCoE==[50]==，DKRL==[11]==，IKRL==[51]==等。 |

```
[12] Z.Wang, J.Zhang, J.Feng和Z.Chen,通过在超平面上进行翻译来嵌入知识图,2014 年.https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8531
[13] Y. Lin, Z. Liu, M.Sun, Y.Liu和X.Zhu, 《用于知识图完成的学习实体和关系嵌入》, 2015年, https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571
[14] G.Ji, S.He, L.Xu, K.Liu和J.Zhao, 通过动态映射矩阵嵌入知识图, 2015年, 第687–696页.doi:10.3115/v1/P15-1067
[15] G.Ji, K.Liu, S.He和J.Zhao,《具有自适应稀疏转移矩阵的知识图完成》, 2016年.https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11982
[16] Y.Jia, Y.Wang, H.Lin, X.Jin和X.Cheng, 知识图嵌入的局部自适应翻译, 载于:第三十届AAAI人工智能会议论文集, AAAI'16, AAAI出版社, 2016年, 第992-998页.http://dl.acm.org/citation.cfm?id=3015812.3015960
[18] B.Yang, W.Yih, X.He, J.Gao和L.Deng, "在知识基础中嵌入实体和关系以进行学习和推理", 第3届国际学习代表大会, ICLR, 2015年, 圣地亚哥, 美国加利福尼亚州, 2015年5月7日至9 日, 会议记录, 2015 年.http://arxiv.org/abs/1412.6575
[19] M.Nickel, L.Rosasco 和 T.Poggio, 知识图的全息嵌入, 载于:第三十届AAAI人工智能会议论文集, AAAI’16, AAAI 出版社, 2016年, 第1955–1961页.http://dl.acm.org/citation.cfm?id=3016100.3016172
[20] T.Trouillon, J.Welbl, S.Riedel, E.Gaussier和G.Bouchard, "用于简单链接预测的复杂嵌入", 载于:第33届国际机器学习国际会议论文集-第48卷, ICML, 16, JMLR.org, 2016年, 第2071-2080页. http://dl.acm.org/citation.cfm?id=3045390.3045609
[21] A.Bordes, X.Glorot, J.Weston和Y.Bengio, 一种用于多关系数据学习的语义匹配能量函数, 机器学习94(2)(2014), 233-259.doi:10.1007/s10994-013-5363-6
[22] R.Socher, D.Chen, C.D.Manning和A.Ng, 使用神经张量网络推理完成知识库, 载于:神经信息处理系统的进展26, C.J.C.Burges, L.Bottou, M.Welling, Z.Ghahramani和K.Q.Weinberger编辑, Curran Associates, Inc., 2013年, 第926–934页
[23] X.Dong, E.Gabrilovich, G.Heitz, W.Horn, N.Lao, K.Murphy, T.Strohmann, S.Sun和W.Zhang, 知识库:概率论的网络规模方法Fusion, 载于:第20届ACM SIGKDD知识发现和数据挖掘国际会议论文集, KDD'14, ACM, 纽约, 美国, 2014年, 第601-610页.ISBN978-1-4503-2956-9.doi:10.1145/2623330.2623623
[24] M.Nickel, V.Tresp和H.-P.Kriegel, 《分解Yago:链接数据的可伸缩机器学习》, 载于:第21届万维网国际会议论文集, ACM, 2012 年
[25] S.Guo, Q.Wang, B.Wang, L.Wang和L.Guo, SSE:知识图的语义平滑嵌入, IEEE知识与数据工程学报, PP(2016年), 1-1.doi:10.1109/TKDE.2016.2638425
[26] R.Xie, Z.Liu和M.Sun, “具有分层类型的知识图的表示学习”, 载于:IJCAI, 2016年
[27] D.Krompa β, S.Baier和V.Tresp, 知识图的类型约束表示学习, 载于:第14届语义网ISWC国际会议论文集-第9366卷, 施普林格出版社, 柏林, 海德堡, 2015年, 第640-655页.ISBN978-3-319-25006-9
[28] Q.Wang, B.Wang 和 L.Guo, 《使用嵌入和规则完成知识库》, 载于:《第24届国际人工智能会议论文集》, IJCAI’15, AAAI出版社, 2015年, 第1859-1865页.ISBN978-1-57735-7384.http://dl.acm.org/citation.cfm?id=2832415.2832507
[29] K.-W.Chang, W.-t.Yih, B.Yang和C.Meek, “用于关系提取的知识库的类型张量分解”, 载于:2014年自然语言处理中的经验方法会议论文集(EMNLP), 计算语言学协会, 多哈, 卡塔尔, 2014年, 第1568-1579页. doi:10.3115/v1/D14-1165.https://www.aclweb.org/anthology/D14-1165
[30] Z.Hu, P.Huang, Y.Deng, Y.Gao和E.Xing, 实体层次结构嵌入, 载于:计算语言学协会第53届年会和第七届国际自然语言联合会议论文集处理(第1卷:长论文), 计算语言学协会, 北京, 中国, 2015年, 第1292-1300 页.doi:10.3115/v1/P15-1125.https://www.aclweb.org/anthology/P15-1125
[31] Y.Lin, Z.Liu和M.Sun, “知识库表示学习的关系路径建模”, EMNLP, 2015年
[32] K.Guu, J.Miller和P.Liang, 在向量空间中遍历知识图谱, 载于:《2015年自然语言处理经验方法会议论文集》, 计算语言学协会, 里斯本, 葡萄牙, 2015年, 第11页.318–327.doi:10.18653/v1/D15-1038 https://www.aclweb.org/anthology/D15-1038
[33] A.García-Durán, A.Bordes和N.Usunier, 《与翻译的关系》, 载于:《2015年自然语言处理经验方法会议论文集》, 计算语言学协会, 葡萄牙里斯本, 2015年, 第11页.286–290.doi:10.18653/v1/D15-1034 https://www.aclweb.org/anthology/D15-1034
[34] A.Neelakantan, B.Roth和A.Mccallum, 知识库完成的成分向量空间模型1(2015).doi:10.3115 /v1/P15-1016
[35] R.Das, A.Neelakantan, D.Belanger和A.Mccallum, 使用递归神经网络的实体, 关系和文本推理链, 2017年, 第132–141.doi:10.18653/v1/E17-1013
[36] Y.Luo, Q.Wang, B.Wang和L.Guo, 上下文相关的知识图嵌入, 载于:《自然语言处理经验方法的会议纪要》, 计算语言学协会, 里斯本, 葡萄牙, 2015年, 第1656–1661页.doi:10.18653/v1/D15-1191.https://www.aclweb.org/anthology/D15-1191
[37] Z.Wei, J.Zhao, K.Liu, Z.Qi, Z.Sun和G.Tian, 大型知识库完成:通过对选定实例进行接地网络采样推断, 载于:CIKM, 2015年
[38] S.Guo, Q.Wang, L.Wang, B.Wang和L.Guo, 联合嵌入知识图和逻辑规则, 2016年, 第192–202.doi:10.18653/v1/D16-1019
[39] T.Rocktäschel, S.Singh和S.Riedel, 将逻辑背景知识注入嵌入关系中以进行关系提取, 载于:计算语言学协会北美分会2015年会议论文集:人类语言技术, 计算语言学, 科罗拉多州丹佛, 2015年, 第1119-1129 页.doi:10.3115/v1/N15-1118.https://www.aclweb.org/anthology/N15-1118
[40] T.Jiang, T.Liu, T.Ge, L.Sha, S.Li, B.Chang和Z.Sui, 编码时间信息以进行时间感知的链接预测, 载于:2016年经验会议论文集《自然语言处理中的方法》, 计算语言学协会, 德克萨斯州奥斯汀, 2016年, 第 2350–2354.doi:10.18653/v1/D16-1260.https://www.aclweb.org/anthology/D16-1260
[41] C.Esteban, V.Tresp, Y.Yang, S.Baier和D.KrompaÃ§, 预测事件和知识图的共同演化, 载于:2016 年第19届国际信息融合会议(FUSION), 2016年, 第98–105页
[42] R.Trivedi, H.Dai, Y.Wang和L.Song, 《知识进化:动态知识图的深度时间推理》, 载于:《第34届机器学习国际会议论文集》, 第70卷, ICML'17,  JMLR.org, 2017年, 第3462-3471页.http://dl.acm.org/citation.cfm?id = 3305890.3306039
[43] J.Feng, M.Huang, Y.Yang和X.Zhu, GAKE:Graph Aware Knowledge Embedding, 载于:COLING 2016会议录中, 第26届国际计算语言学会议:技术论文, COLING2016组织委员会, 日本大阪, 2016年, 第641-651页.https://www.aclweb.org/anthology/C16-1062
[44] X.Jiang, V.Tresp, Y.Huang和M.Nickel, 使用加性模型的多关系图中的链接预测, 载于:2012年语义技术国际会议论文集与推荐系统和大数据量919会合, SeRSy'12, CEUR-WS.org, 德国亚琛, 德国, 2012年, 第1–12页.http://dl.acm.org/citation.cfm?id=2887638.2887639
[45] A.Kristiadi, M.A.Khan, D.Lukovnikov, J.Lehmann和A.Fischer, 《将文字融入知识图嵌入》, 载于:ISWC2019, 2019
[46] Y.Wu和Z.Wang, 知识图嵌入实体的数字属性, 载于:Rep4NLP@ACL, 2018年
[47] A.García-Durán和M.Niepert, KBlrn:具有潜在关系和数值特征的知识库表示的端到端学习, UAI, 2018年
[48] Y.Tay, A.T.Luu, M.C.Phan和S.C.Hui, 用于知识图中非离散属性预测的多任务神经网络, CoRR(2017 年)
[49] P.Pezeshkpour, L.Chen和S.Singh, 《嵌入多峰关系数据以完成知识库》, 载于:《2018年自然语言处理经验方法会议》(EMNLP)会议录, 2018年, 第3208–3218页
[50] M.Chen, Y.Tian, K.-W.Chang, S.Skiena和C.Zaniolo, 知识图和跨语言实体对齐的实体描述的共同训练嵌入, arXiv预印本arXiv:1806.06478(2018)
[51] R.Xie, Z.Liu, T.-S.Chua, H.-B.Luan和M.Sun, 图像体现的知识表示学习, 载于:IJCAI, 2017
```

   This survey is an extension of an already published short survey==[57]==. The major difference between the two versions is that (i) this survey contains a much more detailed theoretical analysis of the KG embedding models with literals proposed so far, and (ii) it performs empirical evaluation of the discussed models under the same experimental settings under the example of link prediction.

[57] G.A.Gesese, R.Biswas and H.Sack, A Comprehensive Survey of Knowledge Graph Embeddings with Literals: Techniques and Applications, in: *Proceedings of the Workshop on Deep Learning for Knowledge Graphs (DL4KG2019) Co-located with the 16th Extended Semantic Web Conference 2019 (ESWC 2019), Portoroz, Slovenia, June 2, 2019.*, 2019, pp. 31–40.http://ceur-ws.org/Vol-2377/paper_4.pdf.

```     
  这个调查是一个已经发表的简短调查的延伸。两个版本之间的主要区别在于: (1) 本调查包含了迄今为止提出的有文字的 KG 嵌入模型的更详细的理论分析，(2) 在相同的实验环境下，以链接预测为例对所讨论的模型进行了实证评价。

[57] GAGesese, R.Biswas和H.Sack,《对具有文字的知识图嵌入的全面调查:技术与应用》, 来自:与第16届扩展语义网络会议同期举办的知识图深度学习研讨会(DL4KG2019)的论文集 2019(ESWC 2019), 斯洛文尼亚 Portoroz, 2019年6月2日, 2019, 第31–40页.http://ceur-ws.org/Vol-2377/paper_4.pdf
```

## 3. Problem Formulation

``` 
问题表述
```

   This section briefly introduces the fundamentals of KGs and KG embeddings followed by a formal definition of KG embeddings with literals. It also poses various research questions about why conducting this study is a stepping stone for future development.

```   
  本节简要介绍 KGs 和 KG 嵌入的基本原理，然后给出带有文字的 KG 嵌入的正式定义。 这也提出了各种研究问题，为什么进行这项研究是未来发展的踏脚石。
```

### 3.1. *Preliminaries*

```   
准备工作
```

   **Knowledge Graphs.** Knowledge Graphs (KGs) consist of a set of triples *K* ⊆ *E* × *R* × (*E* ∪ *L*), where *E* is a set of resources referred to as entities, *L* a set of literals, and *R* a set of relations. An entity is identified by a URI which represents a real-world object or an abstract concept. A relation (or property) is a binary predicate and a literal is a string, date, or number eventually followed by its data type. For a triple <x, r, y>, x is a subject, r is a relation and y is an object. The subject and object are often referred to as *head* and *tail* entity respectively. The triples consisting of literals as objects are often referred to as *attributive triples*.

```   
  知识图. 知识图（KG）由一组三元组 K⊆E×R×(E∪L) 组成，其中 E 是一组称为实体的资源，L 是一组文字，而 R 是一组关系。实体由表示实际对象或抽象概念的 URI 标识。关系（或属性）是一个二进制谓词，而文字是一个字符串，日期或数字，后跟它的数据类型。对于三元组 <x，r，y>，x 是对象，r 是关系，y 是对象。主体和客体通常分别称为头和尾实体。由文字作为对象的三元组通常称为定语三元组。
```

**Relations (or Properties)** :

```   
关系（或属性）
```

   Based on the nature of the objects, relations are classified into two main categories:

``` 
  根据对象的性质，关系可分为两大类：
```

- **Object Relation** links an entity to another entity. E.g., in the triple
  <dbr:Albert_Einstein, dbo:field, dbr:Physics>, both dbr:Albert_Einstein and dbr:Physics are entities, the relation dbo:field is an *Object Relation*.

- **Data Type Relation** links an entity to its values, i.e., literals. For example, in
  <dbr:Albert_Einstein, dbo:birthDate, "1879-03-14">, where "1879-03-14" is a literal value, the relation dbo:birthDate is a *Data Type Relation*.

  ```
  - 对象关系将一个实体链接到另一个实体。 例如，在三元组 <dbr:Albert_Einstein, dbo:field, dbr:Physics>, dbr:Albert_Einstein和dbr:Physics 都是实体，dbo:field 是对象关系。
  - 数据类型关系将实体链接到其值，即文字。 例如，在 <dbr:Albert_Einstein, dbo:birthDate, "1879-03-14"> 中，其中 "1879-03-14" 是文字值，关系 dbo:birthDate 是数据类型关系。
  ```

-    

### 3.2. *Types of Literals*

```   
文字类型
```

   Literals in a KG encode additional information which is not captured by the entities or relations. There are different types of literals present in the KGs:

```   
  KG 中的文字对实体或关系未捕获的其他信息进行编码。 KG 中存在不同类型的文字：
```

* **Text Literals:** A wide variety of information can be stored in KGs in the form of free text such as names, labels, titles, descriptions, comments, etc. In most of the KG embedding models with literals, text information is further categorized into **Short text** and **Long text**. The literals which are fairly short such as for relation like names, titles, labels etc. are considered as *Short text*. On the other hand, for strings that are much longer such as descriptions of entities, comments, etc. are considered as *Long text* and are usually provided in natural language.

* **Numeric Literals:** Information encoded as integers, float and so on such as height, date, population, etc. also provide useful information about an entity. It is worth considering the numbers as distinct entities in the embedding models, as it has its own semantics to be covered which cannot be covered by string distance metrics. For instance, 777 is more similar to 788 than 77.

* **Units of Measurement:** Numeric literals often denote units of measurements to a definite magnitude. For example, Wikidata property wdt:P2048 takes values in mm, cm, m, km, inch, foot and pixel. Hence, discarding the units and considering only the numeric values without normalization results in loss of semantics, especially if units are not comparable, e.g., units of length and units of weight.

* **Image Literals:** Images also provide latent useful information for modelling the entities. For example, a person’s details such as age, gender, etc. can be deduced via visual analysis of an image depicting the person.

* **Other Types of Literals:** Useful information encoded in the form of other literals such as external URIs which could lead to an image, text, audio or video files.

  ```   
  - 文本文字：KG 可以以自由文字的形式储存各种各样的信息，例如名称，标签，标题，描述，注释等。在大多数带有文字的 KG 嵌入模型中，文本信息进一步分类为 Short 文本和 Long 文本。 诸如名称，标题，标签等关系之类的相当短的文字被视为 Short 文本。 另一方面，对于更长的字符串，例如实体描述，注释等，被视为Long 文本，通常以自然语言提供。 
  - 数字文字：以整数、浮点数等形式编码的信息，例如高度，日期，人口等，也提供有关实体的有用信息。在嵌入模型中，数字作为不同的实体是值得考虑的，因为它有自己的语义，不能被字符串距离度量所覆盖。 例如，777 更类似于 788 而不是 77。
  - 度量单位：数字字面值通常表示测量的单位到一个确定的大小。 例如，维基数据属性 wdt: P2048 采用 mm，cm，m，km，inch，foot 和 pixel 的值。 因此，丢弃单位而只考虑数值而不进行规范化会导致语义损失，特别是当单位不可比时，例如长度单位和重量单位。
  - 图像文字：图像还为实体建模提供了潜在的有用信息。 例如，可以通过对描绘此人的图像进行视觉分析来推断其详细信息，例如年龄，性别等。
  - 其他类型的文字：以其他文字形式编码的有用信息，例如外部 URI，可能会链接到一个图像，文本，音频或视频文件。
  ```

### 3.3. *Research Questions*

```
研究问题
```

   As it can be seen from the above discussion that the information represented in the KGs is diverse, modelling these entities is a challenging task. The challenges which are further targeted in this study are given as follows:

```  
  从以上讨论可以看出，KG 中表示的信息是多种多样的，对这些实体进行建模是一项艰巨的任务。这项研究进一步针对的挑战如下：
```

* **RQ1** –*How can structured (triples with object relations) and unstructured information (attributive triples) in the KGs be combined into the representation learning?*

* **RQ2** –*How can the heterogeneity of the types of literals present in the KGs be captured and combined into representation learning?*

  ``` 
  - 研究问题1 -如何将 KGs 中的结构化 (带对象关系的三元组) 和非结构化信息 (属性三元组) 结合到表征学习中？
  - 研究问题2 -KGs 中存在的文字类型的异质性如何被捕捉并结合到表征学习中？
  ```