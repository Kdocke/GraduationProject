# A Survey on Knowledge Graph <u>Embeddings</u> with <u>Literals</u>: Which model links better Literal-ly? 

```
带文字的知识图嵌入综述：哪个模型在文字上链接得更好？

Embeddings: 嵌入
Literals: 文字
```

Genet Asefa Gesese, Russa Biswas, Mehwish Alam, and Harald Sack
FIZ Karlsruhe - Leibniz <u>Institute</u> for Information <u>Infrastructure</u> & Institute for Applied Informatics and Formal
Description Systems (AIFB), Karlsruhe Institute of Technology, Karlsruhe Germany
E-mails: genet-asefa.gesese@fiz-karlsruhe.de, russa.biswas@fiz-karlsruhe.de, mehwish.alam@fiz-karlsruhe.de, harald.sack@fiz-karlsruhe.de

```
杰内・阿塞法・格塞斯、鲁萨・比斯瓦斯、迈赫威・阿拉姆、哈拉德・萨克
卡尔斯鲁厄 - 莱布尼茨信息基础设施研究所 & 应用信息学和形式研究所
描述系统 (AIFB) ，卡尔斯鲁厄理工学院，德国卡尔斯鲁厄
电子邮件: genet-asefa.gesese@fiz-karlsru.de、russa.biswas@fiz-karlsruhe.de、mehwish.alam@fiz-karlsru.de、harald.sack@fiz-karlsruhe.de

institute: 研究所
Infrastructure: 基础设施
```

**Abstract.** Knowledge Graphs (KGs) are <u>composed</u> of structured information about a particular <u>domain</u> in the form of entities and relations. In addition to the structured information KGs help in <u>facilitating</u> interconnectivity and interoperability between different resources <u>represented</u> in the Linked Data Cloud. KGs have been used in a variety of applications such as entity linking, question answering, recommender systems, etc. However, KG applications suffer from high computational and storage costs. Hence, there arises the necessity for a representation able to map the high dimensional KGs into low dimensional spaces, i.e., embedding space, preserving structural as well as relational information. This paper <u>conducts</u> a survey of KG embedding models which not only consider the structured information contained in the form of entities and relations in a KG but also the unstructured information represented as literals such as text, numerical values, images, etc. Along with a <u>theoretical</u> analysis and comparison of the methods proposed so far for generating KG embeddings with literals, an <u>empirical</u> <u>evaluation</u> of the different methods under identical settings has been performed for the general task of link prediction.

```
摘要.知识图（KGs）由有关特定领域的结构化信息以实体和关系的形式组成。除结构化信息外，KGs 还有助于促进链接数据云中表示的不同资源之间的互连性和互操作性。KGs 已用于各种应用程序中，例如实体链接，问题解答，推荐系统等。但是，KGs 应用程序承受着高昂的计算和存储成本。因此，需要一种能够将高维 KGs 映射到低维空间（即，嵌入空间），保留结构以及相关信息的表示形式。本文对 KG 嵌入模型进行了调查，该模型不仅考虑 KG 中实体和关系形式所包含的结构化信息，而且还考虑以文本，数值，图像等文字形式表示的非结构化信息。除了到目前为止提出的用于生成带有文字的 KG 嵌入的方法的理论分析和比较之外，还针对链接预测的一般任务对相同设置下的不同方法进行了经验评估。

composed: 组成
domain: 领域
facilitating: 促进
represented: 代表
dimensional: 纬度
conducts: 进行
theoretical: 理论的
empirical: 经验
evaluation: 评估
```

**Keywords:** Knowledge Graphs, Knowledge Graph Embeddings, Knowledge Graph Embeddings with Literals, Link Prediction, Survey

```
关键字：知识图、知识图嵌入、具有文字的知识图嵌入、链接预测、调查
```

## 1. Introduction

```
引言
```

  **K**nowledge Graphs (KGs) have become quite <u>crucial</u> for storing structured information. There has been a sudden attention towards using KGs for various applications mainly in the area of artificial intelligence. For instance, in a more general sense, KGs can be used to support decision making process and to improve different machine learning applications such as question answering==[1]==, recommender systems==[2]==, and relation extraction==[3]==. Some of the most popular publicly available general purpose KGs are DBpedia==[4]==,
Freebase==[5]==, Wikidata==[6]==, and YAGO==[7]==. These general purpose KGs often consist of huge amount of facts constructed using billions of entities (represented as nodes) and relations (as edges connecting these nodes).

[1] A.Bordes, S.Chopra and J.Weston, Question Answering with <u>Subgraph</u> Embeddings, in: *Proceedings of the 2014 <u>Conference</u> on Empirical Methods in Natural Language Processing* *(EMNLP)*, Association for Computational Linguistics, Doha, Qatar, 2014, pp. 615–620. doi:10.3115/v1/D14-1067. https://www.aclweb.org/anthology/D14-1067.

[2] F.Zhang, N.J.Yuan, D.Lian, X.Xie and W.-Y. Ma, <u>Collaborative</u> Knowledge Base Embedding for Recommender Systems, in: *Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, KDD’16, ACM, New York, NY, USA, 2016, pp.353–362. ISBN 978-1-4503-4232-2. doi:10.1145/2939672.2939673.

[3] J.Weston, A.Bordes, O.Yakhnenko and N.Usunier, Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction, in: *Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.*,2013.

[4] J.Lehmann, R.Isele, M.Jakob, A.Jentzsch, D.Kontokostas, P.N.Mendes, S.Hellmann, M.Morsey, P.Van Kleef, S.Auer et al., DBpedia–A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia, *Semantic Web* (2015).

[5] K.Bollacker, C.Evans, P.Paritosh, T.Sturge and J.Taylor, Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge, in: *ACM SIGMOD international conference on Management of data*, 2008.

[6] D.Vrandeciˇc and M.Krötzsch, Wikidata: A Free Collaborative Knowledge Base (2014).

[7] F.Mahdisoltani, J.Biega and F.M.Suchanek, Yago3: A Knowledge Base from Multilingual Wikipedias, in: *CIDR*, 2013.

```
  知识图（KGs）对于存储结构化信息已变得至关重要。突然之间，人们开始关注将 KGs 用于各种应用程序，主要是在人工智能领域。例如，从更一般的意义上讲，KGs 可用于支持决策过程并改善不同的机器学习应用程序，例如问题回答[1]，推荐系统[2]和关系提取[3]。一些最流行的可公开获得的通用 KGs 是 DBpedia[4]，Freebase[5]，Wikidata[6] 和 YAGO[7]。这些通用 KGs 通常由使用数十亿个实体（表示为节点）和关系（作为连接这些节点的边）构造的大量事实组成。
  
[1] A.Bordes, S.Chopra和J.Weston,《带子图嵌入的问答》, 载于:《 2014 年自然语言处理经验方法会议(EMNLP)会议录》, 计算语言学协会, 多哈, 卡塔尔, 2014年, 第615–620页.doi:10.3115/v1/D14-1067. https://www.aclweb.org/anthology/D14-1067.
[2] F.Zhang, N.J.Yuan, D.Lian, X.Xie和W.-Y.Ma, 面向推荐系统的协同知识库嵌入, 载于: 第22届 ACM SIGKDD知识发现和数据挖掘国际会议论文集, KDD’16, ACM, 美国纽约, 2016年, 第353-362页.ISBN 978-1-4503-4232-2.doi:10.1145/2939672.2939673.
[3] J.Weston, A.Bordes, O.Yakhnenko和N.Usunier, 将语言和知识库与关系提取的嵌入模型相连接, 载于: 2013年自然语言处理经验方法会议论文集, 2013年.
[4] J.Lehmann, R.Isele, M.Jakob, A.Jentzsch, D.Kontokostas, P.N.Mendes, S.Hellmann, M.Morsey, P.Van Kleef, S.Auer等人, DBpedia–大型多语言知识库, 摘自 Wikipedia, 语义网(2015).
[5] K.Bollacker, C.Evans, P.Paritosh, T.Sturge和J.Taylor, Freebase: 一个协同创建的人类知识结构图数据库, 载于: ACM SIGMOD 国际数据管理会议上, 2008年.
[6] D.Vrandeciˇc和M.Krötzsch, 维基数据: 免费的协作知识库(2014).
[7] F.Mahdisoltani, J.Biega和F.M.Suchanek, Yago3:多语言维基百科的知识库, CIDR, 2013年.

crucial: 关键的
sense: 意义
extraction: 提取
purpose: 目的

Subgraph: 子图
Conference: 会议
Collaborative: 协同
```

   Although KGs are effective in representing structured data, there exist some issues which hinder their
efficient manipulation such as i) different KGs are usually based on different <u>rigorous</u> symbolic frameworks
and this makes it hard to utilize their data in other applications==[8]== and ii) the fact that a significant number of important graph algorithms needed for the efficient manipulation and analysis of graphs have proven
to be NP-complete==[9]==. In order to address these issues and use a KG more efficiently, it is beneficial to
transform it into a low dimensional vector space while preserving its <u>underlying</u> <u>semantics</u>. To this end, various attempts have been made so far to learn vector representations (embeddings) for KGs. However, most of these approaches, including the current state-of-the-art TransE==[10]==, are structure-based embeddings which do not make use of any literal information i.e., only triples consisting of entities connected via properties are usually considered. This is a major disadvantage because
information encoded in the literals will be left unused when capturing the semantics of a certain entity.

[8] A.Bordes, J.Weston, R.Collobert and Y.Bengio, Learning Structured Embeddings of Knowledge Bases, in: *Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence*, AAAI’11, AAAI Press, 2011, pp.301–306.http://dl.acm.org/citation.cfm?id=2900423.2900470.

[9] M.R.Garey and D.S.Johnson, *Computers and Intractability; A Guide to the Theory of NP-Completeness*, W. H. Freeman & Co., New York, NY, USA, 1990. ISBN 0716710455.

[10] A.Bordes, N.Usunier, A.Garcia-Duran, J.Weston and O.Yakhnenko, Translating Embeddings for Modeling MultiRelational Data, in: *NIPS*, 2013.     

 ```
  尽管 KGs 可以有效地表示结构化数据，但是仍然存在一些妨碍其有效操作的问题，例如 i）不同的 KGs 通常基于不同的严格符号框架，这使得在其他应用程序中难以利用其数据[8]，并且 ii)事实证明，有效操纵和分析图所需的大量重要图算法都是 NP 完全的[9]。为了解决这些问题并更有效地使用 KG，将其转换为低维向量空间同时保留其基本语义是有益的。为此，迄今为止已经进行了各种尝试来学习 KGs 的向量表示（嵌入）。但是，这些方法中的大多数，包括当前最新的 TransE[10]，都是基于结构的嵌入，不使用任何文字信息，即通常只考虑由通过属性连接的实体组成的三元组。这是一个主要缺点，因为在捕获某个实体的语义时，将不使用原义编码的信息。

[8] A.Bordes, J.Weston, R.Collobert和Y.Bengio,《学习结构化的知识嵌入》,载于:《第二十五届AAAI人工智能会议论文集》, AAAI'11, AAAI出版社, 2011年, 第301-306 页.http://dl.acm.org/citation.cfm?id=2900423.2900470
[9] M.R. Garey和D.S. Johnson, 《计算机与难处理》, NP 完全性理论指南, W.H.Freeman＆Co., 美国纽约, 1990年, ISBN 0716710455.
[10] A.Bordes, N.Usunier, A.Garcia-Duran, J.Weston和O.Yakhnenko,《翻译多关系数据建模的嵌入》, NIPS, 2013年.

rigorous: 严格的
underlying: 潜在的
semantics: 语义学
vector: 向量
 ```

   Literals can bring ad van tages to the process of learning KG embeddings in two major ways:

 ```
文字学习者可以通过两种主要方式将广告范式带入学习 KG 嵌入的过程:
 ```

1. *Learning embeddings for novel entities:* Novel entities are entities which are not linked to any other entity in the KG but have literal values associated with them such as their *textual description*. In most existing structure-based embedding models, it is not possible to learn embeddings for such novel entities. However, this can be addressed by utilizing the information represented in literals to learn embeddings. For example, considering the dataset FB15K-20==[11]==, which is a subset of Freebase, the entity ’/m/0gjd61t’ is a novel entity which does not occur in any of the training triples, but it has a description given as follows in the form *<subject, relation, object>*.

   |                                                              |
   | ------------------------------------------------------------ |
   | </m/0gjd61t, http://rdf.freebase.com/ns/common.topic.description, " Vincent Franklin is an English actor best known for his roles in comedy television programmes..."> |

   In order to learn the embedding for this particular entity (i.e., /m/0gjd61t), the model should be enough to make use of the entity’s description. DKRL==[11]== is one of those approaches which provide embeddings for novel entities using their descriptions.

[11] R.Xie, Z.Liu, J.Jia, H.Luan and M. Sun, Representation Learning of Knowledge Graphs with Entity Descriptions, in: *AAAI*, 2016.

```
1. 学习新实体的嵌入：新实体是未与 KG 中的任何其他实体链接但具有与其关联的文字值（例如其文字说明）的实体。在大多数现有的基于结构的嵌入模型中，无法学习此类新颖实体的嵌入。但是，这可以通过利用文字中表示的信息来学习嵌入来解决。例如，考虑数据集 FB15K-20[11]（它是 Freebase 的子集），实体 '/m/ 0gjd61t' 是一个新颖的实体，在任何训练三元组中都不会出现，但它有如下形式的描述<主题，关系，对象>。

</m/0gjd61t, http://rdf.freebase.com/ns/common.topic.description, "文森特・富兰克林是一位英语演员，以在喜剧电视节目中的作用而闻名">

为了了解该特定实体（即 /m/ 0gjd61t）的嵌入，该模型应足以利用该实体的描述。DKRL[11] 是使用其描述为新颖实体提供嵌入的那些方法之一。

[11] R.Xie, Z.Liu, J.Jia, H.Luan和M.Sun, 带有实体描述的知识图的表示学习, 载于:AAAI, 2016.
```

2. Improving the representation of entities in structure based embedding models: Literals play a vital role in improving the representation learning where an entity is required to appear in at least a minimum number of relational triples. For example, taking into consideration only the information provided in a sample KG presented in Figure 1, which is extracted from DBpedia, it is not possible to tell apart the entities dbr:David_Prowse, dbr:Carrie_Fisher, and dbr:Peter_Mayhew from one another. This is the case due to the fact that the only information that is available regarding these entities in this KG is that they all act in the  movie dbr:Return_of_the_Jedi and this is not enough to know which entities are similar to each other and which are not. Therefore, if some KG embedding model is trained using only this KG, it is not possible to get good representations for the entities dbr:David_Prowse, dbr:Carrie_Fisher, and dbr:Peter-Mayhew.

   ![dbr:Return_of_the_Jedi.jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/dbr：Return_of_the_Jedi.jpg>)

    Fig.1. A small fraction of triples taken from the KG DBpedia[4].

   However, having the model trained with more triples containing literal values for these entities, as shown in Figure 2, would improve the embeddings for the entities. For instance, looking at the values of the data relation foaf:gender, both dbr:David_Prowse and dbr:Peter_Mayhew are male whereas dbr:Carrie_Fisher is a female. This information alone enables the model to learn a better representation for these entities such that the entities dbr:David_-Prowse and dbr:Peter_Mayhew are more similar to each other than they are to dbr:Carrie_Fisher. The above example indicates that the use of literals along with their respective entities would add more semantics so that similar entities can be represented close to each other in the vector space while those dissimilar are further apart.

```
2.在基于结构的嵌入模型中改善实体的表示：在要求实体至少出现在最小数量的关系三元组中的情况下，文字在改善表示学习方面起着至关重要的作用。例如，仅考虑从 DBpedia 中提取的图1所示样本 KG 中提供的信息，就不可能将实体 dbr:David_Prowse，dbr:Carrie_Fisher 和 dbr:Peter_Mayhew 彼此区分开。之所以如此，是因为以下事实：关于该 KG 中这些实体的唯一可用信息是它们都在电影 dbr:Return_of_the_Jedi 中起作用，而这不足以知道哪些实体彼此相似，哪些不相似。因此，如果仅使用该 KG 训练某些 KG 嵌入模型，则无法获得实体 dbr:David_Prowse，dbr:Carrie_Fisher 和 dbr:Peter-Mayhew 的良好表示。

Fig.1.取自 KG DBpedia 的一小部分三元组

但是，如图 2 所示，使用包含这些实体字面值的更多三元组训练模型，将会改善实体的嵌入。例如，查看数据关系 foaf:gender的值，dbr:David_Prowse 和 dbr:Peter_Mayhew 都是男性，而 dbr:Carrie_Fisher 是女性。仅凭此信息，就可以使模型学习更好地表示这些实体，以使实体dbr: David-Prowse 和 dbr: Peter mayhew 彼此之间的相似性比 dbr: Carrie fisher 更高。上面的示例表明，使用文字及其各自的实体将增加更多的语义，以便相似的实体可以在向量空间中彼此接近地表示，而那些不相似的实体则进一步分开。
```

   Recently, some approaches have been proposed which incorporate the information underlying literals to generate KG embeddings. The types of literals considered in these embedding methods are either text, numeric, images, or multi-modal literals, i.e., a combination of more than one medium of information. These methods use different techniques in order to incorporate the literals into the KG embeddings. However, data typed literals are not addressed in these KG embedding models and surveys that are conducted on KG embeddings. The main challenge with data typed literals, such as date and time, is that they require additional semantics to be represented in KG embeddings.

![dbr:Return_of_the_Jedi.jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/dbr：Return_of_the_Jedi%2Babstract.jpg>)

   Fig.2. A small fraction of triples with literals taken from the KG DBpedia[4].

```   
  最近，人们提出了一些利用文本信息生成 KG 嵌入的方法。这些嵌入方法中考虑的文字类型是文本，数字，图像或多模式文字，即多种信息介质的组合。这些方法使用不同的技术，以便将文字合并到 KG 嵌入中。但是，在这些 KG 嵌入模型和对 KG 嵌入进行的调查中，未解决数据类型的文字。数据类型文字（例如日期和时间）的主要挑战在于，它们需要在 KG 嵌入中表示其他语义。
  
Fig.2.三元组的一小部分，其文字取自 KG DBpedia
  
incorporate: 包括
```

   In this survey paper, the focus lies on the analysis of different embedding approaches and highlight their advantages and drawbacks in handling different challenges. Moreover,a review of the different applications used for model evaluation by different KG embedding models has been given with experiments conducted specifically on the link prediction task. The contribution of this paper is summarized as follows:

```
本文重点分析了不同的嵌入方法，突出了它们在处理不同挑战方面的优缺点。此外，还对不同 KG 嵌入模型在模型评估中的应用进行了评述，并针对链接预测任务进行了实验。本文的贡献总结如下：

drawbacks: 缺点
```

1. A detailed analysis of the existing literal enriched KG embedding models and their approaches. In addition, the models are categorized into different classes based on the type of literals used.
2. An evaluation oriented comparison of the existing models on the link prediction task is performed under same experimental settings.
3. The research gaps in the area of KG embeddings in using literals are indicated which can open directions for further research.

```
1. 详细分析了现有文献丰富的 KG 嵌入模型及其方法。 此外，根据所使用的文本类型，模型被分为不同的类。
2. 在相同的实验设置下，对链接预测任务上的现有模型进行了面向评估的比较。
3. 指出了 KG 嵌入在文献使用方面存在的研究空白，为进一步的研究开辟了方向。
```

   The rest of this paper is organized as follows: Section 2 presents a brief overview of related work. In Section 3, the problem formulation including definitions, preliminaries, types of literals and research questions are provided while Section 4 analyses different KG embedding techniques with literals is discussed. Section 5 reviews different tasks used to train or evaluate the embedding models is given. Section 6 discusses the experiment conducted with the existing KG embedding models with literals on the link prediction task. Finally, concluding remarks summarize our findings on KGs with literals and are presented along with future directions in Section 7.

```
  本文的其余部分安排如下：第二部分简要介绍了相关工作。第三部分给出了问题的表述，包括定义、预备知识、文本类型和研究性问题，第四部分分析了不同的 KG 文本嵌入技术。第五部分回顾了训练和评估嵌入模型的不同任务。第六部分讨论了使用现有的 KG 嵌入模型对链接预测任务进行文字编码的实验。最后，结束语总结了我们对带有文字的KGs 的发现，并在第七部分中介绍了未来的方向。
  
preliminaries: 初步的
```

## 2. Related Work

```
相关工作
```

   This section describes the state-of-the-art algorithms proposed for generating KG embeddings. It also
gives a brief overview of the surveys already published following these lines and what is lacking in those studies.

```        
  本节介绍了用于生成 KG 嵌入的最新算法。它还简要概述了按照这些原则已经发表的调查以及这些研究中缺少的内容。
```

   Different KG embedding techniques have been proposed so far which can be categorized as translation based models, semantic matching models, models incorporating entity types, models incorporating relation paths, models using logical rules, models with temporal information, models using graph structures, and models incorporating information represented in literals. A brief overview of the most popular methods, including the state-of-the-art approaches for generating KG embeddings are short listed in Table 1 with respect to the previously defined categories. The main focus of the current study is to analyse the last category in Table 1, i.e., models incorporating information represented as literals in KGs.

```    
  到目前为止，已经提出了不同的 KG 嵌入技术，可以分为基于翻译的模型、语义匹配模型、包含实体类型的模型、包含关系路径的模型、包含逻辑规则的模型、包含时间信息的模型、包含图结构的模型和包含文本信息的模型。表1中相对于先前定义的类别，简要概述了最流行的方法，包括生成 KG 嵌入的最新方法。这项研究的主要重点是分析表 1 的最后一个类别，即包含以 KG 中的文字表示的信息的模型。

temporal:暂时的
```

   Few attempts have been made to conduct surveys on the techniques and applications of KG embeddings ==[52–54]==. The survey==[52]== is conducted on factorization based, random walk based, and deep learning based network embedding approaches such as DeepWalk, Node2vec, and etc. ==[53, 54]== discuss only RESCAL==[17]== and KREAR==[55]== as methods which use attributes of entities for KG embeddings, and focus mostly on the structure-based embedding methods, i.e., methods using non-attributive triples, for example, translation based embedding models listed in Table 1. However, RESCAL is a matrix-factorization method for relational learning which encodes each object/data property as a slice of the tensor leading to an increase in the dimensionality of the tensor automatically. This method suffers from efficiency issues if literals are utilized while generating KG embeddings. Similarly, KREAR only considers those data properties which have categorical values, i.e., fixed number of values and ignores those which take any random literals as values. One of the recent surveys==[56]== summarizes the methods proposed so far on refining KGs. However, this survey does not confine itself to embedding techniques and also does not consider most of the approaches which are making use of literals.

[17] M.Nickel, V.Tresp and H.-P.Kriegel, A Three-Way Model for Collective Learning on Multi-Relational Data., in: *ICML*, 2011.

[52] P.Goyal and E.Ferrara, Graph Embedding Techniques, Applications, and Performance: A Survey, *Knowl. -Based Syst.* (2018).

[53] H.Cai, V.W.Zheng and K.C.-C.Chang, A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications, *TKDE* (2018).

[54] Q.Wang, Z.Mao, B.Wang and L.Guo, Knowledge Graph Embedding: A Survey of Approaches and Applications, *TKDE* (2017).

[55] Y.Lin, Z.Liu and M.Sun, Knowledge Representation Learning with Entities, Attributes and Relations, *ethnicity* (2016).

[56] H.Paulheim, Knowledge graph refinement: A survey of approaches and evaluation methods, *Semantic Web* 8(3) (2017),489–508.doi:10.3233/SW-160218.     

```   
  很少有人尝试对 KG 嵌入的技术和应用进行调查[52-54]。该调查[52]基于因子分解，基于随机游动和基于深度学习的网络嵌入方法（例如 DeepWalk，Node2vec 等）进行。[53,54]仅讨论 RESCAL[17] 和 KREAR[55] 作为使用实体属性进行 KG 嵌入的方法，重点讨论了基于结构的嵌入方法，即使用非属性三元组的方法，例如表1中列出的基于翻译的嵌入模型。但是，RESCAL 是一种用于关系学习的矩阵分解方法，该方法将每个对象/数据属性编码为张量的切片，从而导致张量的维数自动增加。如果在生成 KG 嵌入时使用文字，则该方法会遇到效率问题。同样，KREAR 仅考虑具有分类值（即固定数量的值）的那些数据属性，而忽略那些将任何随机文字作为值的数据属性。最近的一项调查[56]总结了迄今为止提出的提炼 KGs 的方法。然而，此调查并没有局限于嵌入技术，也没有考虑大多数利用文本的方法。
  
[17] M.Nickel, V.Tresp和H.-P.Kriegel,《基于多关系数据的集体学习的三向模型》,载于:ICML, 2011年
[52] P.Goyal和E.Ferrara, “图形嵌入技术, 应用程序和性能:调查,知识”, 基于系统.(2018)
[53] H.Cai, V.W.Zheng和K.C.-C.Chang,《图嵌入的综合调查:问题,技术和应用》,TKDE(2018)
[54] Q.Wang, Z.Mao, B.Wang和L.Guo, 知识图嵌入:方法与应用概述, TKDE(2017)
[55] Y.Lin, Z.Liu和M.Sun, 带有实体, 属性和关系的知识表示学习, 种族(2016)
[56] H.Paulheim, 知识图细化:方法和评估方法概述, 语义网8(3)(2017), 489-508.doi:10.3233/SW-160218
```

   None of the surveys mentioned above include all the existing KG embedding models which make use of literals, such as the ones categorized as models incorporating information represented in literals in Table 1. To the best of our knowledge, this is the first attempt to analyse the algorithms proposed so far for generating KG embeddings using literals. In this paper, discussions on the type of literals, the embedding approaches, and the applications/tasks on which the embedding models are evaluated are given. A categorization of the models based on the type of literals they use is also provided.

```
  上述调查均未包括所有利用文字的现有 KG 嵌入模型，例如被归类为包含表 1 所示文字信息的模型。据我们所知，这是首次尝试分析迄今为止提出的使用文字生成 KG 嵌入的算法。在本文中，讨论了文字类型，嵌入方法以及评估嵌入模型的应用程序/任务。还提供了基于模型使用的文字类型的分类。
```

​                                           Table 1  KG embedding models and their categories.

|            Categories             |                            Models                            |
| :-------------------------------: | :----------------------------------------------------------: |
|   Translational Distance Models   | TransE ==[10]== and its extensions: TransH==[12]== TransR==[13]==, TransD==[14]==, TranSparse==[15]==, TransA==[16]==etc. |
|     Semantic Matching Models      | RESCAL==[17]== and Its Extensions: DistMult==[18]==, HolE==[19]==, ComplEx==[20]==, and etc. Semantic Matching with Neural Networks: SME==[21]==, NTN==[22]==, MLP==[23]==, and etc. |
|     Models using Entity Types     | Extended RESCAL==[24]==, SSE==[25]==, TKRL==[26]==, Type constrained representation learning==[27]==, Rules incorporated KG completion models==[28]==, TRESCAL==[29]==, Entity Hierarchy Embedding==[30]== |
|    Models using Relation Paths    | PTransE==[31]==, Traversing KGs in Vector Space==[32]==, RTRANSE==[33]==, Compositional vector space==[34]==, Reasoning using RNN==[35]==, Context-dependent KG embedding==[36]== |
|    Models using Logical Rules     | Rules incorporated KG completion models==[28]==, Large-scale Knowledge Base Completion==[37]==, KALE==[38]==, Logical Background Knowledge for Relation Extraction==[39]==, and etc. |
| Models using Temporal Information | Time-Aware Link Prediction==[40]==, coevolution of event and KGs[41], Knowevolve==[42]== |
|   Models using Graph Structures   | GAKE==[43]==, Link Prediction in Multi-relational Graphs==[44]== |
|       Models using Literals       | LiteralE==[45]==, TransEA==[46]==, KBLRN==[47]==, MTKGNN==[48]==, MKBE==[49]==, KDCoE==[50]==, DKRL==[11]==, IKRL==[51]==, and etc. |

[12] Z.Wang, J. Zhang, J.Feng and Z.Chen, Knowledge Graph Embedding by Translating on Hyper planes, 2014.https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8531.

[13] Y. Lin, Z. Liu, M.Sun, Y.Liu and X.Zhu, Learning Entity and Relation Embeddings for Knowledge Graph Completion, 2015. https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571.

[14] G.Ji, S.He, L.Xu, K.Liu and J.Zhao, Knowledge Graph Embedding via Dynamic Mapping Matrix, 2015, pp. 687–696.doi:10.3115/v1/P15-1067.

[15] G.Ji, K.Liu, S.He and J.Zhao, Knowledge Graph Completion with Adaptive Sparse Transfer Matrix, 2016. https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11982.

[16] Y.Jia, Y.Wang, H.Lin, X.Jin and X.Cheng, Locally Adaptive Translation for Knowledge Graph Embedding, in: *Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence*, AAAI’16, AAAI Press, 2016, pp. 992–998. http://dl.acm.org/citation.cfm?id=3015812.3015960.

[18] B.Yang, W.Yih, X.He, J.Gao and L. Deng, Embedding Entities and Relations for Learning and Inference in Knowledge Bases, in: *3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*, 2015. http://arxiv.org/abs/1412.6575.

[19] M.Nickel, L.Rosasco and T.Poggio, Holographic Embeddings of Knowledge Graphs, in: *Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence*, AAAI’16, AAAI Press, 2016, pp.1955–1961. http://dl.acm.org/citation.cfm?id=3016100.3016172.

[20] T.Trouillon, J.Welbl, S.Riedel, E.Gaussier and G.Bouchard, Complex Embeddings for Simple Link Prediction, in: *Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48*, ICML’16, JMLR.org, 2016, pp. 2071–2080. http://dl.acm.org/citation.cfm?id=3045390.3045609.

[21] A.Bordes, X.Glorot, J.Weston and Y.Bengio, A semantic matching energy function for learning with multirelational data, *Machine Learning* 94(2) (2014), 233–259.doi:10.1007/s10994-013-5363-6.

[22] R.Socher, D.Chen, C.D.Manning and A.Ng, Reasoning With Neural Tensor Networks for Knowledge Base Completion, in: *Advances in Neural Information Processing Systems 26*, C.J.C.Burges, L.Bottou, M.Welling, Z.Ghahramani and K.Q.Weinberger, eds, Curran Associates, Inc., 2013, pp. 926–934.

[23] X.Dong, E.Gabrilovich, G.Heitz, W.Horn, N.Lao, K.Murphy, T.Strohmann, S.Sun and W.Zhang, Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion, in: *Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, KDD ’14, ACM, New York, NY, USA, 2014, pp. 601–610. ISBN 978-1-4503-2956-9. doi:10.1145/2623330.2623623.

[24] M.Nickel, V.Tresp and H.-P.Kriegel, Factorizing Yago: Scalable Machine Learning for Linked Data, in: *Proceedings of the 21st international conference on World Wide Web*, ACM, 2012.

[25] S.Guo, Q.Wang, B.Wang, L.Wang and L.Guo, SSE: Semantically Smooth Embedding for Knowledge Graphs, *IEEE Transactions on Knowledge and Data Engineering* PP (2016),1–1.doi:10.1109/TKDE.2016.2638425.

[26] R.Xie, Z.Liu and M.Sun, Representation Learning of Knowledge Graphs with Hierarchical Types, in: *IJCAI*, 2016.

[27] D.Krompa β, S.Baier and V.Tresp, Type-Constrained Representation Learning in Knowledge Graphs, in: *Proceedings of the 14th International Conference on The Semantic Web ISWC 2015 - Volume 9366*, Springer-Verlag, Berlin, Heidelberg, 2015, pp.640–655.ISBN 978-3-319-25006-9.

[28] Q.Wang, B.Wang and L.Guo, Knowledge Base Completion Using Embeddings and Rules, in: *Proceedings of the 24th International Conference on Artificial Intelligence*, IJCAI’15, AAAI Press, 2015, pp. 1859–1865. ISBN 978-1-57735-738-4.http://dl.acm.org/citation.cfm?id=2832415.2832507.

[29] K.-W.Chang, W.-t.Yih, B.Yang and C.Meek, Typed Tensor Decomposition of Knowledge Bases for Relation Extraction, in: *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, Association for Computational Linguistics, Doha, Qatar, 2014, pp. 1568–1579. doi:10.3115/v1/D14-1165. https://www.aclweb.org/anthology/D14-1165.

[30] Z.Hu, P.Huang, Y.Deng, Y.Gao and E.Xing, Entity Hierarchy Embedding, in: *Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, Association for Computational Linguistics, Beijing, China, 2015, pp.1292–1300.doi:10.3115/v1/P15-1125. https://www.aclweb.org/anthology/P15-1125.

[31] Y.Lin, Z.Liu and M.Sun, Modeling Relation Paths for Representation Learning of Knowledge Bases, in: *EMNLP*, 2015.

[32] K.Guu, J.Miller and P.Liang, Traversing Knowledge Graphs in Vector Space, in: *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, Association for Computational Linguistics, Lisbon, Portugal, 2015, pp.318–327.doi:10.18653/v1/D15-1038. https://www.aclweb.org/anthology/D15-1038.

[33] A.García-Durán, A.Bordes and N.Usunier, Composing Relationships with Translations, in: *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, Association for Computational Linguistics, Lisbon, Portugal, 2015, pp.286–290.doi:10.18653/v1/D15-1034. https://www.aclweb.org/anthology/D15-1034.

[34] A.Neelakantan, B.Roth and A.Mccallum, Compositional Vector Space Models for Knowledge Base Completion 1(2015). doi:10.3115/v1/P15-1016.

[35] R.Das, A.Neelakantan, D.Belanger and A.Mccallum, Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks, 2017, pp.132–141.doi:10.18653/v1/E17-1013.

[36] Y.Luo, Q.Wang, B.Wang and L.Guo, Context-Dependent Knowledge Graph Embedding, in: *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, Association for Computational Linguistics, Lisbon, Portugal, 2015, pp.1656–1661.doi:10.18653/v1/D15-1191.https://www.aclweb.org/anthology/D15-1191.

[37] Z.Wei, J.Zhao, K.Liu, Z.Qi, Z.Sun and G.Tian, Largescale Knowledge Base Completion: Inferring via Grounding Network Sampling over Selected Instances, in: *CIKM*, 2015.

[38] S.Guo, Q.Wang, L.Wang, B.Wang and L.Guo, Jointly Embedding Knowledge Graphs and Logical Rules, 2016, pp.192–202.doi:10.18653/v1/D16-1019.

[39] T.Rocktäschel, S.Singh and S.Riedel, Injecting Logical Background Knowledge into Embeddings for Relation Extraction, in: *Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, Association for Computational Linguistics, Denver, Colorado, 2015, pp.1119–1129.doi:10.3115/v1/N15-1118. https://www.aclweb.org/anthology/N15-1118.

[40] T.Jiang, T.Liu, T.Ge, L.Sha, S.Li, B.Chang and Z.Sui, Encoding Temporal Information for Time-Aware Link Prediction, in: *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, Association for Computational Linguistics, Austin, Texas, 2016, pp.2350–2354.doi:10.18653/v1/D16-1260. https://www.aclweb.org/anthology/D16-1260.

[41] C.Esteban, V.Tresp, Y.Yang, S.Baier and D.KrompaÃ§, Predicting the co-evolution of event and Knowledge Graphs, in: *2016 19th International Conference on Iformation Fusion (FUSION)*, 2016, pp.98–105.

[42] R.Trivedi, H.Dai, Y.Wang and L.Song, Know-evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs, in: *Proceedings of the 34th International Conference on Machine Learning - Volume 70*, ICML’17, JMLR.org, 2017, pp.3462–3471.http://dl.acm.org/citation.cfm?id=3305890.3306039.

[43] J.Feng, M.Huang, Y.Yang and X.Zhu, GAKE: Graph Aware Knowledge Embedding, in: *Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers*, The COLING 2016 Organizing Committee, Osaka, Japan, 2016, pp.641–651.https://www.aclweb.org/anthology/C16-1062.

[44] X.Jiang, V.Tresp, Y.Huang and M.Nickel, Link Prediction in Multi-relational Graphs Using Additive Models, in: *Proceedings of the 2012 International Conference on Semantic Technologies Meet Recommender Systems & Big Data-Volume 919*, SeRSy’12, CEUR-WS.org, Aachen, Germany, Germany, 2012, pp.1–12.http://dl.acm.org/citation.cfm?id=2887638.2887639.

[45] A.Kristiadi, M.A.Khan, D.Lukovnikov, J.Lehmann and A.Fischer, Incorporating Literals into Knowledge Graph Embeddings, in: *ISWC2019*, 2019.

[46] Y.Wu and Z.Wang, Knowledge Graph Embedding with Numeric Attributes of Entities, in: *Rep4NLP@ACL*, 2018

[47] A.García-Durán and M.Niepert, KBlrn: End-to-End Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features, in: *UAI*, 2018.

[48] Y.Tay, A.T.Luu, M.C.Phan and S.C.Hui, Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs, *CoRR* (2017).

[49] P.Pezeshkpour, L.Chen and S.Singh, Embedding Multimodal Relational Data for Knowledge Base Completion, in: *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, 2018, pp.3208–3218.

[50] M.Chen, Y.Tian, K.W.Chang, S.Skiena and C.Zaniolo, Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-Lingual Entity Alignment, *arXiv preprint arXiv:1806.06478* (2018).

[51] R.Xie, Z.Liu, T.-S.Chua, H.-B.Luan and M.Sun, Imageembodied Knowledge Representation Learning, in: *IJCAI*,2017.

​                                                                       Table 1  KG 嵌入模型及其类别

|        类别        |                             模型                             |
| :----------------: | :----------------------------------------------------------: |
|    平移距离模型    | TransE==[10]== 及其扩展名：TransH==[12]== TransR==[13]==，TransD==[14]==，TranSparse==[15]==，TransA==[16]== 等。 |
|    语义匹配模型    | RESCAL==[17]==及其扩展名：DistMult==[18]==，HolE==[19]==，ComplEx==[20]==等。神经网络的语义匹配：SME==[21]==，NTN==[22]==，MLP==[23]==等。 |
| 使用实体类型的模型 | 扩展 RESCAL==[24]==，SSE==[25]==，TKRL==[26]==，类型约束表示学习==[27]==，包含 KG 完成模型的规则==[28]==，TRESCAL==[29]==，实体层次结构嵌入==[30]== |
| 使用关系路径的模型 | PTransE==[31]==，在向量空间中遍历 KG==[32]==，RTRANSE==[33]==，成分向量空间==[34]==，使用 RNN 进行推理==[35]==，上下文相关的 KG 嵌入==[36]== |
| 使用逻辑规则的模型 | 包含 KG 完成模型的规则==[28]==，大规模知识库完成[37]==，KALE==[38]==，用于关系提取的逻辑背景知识==[39]==等。 |
| 使用时间信息的模型 | 时间感知链路预测==[40]==，事件和 KG 的协同进化==[41]==，Knowevolve==[42]== |
|  使用图结构的模型  |          GAKE==[43]==，多关系图中的链接预测==[44]==          |
|   使用文字的模型   | LiteralE==[45]==，TransEA==[46]==，KBLRN==[47]==，MTKGNN==[48]==，MKBE==[49]==，KDCoE==[50]==，DKRL==[11]==，IKRL==[51]==等。 |

```
[12] Z.Wang, J.Zhang, J.Feng和Z.Chen,通过在超平面上进行翻译来嵌入知识图,2014 年.https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8531
[13] Y. Lin, Z. Liu, M.Sun, Y.Liu和X.Zhu, 《用于知识图完成的学习实体和关系嵌入》, 2015年, https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571
[14] G.Ji, S.He, L.Xu, K.Liu和J.Zhao, 通过动态映射矩阵嵌入知识图, 2015年, 第687–696页.doi:10.3115/v1/P15-1067
[15] G.Ji, K.Liu, S.He和J.Zhao,《具有自适应稀疏转移矩阵的知识图完成》, 2016年.https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11982
[16] Y.Jia, Y.Wang, H.Lin, X.Jin和X.Cheng, 知识图嵌入的局部自适应翻译, 载于:第三十届AAAI人工智能会议论文集, AAAI'16, AAAI出版社, 2016年, 第992-998页.http://dl.acm.org/citation.cfm?id=3015812.3015960
[18] B.Yang, W.Yih, X.He, J.Gao和L.Deng, "在知识基础中嵌入实体和关系以进行学习和推理", 第3届国际学习代表大会, ICLR, 2015年, 圣地亚哥, 美国加利福尼亚州, 2015年5月7日至9 日, 会议记录, 2015 年.http://arxiv.org/abs/1412.6575
[19] M.Nickel, L.Rosasco 和 T.Poggio, 知识图的全息嵌入, 载于:第三十届AAAI人工智能会议论文集, AAAI’16, AAAI 出版社, 2016年, 第1955–1961页.http://dl.acm.org/citation.cfm?id=3016100.3016172
[20] T.Trouillon, J.Welbl, S.Riedel, E.Gaussier和G.Bouchard, "用于简单链接预测的复杂嵌入", 载于:第33届国际机器学习国际会议论文集-第48卷, ICML, 16, JMLR.org, 2016年, 第2071-2080页. http://dl.acm.org/citation.cfm?id=3045390.3045609
[21] A.Bordes, X.Glorot, J.Weston和Y.Bengio, 一种用于多关系数据学习的语义匹配能量函数, 机器学习94(2)(2014), 233-259.doi:10.1007/s10994-013-5363-6
[22] R.Socher, D.Chen, C.D.Manning和A.Ng, 使用神经张量网络推理完成知识库, 载于:神经信息处理系统的进展26, C.J.C.Burges, L.Bottou, M.Welling, Z.Ghahramani和K.Q.Weinberger编辑, Curran Associates, Inc., 2013年, 第926–934页
[23] X.Dong, E.Gabrilovich, G.Heitz, W.Horn, N.Lao, K.Murphy, T.Strohmann, S.Sun和W.Zhang, 知识库:概率论的网络规模方法Fusion, 载于:第20届ACM SIGKDD知识发现和数据挖掘国际会议论文集, KDD'14, ACM, 纽约, 美国, 2014年, 第601-610页.ISBN978-1-4503-2956-9.doi:10.1145/2623330.2623623
[24] M.Nickel, V.Tresp和H.-P.Kriegel, 《分解Yago:链接数据的可伸缩机器学习》, 载于:第21届万维网国际会议论文集, ACM, 2012 年
[25] S.Guo, Q.Wang, B.Wang, L.Wang和L.Guo, SSE:知识图的语义平滑嵌入, IEEE知识与数据工程学报, PP(2016年), 1-1.doi:10.1109/TKDE.2016.2638425
[26] R.Xie, Z.Liu和M.Sun, “具有分层类型的知识图的表示学习”, 载于:IJCAI, 2016年
[27] D.Krompa β, S.Baier和V.Tresp, 知识图的类型约束表示学习, 载于:第14届语义网ISWC国际会议论文集-第9366卷, 施普林格出版社, 柏林, 海德堡, 2015年, 第640-655页.ISBN978-3-319-25006-9
[28] Q.Wang, B.Wang 和 L.Guo, 《使用嵌入和规则完成知识库》, 载于:《第24届国际人工智能会议论文集》, IJCAI’15, AAAI出版社, 2015年, 第1859-1865页.ISBN978-1-57735-7384.http://dl.acm.org/citation.cfm?id=2832415.2832507
[29] K.-W.Chang, W.-t.Yih, B.Yang和C.Meek, “用于关系提取的知识库的类型张量分解”, 载于:2014年自然语言处理中的经验方法会议论文集(EMNLP), 计算语言学协会, 多哈, 卡塔尔, 2014年, 第1568-1579页. doi:10.3115/v1/D14-1165.https://www.aclweb.org/anthology/D14-1165
[30] Z.Hu, P.Huang, Y.Deng, Y.Gao和E.Xing, 实体层次结构嵌入, 载于:计算语言学协会第53届年会和第七届国际自然语言联合会议论文集处理(第1卷:长论文), 计算语言学协会, 北京, 中国, 2015年, 第1292-1300 页.doi:10.3115/v1/P15-1125.https://www.aclweb.org/anthology/P15-1125
[31] Y.Lin, Z.Liu和M.Sun, “知识库表示学习的关系路径建模”, EMNLP, 2015年
[32] K.Guu, J.Miller和P.Liang, 在向量空间中遍历知识图谱, 载于:《2015年自然语言处理经验方法会议论文集》, 计算语言学协会, 里斯本, 葡萄牙, 2015年, 第11页.318–327.doi:10.18653/v1/D15-1038 https://www.aclweb.org/anthology/D15-1038
[33] A.García-Durán, A.Bordes和N.Usunier, 《与翻译的关系》, 载于:《2015年自然语言处理经验方法会议论文集》, 计算语言学协会, 葡萄牙里斯本, 2015年, 第11页.286–290.doi:10.18653/v1/D15-1034 https://www.aclweb.org/anthology/D15-1034
[34] A.Neelakantan, B.Roth和A.Mccallum, 知识库完成的成分向量空间模型1(2015).doi:10.3115 /v1/P15-1016
[35] R.Das, A.Neelakantan, D.Belanger和A.Mccallum, 使用递归神经网络的实体, 关系和文本推理链, 2017年, 第132–141.doi:10.18653/v1/E17-1013
[36] Y.Luo, Q.Wang, B.Wang和L.Guo, 上下文相关的知识图嵌入, 载于:《自然语言处理经验方法的会议纪要》, 计算语言学协会, 里斯本, 葡萄牙, 2015年, 第1656–1661页.doi:10.18653/v1/D15-1191.https://www.aclweb.org/anthology/D15-1191
[37] Z.Wei, J.Zhao, K.Liu, Z.Qi, Z.Sun和G.Tian, 大型知识库完成:通过对选定实例进行接地网络采样推断, 载于:CIKM, 2015年
[38] S.Guo, Q.Wang, L.Wang, B.Wang和L.Guo, 联合嵌入知识图和逻辑规则, 2016年, 第192–202.doi:10.18653/v1/D16-1019
[39] T.Rocktäschel, S.Singh和S.Riedel, 将逻辑背景知识注入嵌入关系中以进行关系提取, 载于:计算语言学协会北美分会2015年会议论文集:人类语言技术, 计算语言学, 科罗拉多州丹佛, 2015年, 第1119-1129 页.doi:10.3115/v1/N15-1118.https://www.aclweb.org/anthology/N15-1118
[40] T.Jiang, T.Liu, T.Ge, L.Sha, S.Li, B.Chang和Z.Sui, 编码时间信息以进行时间感知的链接预测, 载于:2016年经验会议论文集《自然语言处理中的方法》, 计算语言学协会, 德克萨斯州奥斯汀, 2016年, 第 2350–2354.doi:10.18653/v1/D16-1260.https://www.aclweb.org/anthology/D16-1260
[41] C.Esteban, V.Tresp, Y.Yang, S.Baier和D.KrompaÃ§, 预测事件和知识图的共同演化, 载于:2016 年第19届国际信息融合会议(FUSION), 2016年, 第98–105页
[42] R.Trivedi, H.Dai, Y.Wang和L.Song, 《知识进化:动态知识图的深度时间推理》, 载于:《第34届机器学习国际会议论文集》, 第70卷, ICML'17,  JMLR.org, 2017年, 第3462-3471页.http://dl.acm.org/citation.cfm?id = 3305890.3306039
[43] J.Feng, M.Huang, Y.Yang和X.Zhu, GAKE:Graph Aware Knowledge Embedding, 载于:COLING 2016会议录中, 第26届国际计算语言学会议:技术论文, COLING2016组织委员会, 日本大阪, 2016年, 第641-651页.https://www.aclweb.org/anthology/C16-1062
[44] X.Jiang, V.Tresp, Y.Huang和M.Nickel, 使用加性模型的多关系图中的链接预测, 载于:2012年语义技术国际会议论文集与推荐系统和大数据量919会合, SeRSy'12, CEUR-WS.org, 德国亚琛, 德国, 2012年, 第1–12页.http://dl.acm.org/citation.cfm?id=2887638.2887639
[45] A.Kristiadi, M.A.Khan, D.Lukovnikov, J.Lehmann和A.Fischer, 《将文字融入知识图嵌入》, 载于:ISWC2019, 2019
[46] Y.Wu和Z.Wang, 知识图嵌入实体的数字属性, 载于:Rep4NLP@ACL, 2018年
[47] A.García-Durán和M.Niepert, KBlrn:具有潜在关系和数值特征的知识库表示的端到端学习, UAI, 2018年
[48] Y.Tay, A.T.Luu, M.C.Phan和S.C.Hui, 用于知识图中非离散属性预测的多任务神经网络, CoRR(2017 年)
[49] P.Pezeshkpour, L.Chen和S.Singh, 《嵌入多峰关系数据以完成知识库》, 载于:《2018年自然语言处理经验方法会议》(EMNLP)会议录, 2018年, 第3208–3218页
[50] M.Chen, Y.Tian, K.-W.Chang, S.Skiena和C.Zaniolo, 知识图和跨语言实体对齐的实体描述的共同训练嵌入, arXiv预印本arXiv:1806.06478(2018)
[51] R.Xie, Z.Liu, T.-S.Chua, H.-B.Luan和M.Sun, 图像体现的知识表示学习, 载于:IJCAI, 2017
```

   This survey is an extension of an already published short survey==[57]==. The major difference between the two versions is that (i) this survey contains a much more detailed theoretical analysis of the KG embedding models with literals proposed so far, and (ii) it performs empirical evaluation of the discussed models under the same experimental settings under the example of link prediction.

[57] G.A.Gesese, R.Biswas and H.Sack, A Comprehensive Survey of Knowledge Graph Embeddings with Literals: Techniques and Applications, in: *Proceedings of the Workshop on Deep Learning for Knowledge Graphs (DL4KG2019) Co-located with the 16th Extended Semantic Web Conference 2019 (ESWC 2019), Portoroz, Slovenia, June 2, 2019.*, 2019, pp. 31–40.http://ceur-ws.org/Vol-2377/paper_4.pdf.

```     
  这个调查[57]是一个已经发表的简短调查的延伸。两个版本之间的主要区别在于: (1) 本调查包含了迄今为止提出的有文字的 KG 嵌入模型的更详细的理论分析，(2) 在相同的实验环境下，以链接预测为例对所讨论的模型进行了实证评价。

[57] GAGesese, R.Biswas和H.Sack,《对具有文字的知识图嵌入的全面调查:技术与应用》, 来自:与第16届扩展语义网络会议同期举办的知识图深度学习研讨会(DL4KG2019)的论文集 2019(ESWC 2019), 斯洛文尼亚 Portoroz, 2019年6月2日, 2019, 第31–40页.http://ceur-ws.org/Vol-2377/paper_4.pdf
```

## 3. Problem Formulation

``` 
问题表述
```

   This section briefly introduces the fundamentals of KGs and KG embeddings followed by a formal definition of KG embeddings with literals. It also poses various research questions about why conducting this study is a stepping stone for future development.

```   
  本节简要介绍 KGs 和 KG 嵌入的基本原理，然后给出带有文字的 KG 嵌入的正式定义。 这也提出了各种研究问题，为什么进行这项研究是未来发展的踏脚石。
```

### 3.1. *Preliminaries*

```   
准备工作
```

   **Knowledge Graphs.** Knowledge Graphs (KGs) consist of a set of triples *K* ⊆ *E* × *R* × (*E* ∪ *L*), where *E* is a set of resources referred to as entities, *L* a set of literals, and *R* a set of relations. An entity is identified by a URI which represents a real-world object or an abstract concept. A relation (or property) is a binary predicate and a literal is a string, date, or number eventually followed by its data type. For a triple <x, r, y>, x is a subject, r is a relation and y is an object. The subject and object are often referred to as *head* and *tail* entity respectively. The triples consisting of literals as objects are often referred to as *attributive triples*.

```   
  知识图. 知识图（KG）由一组三元组 K⊆E×R×(E∪L) 组成，其中 E 是一组称为实体的资源，L 是一组文字，而 R 是一组关系。实体由表示实际对象或抽象概念的 URI 标识。关系（或属性）是一个二进制谓词，而文字是一个字符串，日期或数字，后跟它的数据类型。对于三元组 <x，r，y>，x 是对象，r 是关系，y 是对象。主体和客体通常分别称为头和尾实体。由文字作为对象的三元组通常称为定语三元组。
```

**Relations (or Properties)** :

```   
关系（或属性）
```

   Based on the nature of the objects, relations are classified into two main categories:

``` 
  根据对象的性质，关系可分为两大类：
```

- **Object Relation** links an entity to another entity. E.g., in the triple
  <dbr:Albert_Einstein, dbo:field, dbr:Physics>, both dbr:Albert_Einstein and dbr:Physics are entities, the relation dbo:field is an *Object Relation*.

- **Data Type Relation** links an entity to its values, i.e., literals. For example, in
  <dbr:Albert_Einstein, dbo:birthDate, "1879-03-14">, where "1879-03-14" is a literal value, the relation dbo:birthDate is a *Data Type Relation*.

  ```
  - 对象关系将一个实体链接到另一个实体。 例如，在三元组 <dbr:Albert_Einstein, dbo:field, dbr:Physics>, dbr:Albert_Einstein和dbr:Physics 都是实体，dbo:field 是对象关系。
  - 数据类型关系将实体链接到其值，即文字。 例如，在 <dbr:Albert_Einstein, dbo:birthDate, "1879-03-14"> 中，其中 "1879-03-14" 是文字值，关系 dbo:birthDate 是数据类型关系。
  ```

-    

### 3.2. *Types of Literals*

```   
文字类型
```

   Literals in a KG encode additional information which is not captured by the entities or relations. There are different types of literals present in the KGs:

```   
  KG 中的文字对实体或关系未捕获的其他信息进行编码。 KG 中存在不同类型的文字：
```

* **Text Literals:** A wide variety of information can be stored in KGs in the form of free text such as names, labels, titles, descriptions, comments, etc. In most of the KG embedding models with literals, text information is further categorized into **Short text** and **Long text**. The literals which are fairly short such as for relation like names, titles, labels etc. are considered as *Short text*. On the other hand, for strings that are much longer such as descriptions of entities, comments, etc. are considered as *Long text* and are usually provided in natural language.

* **Numeric Literals:** Information encoded as integers, float and so on such as height, date, population, etc. also provide useful information about an entity. It is worth considering the numbers as distinct entities in the embedding models, as it has its own semantics to be covered which cannot be covered by string distance metrics. For instance, 777 is more similar to 788 than 77.

* **Units of Measurement:** Numeric literals often denote units of measurements to a definite magnitude. For example, Wikidata property wdt:P2048 takes values in mm, cm, m, km, inch, foot and pixel. Hence, discarding the units and considering only the numeric values without normalization results in loss of semantics, especially if units are not comparable, e.g., units of length and units of weight.

* **Image Literals:** Images also provide latent useful information for modelling the entities. For example, a person’s details such as age, gender, etc. can be deduced via visual analysis of an image depicting the person.

* **Other Types of Literals:** Useful information encoded in the form of other literals such as external URIs which could lead to an image, text, audio or video files.

  ```   
  - 文本文字：KG 可以以自由文字的形式储存各种各样的信息，例如名称，标签，标题，描述，注释等。在大多数带有文字的 KG 嵌入模型中，文本信息进一步分类为 Short 文本和 Long 文本。 诸如名称，标题，标签等关系之类的相当短的文字被视为 Short 文本。 另一方面，对于更长的字符串，例如实体描述，注释等，被视为Long 文本，通常以自然语言提供。 
  - 数字文字：以整数、浮点数等形式编码的信息，例如高度，日期，人口等，也提供有关实体的有用信息。在嵌入模型中，数字作为不同的实体是值得考虑的，因为它有自己的语义，不能被字符串距离度量所覆盖。 例如，777 更类似于 788 而不是 77。
  - 度量单位：数字字面值通常表示测量的单位到一个确定的大小。 例如，维基数据属性 wdt: P2048 采用 mm，cm，m，km，inch，foot 和 pixel 的值。 因此，丢弃单位而只考虑数值而不进行规范化会导致语义损失，特别是当单位不可比时，例如长度单位和重量单位。
  - 图像文字：图像还为实体建模提供了潜在的有用信息。 例如，可以通过对描绘此人的图像进行视觉分析来推断其详细信息，例如年龄，性别等。
  - 其他类型的文字：以其他文字形式编码的有用信息，例如外部 URI，可能会链接到一个图像，文本，音频或视频文件。
  ```

### 3.3. *Research Questions*

```
研究问题
```

   As it can be seen from the above discussion that the information represented in the KGs is diverse, modelling these entities is a challenging task. The challenges which are further targeted in this study are given as follows:

```  
  从以上讨论可以看出，KG 中表示的信息是多种多样的，对这些实体进行建模是一项艰巨的任务。这项研究进一步针对的挑战如下：
```

* **RQ1** –*How can structured (triples with object relations) and unstructured information (attributive triples) in the KGs be combined into the representation learning?*

* **RQ2** –*How can the heterogeneity of the types of literals present in the KGs be captured and combined into representation learning?*

  ``` 
  - 研究问题1 -如何将 KGs 中的结构化 (带对象关系的三元组) 和非结构化信息 (属性三元组) 结合到表征学习中？
  - 研究问题2 -KGs 中存在的文字类型的异质性如何被捕捉并结合到表征学习中？
  ```

## 4. Knowledge Graph Embeddings with Literals

```
带文字的知识图嵌入
```

   This section investigates KG embedding models with literals divided into the following different categories based on the types of literals utilized: (i) Text, (ii) Numeric, (iii) Image, and (iv) Multi-modal. A KG embedding model which makes use of at least two types of literals providing complementary information is considered as multi-modal. In the subsequent sections, a description of the models for each of the previously described categories analyzing their similarities and differences, followed by a discussion of potential drawbacks are provided.

```   
  本节研究 KG 嵌入模型，根据文本类型将文本分为以下不同类别: (i) 文本，(ii) 数字，(iii) 图像，和 (iv) 多模式。一个使用至少两类文本提供互补信息的 KG 嵌入模型被认为是多模式的。在随后的章节中，对前述的每个类别的模型进行描述，分析了它们的相似性和差异，然后讨论了潜在的缺点。
```

### 4.1. *Models with Text Literals*

```
具有文本文字的模型
```

   In this section, four KG embedding models utilizing text literals are discussed, namely, Extended RESCAL==[24]==, DKRL==[11]==, KDCoE==[50]==, and KGloVe with literals==[58]==. First, a detailed description of these models is given followed by a summary presenting the comparison of the models and their drawbacks.

[58] M.Cochez, M.Garofalo, J.Lenßen and M.A.Pellegrino, A First Experiment on Including Text Literals in KGloVe, *arXiv preprint arXiv:1807.11761* (2018).  

```  
  在本节中，我们将讨论四种利用文本文字的 KG 嵌入模型，即带有文字的扩展 RESCAL[24]，DKRL[11]，KDCoE[50] 和 KGloVe[58]。 首先，对这些模型进行了详细描述，然后进行了总结，介绍了模型的比较及其缺点。

[58] M.Cochez, M.Garofalo, J.Lenßen和M.A.Pellegrino, 《将文本文字包含在KGloVe中的第一个实验》, arXiv预印本arXiv:1807.11761(2018)
```

**Extended RESCAL** aims to improve the original RESCAL approach by extending its algorithm to process literal values more efficiently and to deal with the drawback of sparsity that accompanies tensors. In the original RESCAL approach, relational data is modeled as a three-way tensor X of size *n* *×* *n* *×* *m*, where *n* is the number of entities and *m* is the number of relations. An entry *X~ijk~* = 1 denotes the existence of the triple with i-th entity as a subject, k-th relation as a predicate, and j-th entity as an object. If *X~ijk~* is set to 0, it indicates that the triple doesn’t exist. A new approach for tensor factorization is proposed which is performed on X. For further details refer to==[24]==. If attributive triples have to be modeled in such a way, then the literals will be taken as entities even if they cannot occur as subject in the triples. Including literals may lead to an increment in the runtime since a larger tensor has to be factorized.

```   
扩展 RESCAL 旨在通过扩展其算法来更有效地处理文字值并解决张量带来的稀疏性的缺陷，从而改进原始 RESCAL 方法。在原始的 RESCAL 方法中，关系数据被建模为大小为 n×n×m 的三向张量 X，其中 n 是实体数，m 是关系数。xijk = 1 表示存在三元组，其中第 i 个实体作为主语，第 k 个关系作为谓语，第 j 个实体为宾语。如果 Xijk 设置为 0，则表示该三元组不存在。提出了一种在 X 上进行张量分解的新方法。有关更多详细信息，请参见 [24]。如果属性三元组必须以这种方式建模，那么即使它们不能作为主语出现在三元组中，文本也将被视为实体。包含文字可能会导致运行时间增加，因为必须将较大的张量分解。
```

   In contrast to the original algorithm, the extended RESCAL algorithm handles the attributive triples in a separate matrix. The matrix factorization is performed jointly with the tensor factorization of the nonattributive triples. The attributive triples containing only text literals are encoded in an entity-attribute matrix *D* in such a way that the rows are entities and the columns are < *data type relation* , *value* > pairs. Given a triple with a textual data type such as rdfs:label or yago:hasPreferredMeaning, one or more such pairs are created by tokenizing and stemming the text in the object literal. The matrix *D* is then factorized into *D* *≈* *AV* with A and V being the latentcomponent representations of entities and attributes respectively. Despite the advantage that this approach has for handling multi-valued literals, it does not consider the sequence of words of the literal values. Note that Extended RESCAL represents RDF(S) data in such a way that there is no distinction drawn among A-Box and T-Box, i.e., both classes and instances are modeled equally as entities in a tensor. The T-Box is rather taken as soft constraints instead of letting them impose hard constraints on the model.

```   
  与原始算法相反，扩展的 RESCAL 算法在单独的矩阵中处理属性三元组。矩阵分解与非属性三元组的张量分解共同执行。仅包含文本文字的属性三元组以这样的方式在实体属性矩阵 D 中进行编码：行是实体，列是 <数据类型关系，值> 对。对于具有文本数据类型 (如rdfs:label 或 yago:hasPreferredMeaning) 的三元组，可以通过对对象文字中的文本进行标记和截断来创建一个或多个这样的对。然后将矩阵 D 分解为 D≈AV，其中 A 和 V 分别表示实体和属性的潜在成分。尽管此方法具有处理多值文字的优点，但它并未考虑文字值的单词顺序。注意，Extended RESCAL 表示 RDF (s) 数据的方式不区分 A-Box 和 T-Box，也就是说，类和实例都被建模为张量中的实体。T-box 相当于软约束，而不是让它们对模型施加硬约束。
```

   **DKRL** extends TransE==[10]== by utilizing the descriptions of entities. For each entity *e*, two kinds of vector representations are learned, i.e., structure-based *e~s~* and description-based *e~d~*. These two kinds of entity representations are learned simultaneously into the same vector space but not forced to be unified so that novel entities with only descriptions can be represented. In order to achieve this, given a certain triple (*h* , *r*, *t*) the energy function of the DKRL model is defined as:

![Formula(01).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(01).jpg>)

where *h~s~* and *t~s~* are the structure-based representations, and *h~d~* and *t~d~* are the description-based representations of their corresponding entities.

```     
  Dkrl 通过利用实体的描述来扩展 TransE[10]。 对于每个实体 e，学习两种矢量表示，即基于结构的 es 和基于描述的 ed。将这两种实体表示同时学习到相同的向量空间中，但不强制统一，只有描述的新实体才能得到表示。为了达到这个目的，给定一个三元组 (h，r，t) ，DKRL 模型的能量函数定义为:
  
其中 hs 和 ts 是基于结构的表示，hd 和 td 是对应实体的基于描述的表示。
```

   In order to learn structure-based representations, the TransE approach is directly applied which considers
the relation in a triple as the translation from the head entity to the tail entity. On the other hand, Continuous Bag of Words (CBOW) and a deep Convolutional Neural Network (CNN) model have been used to generate the description-based representations of the head and tail entities. In case of CBOW, short text is generated from the description based on keywords and their corresponding word embeddings are summed up to generate the entity embedding. In the CNN model, after preprocessing of the description, pretrained word vectors from Wikipedia are provided as input. This CNN model has five layers and after every convolutional layer pooling is applied to decrease the parameter space of CNN and filter noises. Max-pooling is applied for the first pooling and mean pooling for the last one. The activation function used is either tanh or ReLU. The CNN model works better than CBOW because it preserves the sequence of words.

```   
  为了学习基于结构的表示方法，本文直接采用 TransE 方法三元组中的关系，如从头实体到尾实体的转换。另一方面，连续词袋（CBOW）和深层卷积神经网络（CNN）模型已用于生成基于描述的头和尾实体。在使用 CBOW 的情况下，根据关键词描述生成短文本，并对相应的词嵌入进行归纳，生成实体嵌入。在 CNN 模型中，在对描述进行预处理之后，会提供来自 Wikipedia 的经过预训练的单词向量作为输入。该 CNN 模型有五层，并且在应用每个卷积层池之后，可以减少 CNN 的参数空间和滤波器噪声。最大池适用于第一个池，平均池适用于最后一个池。使用的激活函数可以是 tanh 或 ReLU。CNN 模型比 CBOW 更好，因为它保留了单词的顺序。
```

   In order to train DKRL, the following margin-based score function is considered as an objective function    and minimized using a standard back propagation using stochastic gradient descent (SGD)

![Formula(02).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(02).jpg>)

where γ > 0 is a margin hyperparameter, *d* is a dissimilarity function and *T'* is the set of corrupted triples. The representation of the entities can be either structure-based or description-based.

```  
  为了训练 DKRL，以下基于边距的得分函数被视为目标函数，并使用标准的反向传播（使用随机梯度下降（SGD））将其最小化其中 γ> 0 是余量超参数，d 是相异函数，T' 是损坏的三元组。实体的表示可以基于结构或基于描述。
```

**KDCoE** focuses on the creation of an alignment between entities of multilingual KGs by creating new Inter-Lingual Links (ILLs) based on an embedding approach which exploits entity descriptions. The model uses a weakly aligned multilingual KG for semi-supervised cross-lingual learning. It performs co-training of a multilingual KG embedding Model (KGEM) and a multilingual entity Description Embedding Model (DEM) iteratively in order for each model to propose a new ILL alternately. KGEM is composed of two components, i.e., a knowledge model and an alignment model, to learn embeddings based on structured information from the KGs (the non attributive triples). Given a set of languages *L*, a separate k1-dimensional embedding space R*^k1^* *~L~* is used for each language *L* ∈ *L* to represent the corresponding relations *R~L~* and entities *E~L~*. In order to learn the embeddings for *R~L~* and *E~L~*, the knowledge model adopts TransE and thus uses hinge loss as its objective function. On the other hand, a linear-transformation-based technique which has the best performance in case of cross-lingual inferences is adopted for the alignment model. This technique employs the following objective function:

![Formula(03).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(03).jpg>)

where *I*(*L~i~*, *L~j~*) is ILLs between the languages *L~i~* and *L~j~*, and *M~ij~* is a *k*~1~ *×* *k*~1~ matrix used as a linear transformation on entity vectors from *L~i~* to *L~j~*.

```   
KDCoE 致力于通过基于利用实体描述的嵌入方法来创建新的语言间链接（ILL），从而在多语言 KG 的实体之间创建对齐方式。该模型使用弱对齐的多语言 KG 进行半监督的跨语言学习。它迭代执行多语言 KG 嵌入模型（KGEM）和多语言实体描述嵌入模型（DEM）的联合训练，以便每个模型交替提出新的 ILL。 KGEM 由知识模型和对齐模型两个部分组成，用于根据来自 KG 的结构化信息（非归因三元组）学习嵌入。给定一组语言 L，每种语言 L∈L 使用单独的 k1 维嵌入空间 Rk1 L 表示对应的关系 RL 和实体 EL。为了学习 RL 和 EL 的嵌入，知识模型采用 TransE，因此将铰链损耗作为其目标函数。另一方面，对准模型采用基于线性变换的技术，该技术在进行跨语言推理时具有最佳性能。该技术采用以下目标函数：

其中 I（Li，Lj）是语言 Li 和 Lj 之间的 ILLs，Mij 是 k1×k1 矩阵，用作从 Li 到 Lj 的实体向量的线性变换。
```

   Let *S~K~* be the hinge loss function used by the knowledge model, the KGEM model then minimizes *S~KG~* = *S~K~* + α*S~A~*, where α is a positive hyperparameter. In case of DEM model, an attentive gated recurrent unit encoder (AGRU) is used to encode the multilingual entity descriptions. DEM applies multilingual word embeddings in order to capture the semantic information of multilingual entity descriptions from the word level. The two models, i.e., KGEM and DEM, are iteratively co-trained in order for each model to propose a new ILL alternately.

```   
  令 SK 为知识模型使用的铰链损失函数，然后 KGEM 模型将 SKG = SK +αSA 最小化，其中 α 为正超参数。对于 DEM 模型，采用一种专注的门控循环单元编码器 (AGRU) 对多语种实体描述进行编码。DEM 应用多语言单词嵌入，以便从单词级别捕获多语言实体描述的语义信息。 迭代地训练两个模型，即 KGEM 和 DEM，以便每个模型交替提出新的 ILL。
```

**KGloVe with literals** is an experimental attempt to incorporate entity descriptions in KGloVe KG embedding approach. The experiment is conducted on DBpedia considering the abstracts and comments of entities as their descriptions. The main goal is to extract named entities from the textual description and for every entity in the text, to replace those words representing it with the entity itself and then take its neighbouring words and entities as its context. The approach works by creating two co-occurrence matrices independently and then by merging them at the end so that a joint embedding can be performed. The first matrix is generated using the same technique as in KGloVe==[59]==, i.e., by performing Personalized PageRank (PPR) on the (weighted) graph followed by the same optimisation used in the GloVe==[60]== approach.



[59] M.Cochez, P.Ristoski, S.P.Ponzetto and H.Paulheim, Global RDF Vector Space Embeddings, in: *International Semantic Web Conference*, Springer, 2017.

[60] J.Pennington, R.Socher and C.Manning, Glove: Gobal Vectors for Word Representation, in: *EMNLP*, 2014.   

```
带文字的 KGloVe 是将实体描述纳入 KGloVe KG 嵌入方法的实验性尝试。 该实验是在 DBpedia 上进行的，将实体的摘要和注释作为其描述。 主要目标是从文本描述中提取命名实体，并为文本中的每个实体提取表示实体的单词，然后将其替换为实体本身，然后将其相邻的单词和实体作为上下文。 该方法的工作原理是独立创建两个共现矩阵，然后在最后合并它们，以便可以执行联合嵌入。 使用与 KGloVe[59] 中相同的技术来生成第一矩阵，即，通过对（加权）图执行个性化 PageRank（PPR），然后进行与 GloVe[60] 方法中相同的优化。

[59] M.Cochez, P.Ristoski, S.P.Ponzetto和H.Paulheim, 全球 RDF 矢量空间嵌入, 载于:国际语义网会议, 施普林格, 2017年
[60] J.Pennington, R.Socher和C.Manning,《手套：用于单词表示的全球载体》, 载于:EMNLP, 2014年
```

   In order to create the second matrix, the Named Entity Recognition (NER) task is performed on the entity description text using the list of entities and predicates of the KG as an input. The NER step employs a simple exact string matching technique which leads to numerous drawbacks such as missing entities due to having different keywords with the same semantics. All the English words that do not match any entity labels are added to the entity-predicate list. Then GloVe co-occurrence for text is applied to the modified text (i.e.,DBpedia abstract and comments)usingtheentitypredicate and word list as input. Finally, the two cooccurrence matrices are summed up together to create a single unified matrix. The proposed approach has been evaluated on classification and regression tasks and the result indicates that for most of the classifiers used, except SVM, the approach does not bring significant improvement to KGloVe. However, the approach can be improved using parameter tuning with extensive experiments.

```     
  为了创建第二个矩阵，使用 KG 的实体和谓词列表作为输入，对实体描述文本执行 “命名实体识别（NER）” 任务。 NER 步骤采用简单的精确字符串匹配技术，这会导致许多缺点，例如由于具有相同语义的不同关键字而导致实体缺失。与任何实体标签都不匹配的所有英语单词都将添加到实体谓词列表。然后使用实体谓词和单词列表作为输入，将文本的 GloVe 共现应用于修改后的文本（即 DBpedia 摘要和注释）。最后，将两个同时出现的矩阵求和，以创建一个统一的矩阵。所提出的方法已经在分类和回归任务上进行了评估，结果表明，除了 SVM 之外，对于大多数使用的分类器，该方法并未对 KGloVe 带来显著改善。但是，可以通过大量实验使用参数调整来改进该方法。
```

**Summary** The basic differences between these models lie in the methods used to exploit the information given in the text literals and combine them with structure-based representation. One major advantage of KDCoE over text literal based embedding models is that it considers descriptions present in multilingual KGs. Also, both DKRL and KDCoE embedding models are designed to perform well for the novel entities, which have only attributive triples in the KGs. The presented approaches with text literals focus mostly on descriptions, which is long natural language text. Other types of text literals, such as, names, labels, titles, etc. are not widely considered.

```
总结这些模型之间的基本区别在于用于利用文本文字中给出的信息并将其与基于结构的表示形式相结合的方法。 与基于文本文字的嵌入模型相比，KDCoE 的一个主要优势是它考虑了多语言 KG 中存在的描述。 此外，DKRL 和 KDCoE 嵌入模型均设计用于在 KG 中仅具有属性三元组的新颖实体上表现良好。 所提出的带有文本文字的方法主要集中于描述，即长自然语言文本。 其他类型的文本文字，例如名称，标签，标题等，并未得到广泛考虑。
```

### 4.2. *Models with Numeric Literals*

```
具有数字文字的模型
```

   In this section, the analysis of the presented KG embedding models which use numeric literals, namely, MT-KGNN==[48]==, KBLRN==[47]==, LiteralE==[45]==, and TransEA==[46]== are presented followed by a summary.

```
  在本节中，将对使用数字文字（即 MT-KGNN[48]，KBLRN[47]，LiteralE[45] 和 TransEA[46]）提出的 KG 嵌入模型进行分析，然后进行总结。
```

**MT-KGNN** is an approach for both relational learning and non-discrete attribute prediction on knowledge graphs in order to learn embeddings for entities, object properties, and data properties. It is composed of two networks, namely, the Relational Network (RelNet) and the Attribute Network (AttrNet). RelNet is a binary (pointwise) classifier for triplet prediction whereas AttrNet is a regression task for attribute value prediction. Given *n*, *m*, and *l* as entity, relation, and literal embedding dimensions respectively, the model passes as an input [*e~i~* , *r~k~* , *e~j~* , *t* ] to RelNet and [*a~i~* , *v~i~* , *a~j~* , *v~j~*] to AttrNet, where *e~i~* , *e~j~*  ∈ *R^n^*, *r~k~* ∈ *R^m^*, *t* is the classification target which is 0 or 1, *a~i~* , *a~j~*  ∈ *R^l^*, and *v~i~* and *v~j~* are normalized continuous values in the interval [0, 1]. Note that the inputs to AttrNet, i.e., [*a~i~*, *v~i~*, *a~j~*, *v~j~*], are taken from attributive triples with non-discrete literal values. An embedding lookup layer is used to retrieve the corresponding vector representations given these inputs as one-hot encoded indices.

```
MT-KGNN 是一种用于知识图上的关系学习和非离散属性预测的方法，目的是学习实体，对象属性和数据属性的嵌入。它由两个网络组成，即关系网络（RelNet）和属性网络（AttrNet）。 RelNet 是用于三重态预测的二进制（逐点）分类器，而 AttrNet 是用于属性值预测的回归任务。分别给定 n，m 和 l 作为实体，关系和文字嵌入维，该模型作为输入 [ei，rk，ej，t] 传递给 RelNet，并作为 [ai，vi，aj，vj] 传递给 AttrNet，其中 ei ，ej∈Rn，rk∈Rm，t 是 0 或 1 的分类目标，ai，aj∈Rl，并且 vi 和 vj 是区间 [0，1] 中的归一化连续值。请注意，AttrNet 的输入，即 [ai，vi，aj，vj] 取自具有非离散文字值的属性三元组。在给定这些输入为一热编码索引的情况下，将使用嵌入查找层来检索相应的矢量表示。
```

   In RelNet, a concatenated triplet is passed through a nonlinear transform and then a sigmoid function is applied to get a linear transform:

```
  在 RelNet 中，串联的三元组通过非线性变换，然后应用 S 型函数来获得线性变换：
```

![Formula(04).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(04).jpg>)

where *w* *∈* *R^h×1^* and *W~d~*  ∈ *R^3n×h^* are parameters of the network. σ, *f*, and *b~rel~* are the sigmoid function, the hyperbolic tangent function tanh, and a scalar bias respectively. RelNet is trained by minimizing the following cross entropy loss function:

```
其中 w∈Rh×1 和 Wd∈R3n×h 是网络的参数。 σ，f 和 brel 分别是 S 型函数，双曲正切函数 tanh 和标量偏差。 通过最小化以下交叉熵损失函数来训练 RelNet：
```

![Formula(05).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(05).jpg>)

where ξ*i* denotes triplet *i* in batch of size *N* and *t~i~* takes the value 0 or 1. In case of AttrNet, two regression tasks are performed, one for the head data properties and another for those of the tail. The following scoring functions are defined for these two tasks:

```
其中 ξi 表示批次 N 中的三元组 i，ti 的取值为 0 或 1。在 AttrNet 的情况下，将执行两项回归任务，一项用于头数据属性，另一项用于尾数据属性。 为这两项任务定义了以下评分函数：
```

![Formula(06)(07).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(06)(07).jpg>)

where *u*, *y* *∈* *R^ha×1^* and *B*, *C* ∈ *R^2n×ha^* are parameters of AttrNet. *h~a~* is the size of the hidden layer and *b~z1~* , *b~z2~* are scalar biases. Each AttrNet is trained by optimizing Mean Squared Error (MSE) loss function:

```
其中，u，y∈Rha×1 和 B，C∈R2n×ha 是 AttrNet 的参数。 ha 是隐藏层的大小，bz1，bz2 是标量偏差。 通过优化均方误差（MSE）损失函数来训练每个 AttrNet：
```

![Formula(08).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(08).jpg>)

The overall loss of the AttrNet is computed by adding the MSE of the head AttrNet and that of the tail AttrNet as follows:

```
Attrnet 的总体损失是通过将其头部的 MSE 和尾部的 MSE 相加计算得到的，具体如下:
```

![Formula(09).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(09).jpg>)

where (*a~i~*)^∗^ , (*a~j~*)^∗^ are the ground truth labels. Finally, the two networks are trained in a multi-task fashion using a shared embedding space.

```
其中(ai)∗，(aj)∗是地面真相标签。 最后，使用共享的嵌入空间以多任务方式训练这两个网络。
```

**KBLRN** works by combining relational (R), latent (L), and numerical (N) features together. The model is designed mainly for the purpose of KG completion. It uses a probabilistic PoE (Product of Experts) method to combine these feature types and train them jointly end to end. Each relational feature is formulated as a logical formula, by adopting the rule mining approach AMIE+ [61], to be evaluated in the KB to compute the feature’s value. The latent features are the ones that are usually generated using an embedding approach such as DistMult. Numerical features are used with the assumption that, for some relation types, the differences between the head and tail can be seen as characteristics for the relation itself. Given a triple *d* = (*h*, *r*, *t*), for each (relation type *r*, and feature type *F* ∈ {*L*, *R*, *N*}) pair, individual experts are defined based on linear models and DistMult embedding method as follows:

```
Kblrn 将关系特征 (r)、潜在特征 (l) 和数值特征 (n) 结合起来。该模型主要是为了完成 KG 而设计的。它使用概率 PoE (专家产品) 方法将这些特征类型组合起来，并对它们进行端到端的联合训练。 通过采用规则挖掘方法 AMIE + [61] ，将每个关系特征表示为一个逻辑公式，在知识库中计算特征值。 潜在功能通常是使用嵌入方法（例如 DistMult）生成的。 在假设某些关系类型使用数字特征的前提下，头和尾之间的差异可以视为关系本身的特征。 给定一个三元组 d =（h，r，t），对于每个（关系类型 r 和特征类型 F∈{L，R，N}）对，基于线性模型和 DistMult 嵌入方法定义各个专家，如下所示：
```

![Formula(10)(11).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(10)(11).jpg>)     

![Formula(12)(13).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(12)(13).jpg>)

where *w~r~*, *w^r^ ~rel~*, *w^r^ ~num~* are the parameter vectors for the latent, relational, and numerical features corresponding to the relation r. Also, * is the element-wise product, *·* is the dot product, and φ is the radial basis function (RBF) applied element-wise to the differences of values *n(h*,*t*) computed as follows:

```
其中 wr，wrrel，wrnum 是与关系 r 相对应的潜在特征、关系特征和数值特征的参数向量，* 是元素乘积，・是点乘，φ 是按元素方式应用于值 n(h,t) 的差的径向基函数（RBF），其计算如下：
```

![Formula(14).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(14).jpg>)

Here, *d~n~* corresponds to the relevant numerical features. A PoE’s probability distribution for a triple *d* =
(*h*, *r*, *t*) is defined as follows:

```
dn 在此对应于相关的数值特征。 三元组 d =（h，r，t）的 PoE 概率分布定义如下：
```

![Formula(15).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(15).jpg>)

where c denotes all possible triples. The parameters of the entity embedding model are shared by all the experts in order to create dependencies between them. In this approach, the PoE are trained with negative sampling and a cross entropy loss to give high probability to observed triples.

```
其中 c 表示所有可能的三元组。 实体嵌入模型的参数由所有专家共享，以便在它们之间创建依赖关系。 在这种方法中，采用负采样和交叉熵损失对 PoE 进行训练，从而使观察到的三元组具有很高的概率。
```

**LiteralE** incorporates literals into existing latent feature models designed for link prediction. In this approach, without loss of generality, the focus lies on incorporating numerical literals into three state-of-the-art embedding methods: DistMult, ComplEx, and ConvE. Given a base model, for instance Distmult, LiteralE modifies the scoring function *f* used in Distmult by replacing the vector representations of the entities *e~i~* in *f* with literal enriched representations *e^lit^ ~i~* . In order to generate *e^lit^ ~i~* , LiteralE uses a learnable transformation function *g* which takes *e~i~* and its corresponding literal vectors *l~i~* as inputs and maps them to a new vector. The function *g* is defined, as shown below, based on the concept of GRU in order to make it flexible, learnable, and capable to decide, if it is beneficial to incorporate the literal information or not:

```
Literale 将文字整合到现有的潜在特征模型中，这些模型是为链接预测而设计的。 在这种方法中，不失一般性的重点在于将数值文本合并到三种最先进的嵌入方法中: DistMult、 ComplEx 和 ConvE。给定一个基本模型（例如 Distmult），LiteralE 修改了 Distmult 使用的计分函数 f，用文字强化表示 elit i 替换 f 中实体 ei 的向量表示。利用可学变换函数 g，以 ei 及其相应的文字向量 li 作为输入，将它们映射到一个新的向量，从而生成新的矢量。 函数 g 的定义，如下所示，基于 GRU 的概念，以使其灵活，可学习，并能够决定，是否有利于纳入文字信息:
```

![Formula(16)(17)(18)(19).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(16)(17)(18)(19).jpg>)

Note that **W**~ze~ ∈ R*^H×H^*,  **W**~zl~ ∈ R*^Nd×H^*, **b** ∈ R*^H^*, and W~h~ ∈ R*^H+Nd×H^* are the parameters of *g*, σ is the sigmoid function,  denotes the element-wise multiplication, and h is a component-wise nonlinearity. The scoring function *f*(**e**~i~, **e**~j~, **r**~k~) has been replaced with *f*(*g*(**e**~i~, **l**~i~), *g*(**e**~j~, **l**~j~), **r**~k~) and trained following the same procedure as in the base model.

```
注意，Wze∈RH×H，Wzl∈RNd×H，b∈RH 和 Wh∈RH + Nd×H 是 g 的参数，σ 是 S 型函数，表示按元素相乘，而 h 是分量 非线性。 得分函数 f（ei，ej，rk）已替换为 f（g（ei，li），g（ej，lj），rk）并按照与基本模型相同的过程进行训练。
```

**TransEA** has two component models; a directly adopted translation-based structure embedding model (i.e., TransE) and a newly proposed attribute embedding model. In the former, the scoring function of a given triple < *h*, *r*, *t* >, is defined as follows:

```
TransEA 具有两个组件模型； 直接采用的基于翻译的结构嵌入模型（即 TransE）和新提出的属性嵌入模型。 在前者中，给定三元组 <h，r，t> 的评分函数定义如下：
```

![Formula(20).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(20).jpg>)

where *||* *x*||~1/2~ denotes either the L1 or L2 norm. The loss function of the structure embedding, for all the relational triplets in the KG, is defined as:

```
其中 || x || 1/2 表示 L1 或 L2 范数。 对于 KG 中的所有关系三元组，结构嵌入的损失函数定义为：
```

![Formula(21).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(21).jpg>)

where *S'* denotes the set of negative triplets constructed by corrupting either the head or the tail entity, [*x*]~+~ = *max*{0, *x*}, and γ > 0 is a margin hyperparameter.

```
其中 S' 表示通过破坏头或尾实体构成的一组三元组，[x] + = max {0，x}，且 γ> 0 是余量超参数。
```

   For the attribute embedding, it uses all attributive triples containing numeric values as input and applies a linear regression model to learn embeddings of entities and attributes. Given an attributive triple < *e*, *a*, *v* >, the scoring function is defined as:

```
对于属性嵌入，它使用包含数值的所有属性三元组作为输入，并应用线性回归模型来学习实体和属性的嵌入。 给定属性三元组 <e，a，v>，评分函数定义为：
```

![Formula(22).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(22).jpg>)

where **a** and **e** are vectors of attribute *a* and entity *e*, *b~a~* is a bias for attribute *a*. On the other hand, given all the attributive triples with numeric values *T*, the loss function for the attributive embedding is computed as:

```
其中 a 和 e 是属性 a 和实体 e 的向量，ba 是属性 a 的偏差。另一方面，给定所有数值为 t 的属性三元组，计算属性嵌入的损失函数如下:
```

![Formula(23).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(23).jpg>)

The main loss function for TransEA (*i*.*e*., *L* = (1 − *α*) · *L~R~* + α *·* *L~A~*) is defined by taking the sum of the respective loss functions of the component models with a hyperparameter to assign a weight for each of the models. Finally, the two models are jointly optimized in the training process by sharing the embeddings of entities.

```
TransEA 的主要损失函数（即，L =（1-α）・LR +α・LA）是通过对具有超参数的组件模型的各个损失函数之和进行定义的，从而为每个模型分配权重 。 最后，通过共享实体的嵌入，在训练过程中共同优化了两个模型。
```

Summary Despite their support for numerical literals, all the embedding methods discussed fail to interpret the semantics behind units/data typed literals. For instance, given the following two triples taken from DBpedia,

```
总结 尽管支持数字文字，但是所有讨论的嵌入方法都无法解释单位/数据类型文字背后的语义。例如，给定从 DBpedia 中取得的以下两个三元组，
```

<http://dbpedia.org/resource/Anton_Baraniak, dbp:weight,"110.0"^^http://dbpedia.org/datatype/kilogram>,
<http://dbpedia.org/resource/Katelin_Snyder, dbp:weight, "110.0"^^http://dbpedia.org/datatype/pound>

the literal value "110.0" from the first triple and the literal value "110.0" from the second triple could be considered exactly the same if the semantics of the types kilogram and pound are ignored. Moreover, most of the models do not have a proper mechanism to handle multi-valued literals.

```
如果忽略公斤和磅类型的语义，则可以认为第一个三元组的文字值 “110.0” 和第二个三元组的文字值 “ 110.0” 完全相同。 而且，大多数模型没有适当的机制来处理多值文字。
```

Regarding model complexity, the number of parameters used in each model is presented in Table 2 to show the complexity in terms of the parameters. It is noted that the complexity of the models depend on the size of the dataset and TransEA has lower complexity as compared to the other models.

```
关于模型复杂度，表 2 中列出了每个模型中使用的参数数量，以显示参数方面的复杂性。 注意，模型的复杂度取决于数据集的大小，与其他模型相比，TransEA 具有较低的复杂度。
```

### 4.3. *Models with Image Literals*

```       
具有图像文字的模型
```

   **IKRL**==[51]== learns embeddings for KGs by jointly training astructure-based representation with an image based representation. The structure-based representation of an entity is learned by adapting a conventional embedding model like TransE. For the image-based representation, given the fact that an entity may have multiple image instances, an image encoder is applied to generate an embedding for each instance of a multivalued image relation. The image encoder consists of a neural representation module and a projection module to extract discriminative features from images and to project these representations from image space to entity space respectively.

```
IKRL[51] 通过联合训练基于结构的表示和基于图像的表示来学习 KGs 的嵌入。 通过改编传统的嵌入模型（如 TransE）来学习实体的基于结构的表示形式。 对于基于图像的表示，给定一个实体可能具有多个图像实例的事实，将图像编码器应用于为多值图像关系的每个实例生成嵌入。 图像编码器由神经表示模块和投影模块组成，以从图像中提取判别特征并将这些表示分别从图像空间投影到实体空间。
```

​                                                                                          Table 2
Complexity of the models with numerical literals in terms of the number of parameters. Θ is the number of parameters in the base model, H is the entity embedding size, *N~d~* is the number of data relations, Λ is the size of the hidden layer in the Attrnet networks of MTKGNN, *N~r~* is the number of relations, and M is attribute embedding size.

|        Model         |         Parameters         |
| :------------------: | :------------------------: |
|   LiteralE with g    | Θ + 2*H*^2^ + 2*N~d~*H + H |
| LiteralE with *glin* |     Θ + (*N~d~*H + H)H     |
|        MTKGNN        |  Θ + *N~d~*H + 2(2HΛ + Λ)  |
|         KBLN         |       Θ + *N~r~N~d~*       |
|       TransEA        |          Θ + *M*           |

```
表2
在参数数量方面，具有数字文字的模型的复杂性。 Θ 是基本模型中的参数数量，H 是实体嵌入大小，Nd 是数据关系数，Λ 是 MTKGNN 的 Attrnet 网络中隐藏层的大小，Nr 是关系数，M  是属性嵌入大小。
```

|         模型         |            参数            |
| :------------------: | :------------------------: |
|   LiteralE with g    | Θ + 2*H*^2^ + 2*N~d~*H + H |
| LiteralE with *glin* |     Θ + (*N~d~*H + H)H     |
|        MTKGNN        |  Θ + *N~d~*H + 2(2HΛ + Λ)  |
|         KBLN         |       Θ + *N~r~N~d*        |
|       TransEA        |          Θ + *M*           |

   For the i-th image, its image-based representation *p~i~* in entity space is computed as:

```
对于第 i 个图像，其在实体空间中基于图像的表示 pi 计算为：
```

![Formula(24).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(24).jpg>)

where *M* ∈ R*^di×ds^* is the projection matrix with *d~i~* and *d~s~* representing the dimension of image features and the dimension of entities respectively. *f*(*img~i~*) is the i-th image feature representation in image space.

```
其中 M∈Rdi×ds 是投影矩阵，其中 di 和 ds 分别代表图像特征的维数和实体的维数。 f（imgi）是图像空间中的第 i 个图像特征表示。
```

   Attention-based multi-instance learning is used to integrate the representations learned for each image instance by automatically calculating the attention that should be given to each instance. The attention for the i-th image representation *p* (*i k* ) of the k-th entity is given as:

```
基于注意力的多实例学习用于通过自动计算应给予每个实例的注意力来集成为每个图像实例学习的表示。 第 k 个实体的第 i 个图像表示 p（i k）的注意事项为：
```

![Formula(25).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(25).jpg>)

where *e^(k)^ ~S~* denotes the structure-based representation of the k-th entity. The higher the attention the more
similar the image-based representation is to its corresponding structure-based representation which indicates that it should be given more importance when aggregating the image-based representations. The aggre gated image-based representation for the k-th entity is defined as follows:

```
其中 e（k）S 表示第 k 个实体的基于结构的表示。 注意力越高，就越
类似，基于图像的表示与其对应的基于结构的表示类似，这表示在聚合基于图像的表示时应给予更多的重视。 第 k 个实体的基于聚集门控图像的表示形式定义如下：
```

![Formula(26).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(26).jpg>)

   Given a triple, the overall energy function is defined by combining four energy functions (i.e., *E*(*h*, *r*, *t*) = *E~SS~* + *E~II~* + *E~SI~* + *E~IS~* . These energy functions are based on two kinds of entity representations (i.e, structure-based and image-based representations). The first energy function (i.e., *E~SS~* = ||*h~S~* + *r* *−* *t~S~* ||) is same as TransE and the second function (i.e., *E~II~* = ||*h~I~* + *r* *−* *t~I~*||) uses their corresponding image-based representations for both head and tail entities. The third function (i.e., *E~SI~* = ||*h~S~* + *r* *−* *t~I~*||) is based on the structure-based representation of the head entity and the image-based representation of the tail entity whereas the fourth function (i.e., *E~IS~* = ||*h~I~* + *r* *−* *t~S~*||) is the exact opposite. These third and forth functions ensure that both structure-based representation and image-based representations are learned into the same vector space.

```
给定三元组，则通过组合四个能量函数（即 E（h，r，t）= ESS + EII + ESI + EIS）来定义整体能量函数。这些能量函数基于两种实体表示形式（即结构 能量函数（即 ESS = || hS + r-tS ||）与 TransE 和第二函数（即 EII = || hI + r-tI ||）相同。 ）将其相应的基于图像的表示形式用于头部和尾部实体，第三个函数（即 ESI = || hS + r-tI ||）基于头部的基于结构的表示形式和基于图像的 尾部实体的表示，而第四个函数（即 EIS = || hI + r-tS ||）正好相反，这些第三和第四个函数确保将基于结构的表示和基于图像的表示都学习到相同的向量空间。
```

   Given the energy function *E*(*h*, *r*, *t*), a margin-based scoring function is defined as follows:

```
给定能量函数 E（h，r，t），基于边距的评分函数定义如下：
```

![Formula(27).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(27).jpg>)

where γ is a margin hyperparameter and *T'* is the negative sample set of T generated by replacing the head entity, tail entity or the relation for each triple in T. Note that triples which are already in T are removed from *T'*.

``` 
其中 γ 是余量超参数，T' 是通过替换 T 中每个三元组的头部实体，尾部实体或关系而生成的 T 的负样本集。请注意，已从 T' 中删除了 T 中已存在的三元组。
```

   **Summary** IKRL makes use of the images of entities for KG representation learning by combining structure-based representation with image-based representation. However, given a triple < *h*, *r*, *t* >, in order to achieve very good representations for the entities *h* and *t*, both entities are required to have images associated with them. The other issue with this model is that an image is considered as an attribute of only those entities it is associated with. For example, if there is an image of two entities *e*~1~ and *e*~2~ but the image is associated with only e~1~, then it will be taken as one image instance of *e*~1~ but not of *e*~2~. However, it would be more beneficial to explicitly associate images with all the entities they represent before using them for learning KG embedding.

```
总结 IKRL 通过将基于结构的表示与基于图像的表示相结合，将实体的图像用于 KG 表示学习。 但是，给定三元组 <h，r，t>，以实现对实体 h 和 t 的非常好的表示，要求两个实体都具有与它们关联的图像。 此模型的另一个问题是，仅将图像视为与其关联的那些实体的属性。 例如，如果存在一个包含两个实体 e1 和 e2 的图像，但是该图像仅与 e1 相关联，那么它将被视为 e1 的一个图像实例，而不是 e2 的一个图像实例。 但是，将图像与它们表示的所有实体明确关联在一起，然后再将其用于学习 KG 嵌入，将更为有益。
```

### 4.4. *Models with Multi-modal Literals*

```       
具有多模态文本的模型
```

   This section presents an analysis of the embedding models making use of at least two types of literals providing complementary information. First, the category with numeric and text literals is discussed followed by the category with numeric, text, and image.

```  
本节使用至少两种提供补充信息的文字来分析嵌入模型。 首先，讨论具有数字和文本文字的类别，然后讨论具有数字，文本和图像的类别。
```

#### 4.4.1. *Models with Numeric and Text Literals*

``` 
具有数字和文本文字的模型
```

**LiteralE with blocking**==[62]== proposes to improve the effectiveness of the data linking task by combining LiteralE with a CER blocking==[63]== strategy. Unlike LiteralE, given an attributive triple < *h* , *d*, *v* >, in addition to the object literal value *v* it also takes literals from URI infixes of the head entity *h* and the data relation *d*. The CER blocking is based on a two-pass indexing scheme. In the first pass, Levenshtein distance metric is used to process literal objects and URI infixes whereas in the second pass semantic similarity computation with WordNet==[64]== is applied to process object/data relations. All the extracted literals are tokenized into word lists so as to create inverted indices. The same training procedure as in LiteralE is used to train this model. For every given triple < *h* , *r*, *t* >, the scoring function *f* from LiteralE is adopted to compute scores for all the triples < *h* , *r*, *t* *0* > in the knowledge graph. A sigmoid function, *p* = σ( *f*(.)) , is used to produce probabilities. Then, the model is trained by minimizing the binary cross-entropy loss of the produced probability function vector with respect to the vector of truth values for the triples.

[62] G.de Assis Costa and J.M.P.de Oliveira, Towards Exploring Literals to Enrich Data Linking in Knowledge Graphs, in: *AIKE*, 2018.

[63] G.de Assis Costa and J.M.P. de Oliveira, A Blocking Scheme for Entity Resolution in the Semantic Web, in: *AINA*, 2016.

[64] G.A.Miller, WordNet: a lexical database for English, *Communications of the ACM* 38(11) (1995), 39–41.

```   
带有阻塞功能的 LiteralE[62] 建议通过将 LiteralE 与 CER 阻塞策略[63]相结合来提高数据链接任务的效率。与 LiteralE 不同，给定三元组 <h,d,v>，除了对象文字值 v 之外，它还从头实体 h 和数据关系 d 的 URI 前缀中获取文字。 CER 阻止基于两次遍历索引方案。在第一遍中，Levenshtein 距离度量用于处理文字对象和 URI 前缀，而在第二遍中，使用 WordNet[64] 进行语义相似度计算以处理对象 / 数据关系。所有提取的文字都被标记为单词列表，以创建倒排索引。使用与 LiteralE 中相同的训练过程来训练该模型。对于每个给定的三元组 <h,r,t>，采用 LiteralE 的评分函数 f 来计算知识图中所有三元组 <h,r,t,0> 的分数。使用 S 形函数 p = σ(f(.)) 来产生概率。然后，通过相对于三元组的真值向量最小化产生的概率函数向量的二进制交叉熵损失来训练模型。

[62] G.de Assis Costa和J.M.P.de Oliveira, "致力于探索文字以丰富知识图中的数据链接", 载于:AIKE, 2018年
[63] G.de Assis Costa和J.M.P.de Oliveira，语义网中实体解析的阻塞方案，载于:AINA, 2016
[64] G.A.Miller, WordNet:英语词汇数据库，ACM通讯38(11)(1995), 39-41
```

**EAKGAE**==[65]== is an approach designed for entity alignment between KGs by learning a unified embedding space for the KGs. The entity alignment task has three main modules: Predicate alignment, Embedding learning, and Entity alignment. The predicate alignment module merges two KGs together by renaming similar predicates so as to create unified vector space for the relationship embeddings. The embedding learning module jointly learns entity embeddings of two KGs using structure embedding (by adapting TransE) and attribute character embedding. The adapted TransE is customized in a way that more focus can be given to triples with aligned predicates. This is obtained by adding a weight α to control the embedding learning over the triples. Thus, the following objective function *J~SE~* is defined for the structure-based embedding:

[65] B.D.Trsedya, J.Qi and R. ZhangâLU , Entity Alignment between Knowledge Graphs Using Attribute Embeddings, in: *AAAI*, 2019.

```
EAKGAE[65] 是一种通过学习 KG 的统一嵌入空间来设计 KG 之间的实体对齐的方法。 实体对齐任务具有三个主要模块：谓词对齐，嵌入学习和实体对齐。 谓词对齐模块通过重命名相似的谓词将两个 KG 合并在一起，从而为关系嵌入创建统一的向量空间。 嵌入学习模块使用结构嵌入（通过适应 TransE）和属性字符嵌入来联合学习两个 KG 的实体嵌入。 定制后的 TransE 可以使谓词对齐的三元组得到更多的关注。 这是通过增加权重 α 来控制三元组中的嵌入学习而获得的。 因此，为基于结构的嵌入定义了以下目标函数 JSE：

[65] B.D.Tresedya, J.Qi和R.ZhangâLU, 使用属性嵌入的知识图之间的实体对齐, 载于AAAI, 2019年
```

![Formula(28).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(28).jpg>)

![Formula(29).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(29).jpg>)

where *T~r~* and *T'~r~* are the sets of valid triples and corrupted triples respectively, *count*(*r*) is the number of occurrences of the relation *r*, and |*T*| is the total number of triples in the merged KG.

```   
其中 Tr 和 T'r 分别是有效三元组和损坏的三元组，count（r）是关系 r 的出现次数，| T |  是合并的 KG 中三元组的总数。
```

   On the other hand, the attributing character embedding is designed to learn embeddings for entities from the strings occurring in the attributes associated with the entities. The purpose is to enable the entity embeddings from two KGs to fall into the same vector space despite the fact that the attributes come from different KGs. The attribute character embedding is inspired by the concept of translation in TransE. Given a triple (*h*, *r*, *a*), the data property r is interpreted as a translation from the head entity *h* to the literal value *a* i.e. *h* + *r* = *f~a~*(*a*) where *f~a~*(*a*) is a compositional function. This function encodes the attribute values into a single vector mapping similar attribute values into similar representation. Three different compositional functions SUM, LSTM, and N-gram-based functions have been proposed. SUM is defined as a summation of all character embeddings of the attribute value. In LSTM, the final hidden state is taken as a vector representation of the attribute value. The N-gram-based function, which shows better performance than the others according to their experiments, uses the summation of n-gram combination of the attribute value.

```
另一方面，属性字符嵌入旨在从出现在与实体相关联的属性中的字符串中学习实体的嵌入。目的是使来自两个 KG 的实体嵌入能够落入相同的向量空间，尽管属性来自不同的 KG。属性字符嵌入的灵感来自 TransE 中的翻译概念。给定一个三元组（h，r，a），数据属性 r 被解释为从头部实体 h 到文字值 a 的转换，即 h + r = fa（a）其中 fa（a）是一个合成函数。此函数将属性值编码为单个向量，将相似的属性值映射为相似的表示形式。已经提出了三种不同的组成函数 SUM，LSTM 和基于 N-gram 的函数。 SUM 定义为属性值的所有字符嵌入的总和。在 LSTM 中，最终的隐藏状态被视为属性值的矢量表示。基于 N 元语法的函数根据其实验显示出比其他函数更好的性能，该函数使用属性值的 N 元语法组合的总和。
```

   The following objective function is defined for the attribute character embedding:

```
为属性字符嵌入定义了以下目标函数：
```

![Formula(30).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(30).jpg>)

where, *T~a~* and *T'~a~* are the sets of valid attribute triples and corrupted attribute triples with A being the set of attributes in a given KG *G*. The corrupted triples are created by replacing the head entity with a random entity or the attribute with a random attribute value. Here, *f*( *t~a~*) is the plausibility score computed based on the embedding of the head entity h, the embedding of the relation r, and the vector representation of the attribute value obtained using one of the compositional functions *f~a~*(*a*).

``` 
其中，Ta 和 T'a 是有效属性三元组和损坏的属性三元组，而 A 是给定 KG G 中的属性集。损坏的三元组是通过将头部实体替换为随机实体或将属性替换为 a  随机属性值。 在此，f（ta）是基于头实体 h 的嵌入，关系 r 的嵌入以及使用合成函数 fa（a）之一获得的属性值的矢量表示而计算出的合理性评分。
```

   The attribute character embedding *h~ce~* is used to shift the structure embedding *h~se~* into the same vector space by minimizing the following objective function:

```
通过最小化以下目标函数，利用嵌入 hce 的属性特征将嵌入 hse 的结构转换到同一个向量空间:
```

![Formula(31).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(31).jpg>)

where, ||*x*||~2~ is the L~2~-Norm of vector x. This way the similarity of entities from two KGs is captured by the structure embedding based on the entity relationships and by the attribute embedding based on the attribute values.

```        
其中 || x || 2 是向量 x 的 L2 - 范数。 这样，通过基于实体关系的结构嵌入和通过基于属性值的属性嵌入，可以获取来自两个 KG 的实体的相似性。
```

   All the three functions are summed up to an overall objective function J (i.e., *J* = *J~SE~* + *J~CE~* + *J~SIM~*) for jointly learning both structure and attribute embeddings. Finally, the alignment is done by defining a similarity equation with a specified threshold. Moreover, a transitivity rule has been applied to enrich triples in the KGs to get a better attribute embedding result.

```   
所有这三个函数都被汇总为一个总体目标函数 J（即 J = JSE + JCE + JSIM），用于共同学习结构和属性嵌入。 最后，通过定义具有指定阈值的相似性方程式来完成对齐。 此外，已应用传递性规则来丰富 KG 中的三元组，以获得更好的属性嵌入结果。
```

**Summary** The common drawback with both methods (LiteralE with blocking and EAKGE) is that text and numeric literals are treated in the same way. They also do not consider literal data type semantics or multi-valued literals in their approach. Furthermore, since EAKGAE is using character-based attribute embedding, it fails to capture the semantics behind the co-occurrence of syllables.

```   
总结 这两种方法(带阻塞的 LiteralE 和 EAKGE)的共同缺点是文本和数值文本以相同的方式处理。 他们的方法也不考虑文字数据类型语义或多值文字。 此外，由于 EAKGAE 使用基于字符的属性嵌入，它无法捕捉音节共现背后的语义。
```

#### 4.4.2. *Models with Numeric, Text, and Image Literals*

```   
具有数字，文本和图像文字的模型
```

**MKBE**==[49]== is a multi-modal KG embedding, in which the text, numeric and image literals are modelled together. The main objective of this approach is to utilize all the observed subjects, objects, and relations (object properties and data properties) in order to predict whether any fact holds. It extends DistMult, which creates embedding for entities and relations, by adding neural encoders for different data types. Given a triple < *s* , *r*, *o* >, the head entity *s* and the relation *r* are encoded as independent embedding vectors using one-hot encoding through a dense layer. Similarly, if the object *o* is a categorical value, then it will be represented through a dense layer with a relu activation which has the same number of nodes as the embedding space dimension. On the other hand, if the object *o* is rather a numerical value, then a feed forward layer, after standardizing the input, is used in order to learn embeddings for *o* by projecting it to a higher-dimensional space. If *o* is a short text (such as names and titles), it is encoded using character-based stacked, bidirectional GRUs and the final output of the top layer will be taken as the representation of *o*. On the contrary, if *o* is a long text such as entity descriptions, CNN over word embeddings will be used to get the embeddings for *o*. The object *o* can also be an image, and in such a case, the last hidden layer of VGG pretrained network on ImageNet==[66]==, followed by compact bilinear pooling, is used to obtain the embedding of *o*. Given the vector representations of the entities, relations and attributes, the same scoring function from DistMult is used to determine the correctness probability of triples.

[66] K.Simonyan and A.Zisserman, Very deep convolutional networks for large-scale image recognition, in: *International Conference on Learning Representations (ICLR)*, 2015.

```   
MKBE[49] 是一种多模式 KG 嵌入，其中文本，数字和图像文字被一起建模。这种方法的主要目标是利用所有观察到的主题，对象和关系（对象属性和数据属性），以预测是否存在任何事实。它扩展了 DistMult，它通过为不同数据类型添加神经编码器来创建实体和关系的嵌入。给定一个三元组 <s，r，o>，头实体 s 和关系 r 使用一个热编码通过密集层编码为独立的嵌入矢量。类似地，如果对象 o 是分类值，则它将通过具有 relu 激活的密集层表示，该层具有与嵌入空间维数相同的节点数。另一方面，如果对象 o 是一个数值，则在标准化输入之后使用前馈层，以便通过将 o 投影到更高维度的空间来学习 o 的嵌入。如果 o 是短文本（例如名称和标题），则使用基于字符的堆叠式双向 GRU 对其进行编码，并且顶层的最终输出将用作 o 的表示形式。相反，如果 o 是诸如实体描述之类的长文本，则将使用单词嵌入上的 CNN 来获取 o 的嵌入。对象 o 也可以是图像，在这种情况下，使用 ImageNet[66] 上 VGG 预训练网络的最后一个隐藏层，然后进行紧凑的双线性池化，即可获得 o 的嵌入。给定实体，关系和属性的矢量表示形式，使用 DistMult 相同的评分功能确定三元组的正确性概率。

[66] K.Simonyan和A.Zisserman, 用于大规模图像识别的超深度卷积网络, 见:国际学习表示会议(ICLR), 2015年
```

   The binary cross-entropy loss, as defined below, is used to train the model:

```
如下定义的二进制交叉熵损失用于训练模型：
```

![Formula(32).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Formula(32).jpg>)

where for a given subject relation pair (*s*, *r*), binary label vector *t^s,r^* over all entities is used to indicate whether <*s*, *r*, *o*> is observed during training. *p~o~ ^s,r^* denotes the model’s probability of truth for any triple <*s*, *r*, *o*> computed using a sigmoid function.

```   
对于给定的主题关系对（s，r），所有实体上的二进制标签向量 ts，r 用于指示在训练过程中是否观察到 <s，r，o>。 po s，r 表示使用 S 型函数计算出的任何三元组 <s，r，o> 的模型的真实概率。
```

   Moreover, using these learned embeddings and different neural decoders, a novel multimodal imputation model is introduced to generate missing multimodal values, such as numerical data, categorical data, text, and images, from information in the knowledge base. In order to predict the missing numerical and categorical data such as dates, gender, and occupation, a simple feed-forward network on the entity embedding is used. For text, the adversarially regularized autoencoder (ARAE) has been used to train generators that decodes text from continuous codes, having the generator conditioned on the entity embeddings instead of random noise vector. Similarly, the combination of BE-GAN structure with pix2pix-GAN model is used to generate images, conditioning the generator on the entity embeddings.

```   
此外，使用这些学习的嵌入和不同的神经解码器，引入了一种新颖的多峰插补模型，以从知识库中的信息生成缺失的多峰值，例如数值数据，分类数据，文本和图像。 为了预测缺失的数字和分类数据（例如日期，性别和职业），使用了一个简单的实体嵌入前馈网络。 对于文本，对抗性正则化自动编码器（ARAE）已用于训练生成器，该生成器从连续代码解码文本，并使生成器以实体嵌入而不是随机噪声向量为条件。 类似地，BE-GAN 结构与 pix2pix-GAN 模型的组合用于生成图像，从而将生成器置于实体嵌入上。
```

   **Summary** Despite the attempt made in incorporating text literals, numeric literals, and images into the KG embedding, the model (MKBE) fails to capture the semantics of the data types/units of (numeric) literal values. Besides, similar to IKRL, it takes an image *I* as an instance of a certain entity *e* only if, *I* is initially associated with *e* in the dataset considered (refer to Section 4.3 for more details).

```
总结 尽管已尝试将文本文字，数字文字和图像合并到 KG 嵌入中，但是模型（MKBE）未能捕获（数字）文字值的数据类型/单位的语义。 此外，类似于 IKRL，仅当 I 最初与所考虑的数据集中的 e 相关联时，才将图像 I 作为某个实体 e 的实例（有关更多详细信息，请参见第 4.3 节）。
```

## 5. Applications

   This section discusses different applications of KG embeddings on which the previously described methods have been trained and/or evaluated.

```
本节讨论了 KG 嵌入的不同应用，在这些应用中已经对前面描述的方法进行了训练 和/或 评估。
```

**Link prediction.** In general terms, link prediction can be defined as a task of identifying missing information in complex networks==[67, 68]==. Specifically in the case of KGs, link prediction models aim at predicting new relations between entities leveraging the existing links for training. Along with predicting relations between the entities link prediction also focuses on the task of predicting either the head or the tail entity with respect to a relation. Then it decides if a new triple, which is not observed in the KG, is valid or not. Formally, let *G* be a KG with a set of entities *E* = *{e*~1~, . . . , *e~n~*} and a set of object relations *R* = {*r*~1~, . . . , *r~m~*}, then link prediction can be defined by a mapping function ψ : *E* *×* *E* *×* *R* *→* *R* which assigns a score to every possible triple (*e~i~*, *e~j~*, *r~k~*) ∈ *E* *×* *E* *×* *R*. A high score indicates that the triple is most likely to be true. 

[67] D.Liben-nowell and J.Kleinberg, The Link Prediction Problem for Social Networks, *Journal of the American Society for Information Science and Technology* **58** (2003).doi:10.1002/asi.20591.

[68] L.Lü and T.Zhou, Link prediction in complex networks: A survey, *Physica A: statistical mechanics and its applications* 390(6) (2011), 1150–1170. 

```
链接预测。 一般来说，链路预测可以定义为在复杂网络中识别缺失信息的任务。 特别是在 KG 的情况下，连结预测模式旨在预测利用现有连结进行训练的实体之间的新关系[67, 68]。 除了预测实体之间的关系，链接预测还着重于预测任何一个关系的头或尾实体。 然后，它决定一个新的三元组是否有效(KG 中没有观察到这一点)。 在形式上，设 g 是一个 KG，具有一组实体 e { e1，... ，en }和一组对象关系 r {r1，... ，rm} ，则链路预测可以用一个映射函数来定义: E × E × R → R，该映射函数给每个可能的三元组(ei，ej，rk)分配一个得分。 高分表明那个三元组最有可能是真的。

[67] D.Liben-nowell和J.Kleinberg,《社交网络的链接预测问题》, 美国信息科学技术学会学报 58(2003).doi:10.1002/asi.20591
[68] L.Lü和T.Zhou,《复杂网络中的链接预测:一项调查》,《物理学》A：统计力学及其应用390(6)(2011), 1150–1170年
```

   Link prediction is one of the most common tasks used for evaluating the performance of KG embeddings. Head prediction, tail prediction, and relation prediction are different kinds of sub-tasks related to link prediction. Head prediction aims at identifying a missing head entity where the relation and tail entity are given, and analogously for tail prediction and relation prediction. Most of the models discussed in Section 4 have been evaluated on one or more of these prediction tasks. Head and tail prediction are used to evaluate the models LiteralE==[45]==, TransEA==[46]==, KBLRN==[47]==, KDCoE==[50]==, EAKGAE==[65]==, and IKRL==[51]==. On the other hand, DKRL==[11]== has been evaluated on all kinds of link prediction tasks: head, tail, and relation predictions whereas MKBE==[49]== has been evaluated on tail prediction. In Extended RESCAL==[24]==, two kinds of link prediction experiments have been conducted on the Yago 2==[69]==, i.e., i) tail prediction by fixing the relation type to rdf:type, and ii) general link prediction experiments for all relation types. Unfortunately, it is not possible to compare the obtained evaluation results of all these models because the experiments have been   carried out on different datasets and also different link prediction procedures have been followed. Taking this into consideration, in this survey, experiments have been conducted on head and tail prediction tasks for these models (see Section 6).

[69] F.M.Suchanek, G.Kasneci and G.Weikum, Yago: A Core of Semantic Knowledge, in: *Proceedings of the 16th International Conference on World Wide Web*, WWW ’07, ACM, New York, NY, USA, 2007, pp.697–706. ISBN 978-1-59593-654-7.doi:10.1145/1242572.1242667.

```
链接预测是用于评估 KG 嵌入性能的最常见任务之一。头部预测，尾部预测和关系预测是与链接预测有关的不同种类的子任务。头部预测旨在识别给出了关系和尾部实体的丢失的头部实体，并且类似地用于尾部预测和关系预测。在第 4 节中讨论的大多数模型已针对这些预测任务中的一项或多项进行了评估。头尾预测用于评估模型 LiteralE [45]，TransEA[46]，KBLRN[47]，KDCoE[50]，EAKGAE[65] 和 IKRL[51]。另一方面，DKRL[11] 已在各种链接预测任务上进行了评估：头，尾和关系预测，而 MKBE[49] 已在尾部预测上进行了评估。在 Extended RESCAL[24] 中，已经对 Yago 2[69] 进行了两种链接预测实验，即 i）通过将关系类型固定为 rdf：type 进行尾部预测，ii）所有关系的通用链接预测实验类型。不幸的是，不可能对所有这些模型的评估结果进行比较，因为实验是在不同的数据集上进行的，并且遵循了不同的链接预测程序。考虑到这一点，在本次调查中，针对这些模型的头和尾预测任务进行了实验（请参见第 6 节）。

[69] FMSuchanek, G.Kasneci和G.Weikum, Yago:语义知识的核心, 载于:第16届国际互联网会议论文集, WWW'07, ACM, 美国纽约, 2007年, 第697-706页.ISBN 978-1-59593-654-7.doi:10.1145/1242572.1242667
```

**Triple Classification.** The goal of the triple classification task is the same as that of link prediction. A potential triple (*e~i~*, *e~j~*, *r~k~*) is classified as 0 (false) or 1 (true), i.e., a binary classification task. The embedding models MTKGNN==[11]== and IKRL==[51]== have been evaluated on this task. However, since they do not use a common evaluation dataset, it is not possible to compare the reported results directly.

```
三重分类。 三分类任务的目标与链路预测的目标是一致的。 一个潜在的三元组 (ei，ej，rk) 被分类为 0 (false) 或 1 (true) ，即二进制分类任务。 嵌入模型 MTKGNN[11] 和 IKRL[51] 已针对此任务进行了评估。 但是，由于他们不使用公共评估数据集，因此无法直接比较报告的结果。
```

**Entity Classification.** Given a KG *G*, with a set of entities *E* and types *T* and with an entity *e* ∈ *E* and type *t* ∈ *T*, the task of entity classification is to determine if a potential entity type pair (*e*, *t*) which is not observed in G ((*e*, *t*) *∈*/ *G*) is a missing fact or not. This task is an entity type prediction using a multi-label classification algorithm considering the entity types in G as given classes. In DKRL==[11]==, the proposed model has been evaluated on this task using the dataset FB15K==[10]==.

```
实体分类。 给定一个 KG G，它具有一组实体 E 和类型 T，并且具有实体 e∈E 和类型 t∈T，则实体分类的任务是确定是否未观察到潜在的实体类型对（e，t） 在 G（（e，t）∈/ G）中是否存在缺失事实。 此任务是使用多标签分类算法的实体类型预测，将 G 中的实体类型视为给定类。 在 DKRL[11] 中，已使用数据集 FB15K[10] 对提出的模型进行了评估。
```

**Entity Alignment.** Given two KGs *G~1~* and *G~2~*, the goal of the entity alignment task is to identify those entity pairs (*e~1~*, *e~2~*) where *e~1~* is an entity in *G~1~* and *e* 2 is an entity in *G~2~* which denote the same real world entities, and hence the integration of *G~1~* and *G~2~* can be possible through these unified entities, i.e., entity pairs. Different embedding-based models have been proposed recently for the entity alignment task. Among the models that are included in this survey, EAKGAE==[65]== and KDCoE==[50]== have been proposed for the entity alignment task. Specifically, KDCoE==[50]== uses a cross-lingual entity alignment task which determines similar entities in different languages. Despite the fact that both these models use the same task for evaluation, the entity alignment task, their experimental results cannot be compared since they are based on different datasets.

```
实体对齐。 给定两个 KG G1 和 G2，实体对齐任务的目标是确定那些实体对（e1，e2），其中 e1 是 G1 中的实体，e 2 是 G2 中的实体，它们表示相同的现实世界实体，因此 通过这些统一的实体，即实体对，可以实现 G1 和 G2 的集成。 最近针对实体对齐任务提出了不同的基于嵌入的模型。 在本次调查所包括的模型中，已经提出了 EAKGAE[65] 和 KDCoE[50] 用于实体对齐任务。 具体来说，KDCoE[50] 使用跨语言的实体对齐任务，该任务确定不同语言的相似实体。 尽管这两个模型都使用相同的评估任务（实体对齐任务），但是由于它们基于不同的数据集，因此无法比较它们的实验结果。
```

**Other Applications.** Attribute-valueprediction,nearest-neighbor analysis, data linking, and document classification are other application scenarios used for the evaluation of the models under discussion. Attributevalue prediction is the process of predicting the values of (discrete) attributes in a KG. For example, a missing value of a person’s weight can be identified using the attribute value prediction task which is commonly seen as a KG completion task. In MTKGNN==[48]==, attribute   value prediction is applied using an attribute-specific Linear Regression classifier for evaluation. The same task has been employed in MKBE==[49]== for model evaluation by imputing different multi-modal attribute values.

```
其他应用。 属性值预测，最近邻分析，数据链接和文档分类是用于评估正在讨论的模型的其他应用场景。 属性值预测是预测 KG 中（离散）属性值的过程。 例如，可以使用通常被视为 KG 完成任务的属性值预测任务来识别体重的缺失值。 在 MTKGNN[48] 中，属性值预测是使用特定于属性的线性回归分类器进行评估的。 通过估算不同的多峰属性值，MKBE[49] 中采用了相同的任务进行模型评估。
```

​    Nearest Neighbor Analysis is a task of detecting the nearest neighbors of some given entities in the latent space learned by an embedding model. This task has been performed in LiteralE==[45]== to compare DistMult+LiteralE with the base model DistMult. On the other hand, data linking and document classification tasks have been used in LiteralE with blocking ==[62]== and KGlove with literals==[58]== respectively (refer to==[62]== and==[58]== for more details). Table 9 summarizes all the applications on which the KG embedding models with literals have been evaluated.

```
最近邻分析是一项任务，用于检测通过嵌入模型学习的潜在空间中某些给定实体的最近邻。 已在 LiteralE[45] 中执行了此任务，以将 DistMult + LiteralE 与基本模型 DistMult 进行比较。 另一方面，数据链接和文档分类任务已分别在 LiteralE 中使用阻塞[62] 和带有文字[58] 的 KGlove 使用（有关更多详细信息，请参阅[62] 和[58]）。 表 9 总结了评估了带文字的 KG 嵌入模型的所有应用程序。
```

## 6. **Experiments on Link Prediction**

```         
链路预测实验
```

   This section provides an empirical evaluation of the methods discussed in the previous section under a unified environmental settings and discusses the results based on the performance of the approaches applied to the task of link prediction. In this work, link prediction is chosen because most of the KG embedding models with literals are trained and evaluated on it. One of the major issues encountered while conducting these experiments is that the source code of some of these models is not openly available and is not easily reproducible. Such methods were excluded from the experimentation. In the subsequent sections, the datasets and the experiments with text, numeric, images and multimodal literals are presented.

```      
  本节对前一节在统一环境条件下讨论的方法进行了实证评价，并基于应用于链接预测任务的方法的性能讨论了结果。 在这项工作中，选择链接预测是因为对大多数带有文字的 KG 嵌入模型进行了训练和评估。 进行这些实验时遇到的主要问题之一是这些模型中某些模型的源代码不是公开可用的，并且不容易复制。 这种方法被排除在实验之外。 在随后的部分中，将介绍数据集以及带有文本，数字，图像和多模式文字的实验。
```

### 6.1. *Datasets*

```          
数据集
```

   The performance of the aforementioned models was measured using two of the most commonly used datasets for link prediction, i.e., FB15K==[10]== and FB15K-237==[70]== are considered. FB15K is a subset of Freebase==[5]== which mostly contains triples describing the facts about movies, actors, awards, sports and sport teams. It contains a randomly split training, validation, and test sets. The issue with this dataset is that the test set contains a large number of triples which are obtained by simply inverting triples in the training set. This enables a simple embedding model which is symmetric with respect to the head and tail entity to obtain an excellent performance. In order to avoid this, the dataset FB15K-237 has been created by removing the inverse relations from FB15K. The statistics of these datasets is given in Table 3.

[70] K.Toutanova and D.Chen, Observed versus latent features for knowledge base and text inference, in: *Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality*, Association for Computational Linguistics, Beijing, China, 2015, pp.57–66. doi:10.18653/v1/W15-4007. https://www.aclweb.org/anthology/W15-4007.  

```
  使用两个最常用的链接预测数据集（即 FB15K[10] 和 FB15K-237[70]）测量了上述模型的性能。 FB15K 是 Freebase[5] 的子集，其中大部分包含描述电影，演员，奖项，体育和运动队的事实的三元组。 它包含随机分组的训练，验证和测试集。 该数据集的问题在于，测试集包含大量的三元组，而这些三元组是通过简单地将训练集中的三元组反转而获得的。 这使得相对于头和尾实体对称的简单嵌入模型能够获得出色的性能。 为了避免这种情况，已通过从 FB15K 中删除逆关系创建了数据集 FB15K-237。 表 3 给出了这些数据集的统计信息。
  
K.Toutanova和D.Chen, 知识库和文本推理的观察与潜在特征, 载于:连续向量空间模型及其组成的第三届研讨会论文集, 计算语言学协会, 北京, 中国, 2015年, 第57–66页.doi:10.18653/v1/W15-4007.https://www.aclweb.org/anthology/W15-4007
```

​                                                                                    Table 3
The number of entities, object relations, data relations, relational triples, train sets, valid sets, and test sets of the FB15K and the FB15K-237 datasets.

![Table(03).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Table(03).jpg>)

```   
FB15K 和 FB15K-237 数据集的实体数量，对象关系，数据关系，关系三元组，训练集，有效集和测试集。
```

### 6.2. *Experiments with Text Literals*

```         
文本文字实验
```

   As discussed in Section 4.1, the embedding models Extended RESCAL, DKRL, KDCoE, and KGloVe with literals utilize text literals. However, all of these models except DKRL are not considered for experimentation due to the following issues:

```   
如第 4.1 节所述，带有文字的嵌入模型 Extended RESCAL，DKRL，KDCoE 和 KGloVe 使用文本文字。 但是，由于以下问题，除 DKRL 以外的所有这些模型均未考虑进行试验：
```

- The implementation of the model KGloVe with literals is not publicly available and it is not easily reproducible.

- KDCoE is designed specifically for cross-lingual entity alignment task which makes it difficult to apply it for link prediction.   

- In case of Extended RESCAL, practically this method is computationally expensive and thus not considered as a feasible embedding model to incorporate literals. Moreover, none of the models with literals which are discussed in this paper consider Extended RESCAL in their experiments.

  ```
  - 带有文字的 KGloVe 模型的实现不是公开可用的，并且不容易复制。
  - KDCoE 专为跨语言实体对齐任务而设计，这使其很难将其应用于链接预测。
  - 在扩展 RESCAL 的情况下，该方法实际上在计算上是昂贵的，因此不被视为合并文字的可行嵌入模型。 此外，本文讨论的带文字的模型均未在实验中考虑扩展 RESCAL。
  ```

**Extended Dataset:** In order to conduct the experiments with text literals, both the datasets FB15K and FB15K-237 given in Table 3 are extended with a set of 15239 attributive triples containing only text literals. For pre-processing of the text (the entity descriptions), spacy.io^1^ has been used. This includes tokenization, named entity recognition and conversion of numbers to text, i.e., 16 has been converted to ’sixteen’. After the pre-processing step, all the entities along with the corresponding triples having no or short description of less than 3 words are removed. Also, the triples containing these entities are removed as mentioned by the authors in the paper. Moreover, only one description is chosen randomly for the entities with multiple text descriptions.

```  
扩展数据集：为了用文本文字进行实验，表 3 中给出的数据集 FB15K 和 FB15K-237 都用一组仅包含文本文字的 15239 属性三元组进行了扩展。 对于文本（实体描述）的预处理，已使用 spacy.io1。 这包括令牌化，命名实体识别以及将数字转换为文本，即 16 已转换为 “十六”。 在预处理步骤之后，将删除所有实体以及不包含少于 3 个单词的简短描述的相应三元组。 而且，如作者在论文中提到的那样，删除了包含这些实体的三元组。 此外，对于具有多个文本描述的实体，仅随机选择一个描述。
```

```      
实验设置：用于 DKRL 的超参数如下：学习速率 0.001，嵌入大小 100，损失余量 1，批量大小 100，单位时间  1000。实验在具有 503GiB RAM 和 2.60GHz 速度的 Ubuntu 16.04.5 LTS 系统上进行。
```

**Evaluation Procedure and Results:** The performance of the model is evaluated based on the link prediction task. For each triple in the test set, a set of corrupted triples is generated with respect to the head or the tail entity. A triple is said to be corrupted with respect to its head entity if that head entity is replaced with any other entity from the KG, and analogously for a triple corrupted with respect to its tail entity. The evaluation metrics MR (Mean Rank), MRR (Mean Reciprocal Rank), hits@1, hits@3, and hits@10 are used for measuring the performance of the models. Since the corrupted triples that occur in the training, validation or test set may underestimate the metrics, they are filtered out before computing the scores with the metrics. Therefore, the experimental results reported in this paper are based on the above described (filtered) setting.

```    
评估程序和结果：基于链接预测任务评估模型的性能。 对于测试集中的每个三元组，将针对头部或尾部实体生成一组损坏的三元组。 如果该头实体被 KG 中的任何其他实体替换，则称其三元组相对于其头实体已损坏，并且类似地，针对其尾部实体损坏了三元组。 评估指标 MR（平均排名），MRR（平均倒数排名），hits @ 1，hits @ 3 和 hits @ 10 用于评估模型的性能。 由于在训练，验证或测试集中出现的损坏的三元组可能会低估指标，因此在使用指标计算分数之前，将其过滤掉。 因此，本文报道的实验结果基于上述（过滤）设置。
```

   The results of link prediction on FB15K and FB15K-237 datasets are shown in Table 4. These results are reported separately for the head entity, tail entity and relation predictions. The overall results obtained by taking the mean of the head and tail predictions are also provided. The best scores are the ones which are highlighted in bold text. It is to be noted that the dataset FB15K-237 achieves slightly better result compared to FB15K because the former one does not contain symmetric relations. Furthermore, the result
shows that DKRL model with Bernoulli distribution (DKRL*~Bern~*) has performed better than the model with Uniform distribution (DKRL*~unif~*) for both the datasets. The Bernoulli distribution for sampling as defined in==[12]== is a probability distribution, *tph/(tph +hpt)*, where *tph* is the average number of tail entities per head entity and *hpt* is the average number of head entities per tail entity. Given a golden triplet (*h* , *r*, *t* ), with the aforementioned probability, the triplet is corrupted by replacing the head, and with probability *hpt/(tph +hpt)*, the triplet is corrupted by replacing the tail. DKRL*~Bern~* works best for the prediction of head, relation, and tail with respect to MRR, Hits@1, and Hits@3 whereas the *DKRL~Unif~* method works better according to MR for both the datasets. DKRL*Bern* works slightly better than *DKRL~Unif~* for FB15K-237 dataset.

```   
  表 4 中显示了 FB15K 和 FB15K-237 数据集上的链接预测结果。这些结果分别针对头部实体，尾部实体和关系预测进行报告。还提供了通过对头和尾预测的平均值获得的总体结果。最好的分数是用粗体突出显示的分数。要注意的是，与 FB15K 相比，数据集 FB15K-237 取得了更好的结果，因为前者不包含对称关系。此外，结果
结果表明，对于两个数据集，具有伯努利分布的 DKRL 模型（DKRLBern）的性能要优于具有均匀分布的模型（DKRLunif）。如[12] 中定义的用于采样的伯努利分布是概率分布，tph /（tph + hpt），其中 tph 是每个头实体的尾部实体的平均数量，hpt 是每个尾部实体的头部实体的平均数量。给定黄金三元组（h，r，t），具有上述概率，则三元组通过替换头部而损坏，而概率 hpt /（tph + hpt），则三元组通过替换尾部而损坏。 DKRLBern 最适合预测 MRR，Hits @ 1 和 Hits @ 3 的头部，关系和尾部，而 DKRLUnif 方法根据这两个数据集的 MR 均能更好地预测。对于 FB15K-237 数据集，DKRLBern 比 DKRLUnif 稍好。
```

### 6.3. *Experiment with Numeric Literals*

```       
使用数字文本进行实验
```

   MT-KGNN, KBLRN, LiteralE, and TransEA are the KG embedding models which make use of numeric literals (see Section 4.2). KBLN, the submodel of KBLRN, which excludes the relational information provided by graph feature methods is used in the experiment instead of the main model KBLRN. This is the case because KBLN is directly comparable with the other three models (i.e., MT-KGNN, LiteralE, and TransEA) whereas KBLRN is not. The code^2^ for the TransEA model is the original implementation from TransEA==[46]== where as the source codes^3^ for the models MT-KGNN, KBLN, and LiteralE are taken from the implementation in LiterlaE==[45]==. As described in Section. 4.2, the structure-based embedding component of MT-KGNN is based on a neural network and it is referred to as RelNet. However, in the version implemented in LiteralE==[45]==, they have replaced RelNet with DistMult as a baseline in order to have a directly comparable MTKGNN-like method to their proposed approach. Thus, in this survey, the MT-KGNN like model has been used instead of the original MT-KGNN model.

```   
MT-KGNN，KBLRN，LiteralE 和 TransEA 是使用数字文字的 KG 嵌入模型（请参见第 4.2 节）。实验中使用 KBLN（KBLRN 的子模型）代替了主要模型 KBLRN，该子模型排除了图特征方法提供的关系信息。之所以如此，是因为 KBLN 可直接与其他三个模型（即 MT-KGNN，LiteralE 和 TransEA）进行比较，而 KBLRN 则不能。 TransEA 模型的 code2 是 TransEA[46] 的原始实现，其中 MT-KGNN，KBLN 和 LiteralE 模型的源代码 3 来自 LiterlaE[45] 的实现。如本节所述。 4.2，MT-KGNN 的基于结构的嵌入组件基于神经网络，称为 RelNet。然而，在 LiteralE[45] 中实现的版本中，他们已经用 DistMult 代替了 RelNet 作为基线，以便能够直接将类似 MTKGNN 的方法与他们提出的方法进行比较。因此，在本次调查中，使用了类似 MT-KGNN 的模型来代替原始的 MT-KGNN 模型。
```

   Moreover, the model LiteralE has different varieties depending on the baseline model and the transformation function used. As discussed in Section 4, in LiteralE there are two transformation functions: *g*(GRU based function) and *lin* (a simple linear function), and there are three baseline models-DistMult, ConvE and ComplEx. Thus, in this experiment, six varieties of the LiteralE model are considered: DistMult-Literale*~g~*, ComplEx-Literale*~g~*, ConvELiterale*~lin~*, DistMult-Literale*~lin~*, ComplEx-Literale*~lin~*, and ConvE-Literale*~lin~*. The datasets, the experimental setup, and the evaluation results are discussed in the subsequent sections.

```    
此外，根据基准模型和所使用的转换函数，模型 LiteralE 具有不同的品种。 如第 4 节所述，在 LiteralE 中，有两个转换函数：g（基于 GRU 的函数）和 lin（一个简单的线性函数），并且有三个基线模型：DistMult，ConvE 和 ComplEx。 因此，在本实验中，考虑了 LiteralE 模型的六个变体：DistMult-Literaleg，ComplEx-Literaleg，ConvELiteralelin，DistMult-Literalelin，ComplEx-Literalelin 和 ConvE-Literalelin。 数据集，实验设置和评估结果将在后续章节中讨论。
```

**Extended Dataset:** In order to conduct the experiments with numeric literals, both the datasets FB15K and FB15K-237 given in Table 3 are extended with a set of 23521 attributive triples containing only numeric literals. These triples are created based on the attributive triples from TransEA==[46]==. In TransEA, the authors have provided a set of attributive triples where the object values are numeric. However, it is not possible to directly use this data as the literal values are normalized in the interval [0-1] as required by the model but the other models in this experiment, like LiteralE, use the original unnormalized literal values instead. Therefore, it was necessary to query Freebase to replace the normalized object literal value for each (subject, data relation) pair from the TransEA attributive triples data. Moreover, only those data relations which occur in at least 5 triples are taken into consideration.

```   
扩展数据集：为了用数字文字进行实验，表 3 中给出的数据集 FB15K 和 FB15K-237 都扩展了一组 23521 属性三元组，它们仅包含数字文字。 这些三元组是基于 TransEA 的属性三元组创建的[46]。 在 TransEA 中，作者提供了一组属性三元组，其中对象值是数字。 但是，不可能直接使用此数据，因为文字值会按照模型的要求在 [0-1] 区间中进行归一化，但是本实验中的其他模型（如 LiteralE）会使用原始的未归一化的文字值来代替。 因此，有必要查询 Freebase 以替换 TransEA 属性三元组数据中每个（主题，数据关系）对的规范化对象文字值。 此外，仅考虑至少出现在三个三元组中的那些数据关系。
```

​                                                                                      Table 4

​                               Experiment results using DKRL model on FB15K and FB15K-237 datasets.

![Table(04).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Table(04).jpg>) 

```
在 FB15K 和 FB15K-237 数据集上使用 DKRL 模型进行实验的结果。
```

**Experimental Setup:** Same as in LiteralE, the hyperparameters used for all models except TransEA for both datasets are: learning rate 0.001, batch size 128, embedding size either 100 or 200, embedding dropout probability 0.2, label smoothing 0.1, and epochs 100. The 1-N training approach, same as in the experiment of LiteralE [45], has been followed in this experiment (refer to LiteralE [45] for more details). In the case of TransEA, for both datasets, the parameters are: epoch 3000, dimension 100, batches 100, margin 2, and learning rate 0.3. The experiments were performed on Ubuntu 16.04.5 LTS system with 503GiB RAM and 2.60GHz speed. TITAN X (Pascal) GPU has been used for the models LiteralE, KBLN, and MTKGNN.

```      
实验设置：与 LiteralE 中一样，除了 TransEA 以外，所有两个数据集用于所有模型的超参数是：学习率 0.001，批处理大小 128，嵌入大小 100 或 200，嵌入丢失概率 0.2，标签平滑 0.1 和历元 100。 该实验遵循与 LiteralE [45] 实验相同的 1-N 训练方法（有关更多详细信息，请参考 LiteralE [45]）。 对于 TransEA，对于这两个数据集，参数均为：时代 3000，维度 100，批次 100，保证金 2 和学习率 0.3。 实验是在具有 503GiB RAM 和 2.60GHz 速度的 Ubuntu 16.04.5 LTS 系统上进行的。 TITAN X（Pascal）GPU 已用于 LiteralE，KBLN 和 MTKGNN 模型。
```

**Evaluation Procedure and Results:** The same evaluation metrics which are discussed in Section 6.2 has been used to evaluate the performance of the models with numeric literals on the link prediction task. As shown in Table 5, according to the overall result, the model KBLN has considerably better performance than the other models in all metrics except MR. The results from the ComplEx-LiteralE*~g~* model shows that it is capable to produce a highly competitive performance having the second best results with respect to the same metrics. This is the case due to the fact that this model is able to handle the inverse relations in FB15K by applying the complex conjugate of an entity embedding when the entity is used as a tail and its normal embedding when it is the head.

```   
评估过程和结果: 在第6.2节中讨论的相同的评估指标已经被用来评估链接预测任务的数字文字模型的性能。 如表5所示，从总体结果来看，KBLN 模型在除先生之外的所有指标上都比其他模型具有更好的性能。 这是因为这个模型能够处理 FB15K 中的逆关系，当实体被用作尾部时应用实体嵌入的共轭复数，当实体被用作头部时应用正常嵌入。
```

   On the other hand, referring to the overall result on FB15K-237 dataset as shown in Table 6, the model DistMult-LiteralE*~g~* outperforms the other models according to all metrics. This entails that applying LiteralE to DistMult on FB15K-237 provides better performance than applying it to other baseline models. Note that the reason for DistMult-LiteralE*~g~* model to achieve the best result on FB15K-237 dataset is the fact that this dataset does not have any symmetric relation. Regarding the two transformation functions *g* and *g~lin~*, the function *g* leads to better results than *g~lin~* according to the results on both dataset.

```      
另一方面，参考表 6 所示的 FB15K-237 数据集的总体结果，根据所有指标，模型 DistMult-LiteralEg 优于其他模型。 这意味着将 LiteralE 应用于 FB15K-237 上的 DistMult 会提供比将其应用于其他基准模型更好的性能。 请注意，DistMult-LiteralEg 模型在 FB15K-237 数据集上获得最佳结果的原因是该数据集没有任何对称关系。 关于两个变换函数 g 和 glin，根据两个数据集上的结果，函数 g 比 glin 产生了更好的结果。
```

​                                                                                         Table 5

​                                           Link prediction results on FB15K dataset using filtered setting.

![Table(05).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Table(05).jpg>)

```   
使用过滤设置的 FB15K 数据集上的链接预测结果。
```

### 6.4. *Experiment with Images*

```
图片实验
```

   Note that it is not possible to compare the whole of MKBE==[49]== with any other model as it is the only embedding model which utilizes the three types of literals together: text, numeric, and images. Therefore, its sub model S+I which uses only images has been compared with the embedding model IKRL==[51]==. Since this comparison has already been done by the authors of MKBE==[49]==, the result shown in Table 7 is directly taken from their paper. They have compared the models DistMult+S+I, ConvE+S+I, and IKRL where S stands for structure and I for Image. Both DistMult+S+I and ConvE+S+I are sub models of MKBE which use only relational triples and Images. The result indicates that ConvE+S+I outperforms the other   two models in all metrics on the YAGO-10 dataset (refer to MKBE [49] for more details).  

```      
  注意，不可能将整个 MKBE [49] 与任何其他模型进行比较，因为它是唯一同时使用三种类型的文字的嵌入模型：文本，数字和图像。 因此，将其仅使用图像的子模型 S + I 与嵌入模型 IKRL[51] 进行了比较。 由于这种比较已经由 MKBE[49] 的作者完成，因此表 7 中显示的结果直接取自他们的论文。 他们比较了 DistMult + S + I，ConvE + S + I 和 IKRL 模型，其中 S 代表结构，I 代表图像。 DistMult + S + I 和 ConvE + S + I 都是 MKBE 的子模型，它们仅使用关系三元组和图像。 结果表明，在 YAGO-10 数据集的所有指标上，ConvE + S + I 均优于其他两个模型（有关更多详细信息，请参阅 MKBE[49]）。
```

### 6.5. *Experiment with Multi-modal Literals*

```   
多模式文本的实验
```

   As discussed in Section 4, the existing multi-modal embeddings are categorized into two types: i) models with text literal, numeric literal and image literals and ii) models with text and numeric literals. However, since MKBE is the only model in the first category only its submodel *S* + *I* could be compared with IKRL (see Section 6.4). Regarding the models with text and numeric literals, i.e., LiteralE with blocking and EAKGAE, they are not included in the experiment as well. The issue with EAKGAE is the same as that of KDCoE, i.e., it is trained on entity alignment task where as the reason for not having LiteralE with blocking is that its code is not publicly available. On the contrary, LiteralE (a model with numerics) has also been adopted to incorporate text literals in the experiments conducted by the authors. Similarly, in our experiment, the LiteralE approach has been tried out with the combination of text and numeric literals, i.e., the model DistMult-LiteralE*~g~*-text in Table 8. Then, the result has been compared with LiteralE with just numeric literals (DistMult-LiteralE*~g~*) and DKRL (a model using only text literals) so as to investigate the benefits of utilizing information represented by different types of literals.

```
如第 4 节所述，现有的多模式嵌入分为两种类型：i）具有文本文字，数字文字和图像文字的模型，以及 ii）具有文本和数值文字的模型。但是，由于 MKBE 是第一类中唯一的模型，因此只能将其子模型 S + I 与 IKRL 进行比较（请参见 6.4 节）。对于带有文字和数字文字的模型，即带有阻塞功能的 LiteralE 和 EAKGAE，它们也未包含在实验中。 EAKGAE 的问题与 KDCoE 的问题相同，即，它是在实体对齐任务上接受训练的，因为没有 LiteralE 带阻塞的原因是其代码不可公开获得。相反，在作者进行的实验中，还采用了 LiteralE（带有数字的模型）将文字文字包含在内。同样，在我们的实验中，已尝试将文本和数字文字结合使用 LiteralE 方法，即表 8 中的模型 DistMult-LiteralEg-text。然后，将结果与只使用数字字面值 (DistMult-LiteralEg) 和只使用文本字面值的模型 DKRL 进行比较，以探讨利用不同类型字面值表示的信息的好处。
```

​                                                                                         Table 6

​                                        Link prediction results on FB15K-237 dataset using filtered setting.

![Table(06).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Table(06).jpg>)  

​                                                                                      Table 7

​                                MRR results on link prediction task on YAGO-10 taken from MKBE [49].

![Table(07).jpg](<https://raw.githubusercontent.com/Kdocke/MyDocumentImg/master/GraduationProject/LiteratureTranslation/Table(07).jpg>)

```     
表6: 过滤设置在 FB15K-237数据集上的链路预测结果。
表7: 来自 MKBE 的 YAGO-10 上的链接预测任务的 MRR 结果 [49]。
```

   DistMult-LiteralE*~g~*-text is a model which applies the LiteralE approach to DistMult by using both numeric and text literals. Note that DistMult is chosen here as a baseline due to the reason that the best result in the experiments with numerics on the FB15K-237 dataset is achieved using this model as discussed in Section 6.3. The datasets listed in Table 3 are also used for this experiment along with additional text attributive triples which are descriptions of entities. DistMult-LiteralE*~g~*-text has also been compared with its numeric only equivalent DistMult-LiteralE*~g~* and DKRL*~Bern~*.

```   
DistMult-LiteralEg 文本是一个模型，该模型通过使用数字和文本文字，将 LiteralE 方法应用于 DistMult。 请注意，此处选择 DistMult 作为基准，是因为使用 6.3 节中讨论的模型可以在 FB15K-237 数据集上进行数字实验的最佳结果。 表 3 中列出的数据集也与其他文本属性三元组（对实体的描述）一起用于本实验。 还比较了 DistMult-LiteralEg 文本与其仅等效数字的 DistMult-LiteralEg 和 DKRLBern。
```

   The experimental results obtained on the datasets FB15K and FB15K-237 are shown in Table 8. As the result indicates, combining text and numeric literals on FB15K dataset with DistMult-LiteralE*~g~*-text approach does not produce better results as compared to the other models DistMult-LiteralE*~g~* and DKRL*~Bern~*. As mentioned before, this dataset contains a set of inverse relations which may lead to having a triple whose inverse has a different label. Given the fact that DistMult fails to model such asymmetric relations, incorporating more literals with DistMult may introduce much noise than improving the performance. On the other hand, for FB15K-237 dataset, according to all the measures except MR, DistMult-LiteralE*~g~*-text model works better for the head entity prediction compared to the other two models. For tail entity prediction, DKRL*~Bern~* works better with respect to all measures for the same dataset.

```
在数据集 FB15K 和 FB15K-237 上获得的实验结果如表 8 所示。 结果表明，在 FB15K 数据集上采用 DistMult-LiteralEg-text 方法结合文本和数值文本，并不能产生比其他模型 DistMult-LiteralEg 和 DKRLBern 更好的结果。 如前所述，该数据集包含一组逆关系，这可能导致三元组的逆具有不同的标签。 鉴于 DistMult 无法为这种不对称关系建模的事实，将更多文字与 DistMult 结合使用可能会带来比提高性能更多的噪音。 另一方面，对于 FB15K-237 数据集，根据除 MR 之外的所有度量，与其他两个模型相比，DistMult-LiteralEg 文本模型对头部实体预测的效果更好。 对于尾部实体预测，对于相同数据集的所有度量，DKRLBern 效果更好。
```

